{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlg6V4h+bhYHTT9iRWFth5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"3d20a623558b43588a50e30c5ed09b69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b2136199c0a4ae0845d991b0c2b0e43","IPY_MODEL_12805ae18dc848a995793d2e61891968","IPY_MODEL_b12c175ed602456bacd149364816b4cb"],"layout":"IPY_MODEL_e5279feca6df4367b595ca9f327fb470"}},"5b2136199c0a4ae0845d991b0c2b0e43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d27381b110b842ad9a6efa07339ea3bb","placeholder":"​","style":"IPY_MODEL_af7451f209fa403f8663cf99a5a2b5f6","value":"Batches: 100%"}},"12805ae18dc848a995793d2e61891968":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34b3ffb209ea4e3cae83244f2e705ced","max":251,"min":0,"orientation":"horizontal","style":"IPY_MODEL_284bf98d36024ad6b5e1ea8db52699c8","value":251}},"b12c175ed602456bacd149364816b4cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cf2b557449742559aabaab4563142f4","placeholder":"​","style":"IPY_MODEL_69d521fca2b4439687dd2aaed302d619","value":" 251/251 [00:06&lt;00:00, 42.34it/s]"}},"e5279feca6df4367b595ca9f327fb470":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d27381b110b842ad9a6efa07339ea3bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af7451f209fa403f8663cf99a5a2b5f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34b3ffb209ea4e3cae83244f2e705ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"284bf98d36024ad6b5e1ea8db52699c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9cf2b557449742559aabaab4563142f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d521fca2b4439687dd2aaed302d619":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxkxHomRZE_a","executionInfo":{"status":"ok","timestamp":1668723525492,"user_tz":300,"elapsed":2698,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"33a8276e-5e35-4662-b6c7-8988d12749dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n","Requirement already satisfied: sentence_transformers==0.3.3 in /usr/local/lib/python3.7/dist-packages (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.53)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers==0.3.3) (3.7)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers==0.3.3) (1.12.1+cu113)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers==0.3.3) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers==0.3.3) (1.7.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->sentence_transformers==0.3.3) (4.1.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers==0.3.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers==0.3.3) (1.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers==0.3.3) (3.1.0)\n"]}],"source":["pip install transformers==3.0.2 sentence_transformers==0.3.3"]},{"cell_type":"code","source":["import re\n","import os\n","import json\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import random\n","import torch \n","from torch import nn\n","import seaborn as sns\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"ZCxhdds_ZKFI","executionInfo":{"status":"ok","timestamp":1668723501286,"user_tz":300,"elapsed":4920,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7ouigMxZMG2","executionInfo":{"status":"ok","timestamp":1668723522796,"user_tz":300,"elapsed":21516,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"4c34ae42-e0e7-4352-9023-e6681855f08c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Assignments/capstone/phrase-bert-topic-model-master/phrase-topic-model/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bSLh-Oye0Ih","executionInfo":{"status":"ok","timestamp":1668723526781,"user_tz":300,"elapsed":1296,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"5004dc93-6c2f-4df1-a0c4-7431abac7874"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assignments/capstone/phrase-bert-topic-model-master/phrase-topic-model\n"]}]},{"cell_type":"code","source":["from model.dae_model import DictionaryAutoencoder\n","from model_utils import run_epoch, text_to_topic, rank_topics_by_percentage"],"metadata":{"id":"QzGz1ftQe_da","executionInfo":{"status":"ok","timestamp":1668723530130,"user_tz":300,"elapsed":3352,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Assignments/capstone/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hitHKPFZNrO","executionInfo":{"status":"ok","timestamp":1668723530131,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"4246657e-0e48-4a60-852e-5869fda71af1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assignments/capstone\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5mMSNsOZSCR","executionInfo":{"status":"ok","timestamp":1668723530638,"user_tz":300,"elapsed":511,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"288bb62f-9cc4-4052-d390-7c99d4b257d0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":[" 2017_headline_results.csv         pntm_headlines.ipynb\n"," FINAL_semeval2010t8_train.csv     pntm_semeval.ipynb\n"," headline_topic_df_100.csv         pntm_semeval_labels.ipynb\n"," phrase_bert_similarity.ipynb     \u001b[0m\u001b[01;34m'pooled_context_para_triples_p=0.8'\u001b[0m/\n"," \u001b[01;34mphrase-bert-topic-model-master\u001b[0m/   semeval2010t8_test.csv\n"," \u001b[01;34mpntm\u001b[0m/                             semeval2010t8_train.csv\n"]}]},{"cell_type":"code","source":["semeval_train = pd.read_csv('semeval2010t8_train.csv')\n","semeval_test = pd.read_csv('semeval2010t8_train.csv')\n","semeval_label = pd.concat([semeval_train, semeval_test])"],"metadata":{"id":"NAWLgS6OZZgH","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":1370,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["semeval_label.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"ILs72v1rZlkQ","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":10,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"1261aa75-696a-4772-f446-3c054f4d2166"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          corpus      doc_id  sent_id  eg_id                         index  \\\n","0  semeval2010t8  train.json        0      0  semeval2010t8_train.json_0_0   \n","1  semeval2010t8  train.json        1      0  semeval2010t8_train.json_1_0   \n","2  semeval2010t8  train.json        2      0  semeval2010t8_train.json_2_0   \n","3  semeval2010t8  train.json        3      0  semeval2010t8_train.json_3_0   \n","4  semeval2010t8  train.json        4      0  semeval2010t8_train.json_4_0   \n","\n","                                                text  \\\n","0  The system as described above has its greatest...   \n","1  The child was carefully wrapped and bound into...   \n","2  The author of a keygen uses a disassembler to ...   \n","3             A misty ridge uprises from the surge .   \n","4  The student association is the voice of the un...   \n","\n","                                        text_w_pairs  seq_label  pair_label  \\\n","0  The system as described above has its greatest...          0           0   \n","1  The <ARG1>child</ARG1> was carefully wrapped a...          0           0   \n","2  The <ARG1>author</ARG1> of a keygen uses a <AR...          0           0   \n","3  A misty <ARG1>ridge</ARG1> uprises from the <A...          0           0   \n","4  The <ARG0>student</ARG0> <ARG1>association</AR...          0           0   \n","\n","   context  num_sents  \n","0      NaN          1  \n","1      NaN          1  \n","2      NaN          1  \n","3      NaN          1  \n","4      NaN          1  "],"text/html":["\n","  <div id=\"df-44ced75c-f2f6-490a-8fef-08b911ca312b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus</th>\n","      <th>doc_id</th>\n","      <th>sent_id</th>\n","      <th>eg_id</th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>text_w_pairs</th>\n","      <th>seq_label</th>\n","      <th>pair_label</th>\n","      <th>context</th>\n","      <th>num_sents</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_0_0</td>\n","      <td>The system as described above has its greatest...</td>\n","      <td>The system as described above has its greatest...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_1_0</td>\n","      <td>The child was carefully wrapped and bound into...</td>\n","      <td>The &lt;ARG1&gt;child&lt;/ARG1&gt; was carefully wrapped a...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_2_0</td>\n","      <td>The author of a keygen uses a disassembler to ...</td>\n","      <td>The &lt;ARG1&gt;author&lt;/ARG1&gt; of a keygen uses a &lt;AR...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_3_0</td>\n","      <td>A misty ridge uprises from the surge .</td>\n","      <td>A misty &lt;ARG1&gt;ridge&lt;/ARG1&gt; uprises from the &lt;A...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_4_0</td>\n","      <td>The student association is the voice of the un...</td>\n","      <td>The &lt;ARG0&gt;student&lt;/ARG0&gt; &lt;ARG1&gt;association&lt;/AR...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44ced75c-f2f6-490a-8fef-08b911ca312b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-44ced75c-f2f6-490a-8fef-08b911ca312b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-44ced75c-f2f6-490a-8fef-08b911ca312b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# filter the dataframe by pair_label for only causal texts\n","semeval_label_causal = semeval_label[semeval_label['pair_label'] == 1] \n","len(semeval_label_causal), len(semeval_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XamO9BJZZZdF","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":9,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"60914904-968f-4f10-b2d0-f781e6309cf2"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2006, 16000)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["semeval_label_causal.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":635},"id":"4wwNEzukbj4Q","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":8,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"88735794-21fa-47ee-f074-1b4112d493ac"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           corpus      doc_id  sent_id  eg_id                          index  \\\n","6   semeval2010t8  train.json        6      0   semeval2010t8_train.json_6_0   \n","13  semeval2010t8  train.json       13      0  semeval2010t8_train.json_13_0   \n","22  semeval2010t8  train.json       22      0  semeval2010t8_train.json_22_0   \n","26  semeval2010t8  train.json       26      0  semeval2010t8_train.json_26_0   \n","31  semeval2010t8  train.json       31      0  semeval2010t8_train.json_31_0   \n","\n","                                                 text  \\\n","6   The current view is that the chronic inflammat...   \n","13  The burst has been caused by water hammer pres...   \n","22  The singer , who performed three of the nomina...   \n","26  Suicide is one of the leading causes of death ...   \n","31  He had chest pains and headaches from mold in ...   \n","\n","                                         text_w_pairs  seq_label  pair_label  \\\n","6   The current view is that the chronic <ARG1>inf...          1           1   \n","13  The <ARG1>burst</ARG1> has been caused by wate...          1           1   \n","22  The <ARG0>singer</ARG0> , who performed three ...          1           1   \n","26  <ARG0>Suicide</ARG0> is one of the leading cau...          1           1   \n","31  He had chest pains and <ARG1>headaches</ARG1> ...          1           1   \n","\n","    context  num_sents  \n","6       NaN          1  \n","13      NaN          1  \n","22      NaN          1  \n","26      NaN          1  \n","31      NaN          1  "],"text/html":["\n","  <div id=\"df-d429d969-f47e-4035-bd8a-79f6c81605e1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus</th>\n","      <th>doc_id</th>\n","      <th>sent_id</th>\n","      <th>eg_id</th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>text_w_pairs</th>\n","      <th>seq_label</th>\n","      <th>pair_label</th>\n","      <th>context</th>\n","      <th>num_sents</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_6_0</td>\n","      <td>The current view is that the chronic inflammat...</td>\n","      <td>The current view is that the chronic &lt;ARG1&gt;inf...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_13_0</td>\n","      <td>The burst has been caused by water hammer pres...</td>\n","      <td>The &lt;ARG1&gt;burst&lt;/ARG1&gt; has been caused by wate...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_22_0</td>\n","      <td>The singer , who performed three of the nomina...</td>\n","      <td>The &lt;ARG0&gt;singer&lt;/ARG0&gt; , who performed three ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_26_0</td>\n","      <td>Suicide is one of the leading causes of death ...</td>\n","      <td>&lt;ARG0&gt;Suicide&lt;/ARG0&gt; is one of the leading cau...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_31_0</td>\n","      <td>He had chest pains and headaches from mold in ...</td>\n","      <td>He had chest pains and &lt;ARG1&gt;headaches&lt;/ARG1&gt; ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d429d969-f47e-4035-bd8a-79f6c81605e1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d429d969-f47e-4035-bd8a-79f6c81605e1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d429d969-f47e-4035-bd8a-79f6c81605e1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# textwpairs column\n","semeval_label_textwpairs = semeval_label_causal['text_w_pairs']"],"metadata":{"id":"6cYVOvUnbklr","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["semeval_label_textwpairs.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfwxKNsUbkST","executionInfo":{"status":"ok","timestamp":1668723532005,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"1e571178-ee22-49a9-c1c4-8b74d1d979a6"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6     The current view is that the chronic <ARG1>inf...\n","13    The <ARG1>burst</ARG1> has been caused by wate...\n","22    The <ARG0>singer</ARG0> , who performed three ...\n","26    <ARG0>Suicide</ARG0> is one of the leading cau...\n","31    He had chest pains and <ARG1>headaches</ARG1> ...\n","Name: text_w_pairs, dtype: object"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["def extract_args(dataset):\n","    arg0s = []\n","    arg1s = []\n","    for textwpair in dataset:\n","        arg0 = re.findall(r\"<ARG0>(.*?)</ARG0>\", textwpair) # list of all argument0s in string textwpair\n","        arg1 = re.findall(r\"<ARG1>(.*?)</ARG1>\", textwpair) # list of all argument1s in string textwpair\n","        if len(arg0) != 0:\n","            # unpack the list of argument0s and append them one by one\n","            for arg in arg0:\n","                arg0s.append(arg)\n","        if len(arg1) != 0:\n","            # unpack the list of argument1s and append them one by one\n","            for arg in arg1:\n","                arg1s.append(arg)\n","    return arg0s, arg1s"],"metadata":{"id":"33sLm7nqcUuI","executionInfo":{"status":"ok","timestamp":1668723532006,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# list of arg0s and arg1s for SemEval labels\n","semeval_label_arg0s, semeval_label_arg1s = extract_args(semeval_label_textwpairs)\n","# list of all args for SemEval labels\n","semeval_label_args = semeval_label_arg0s + semeval_label_arg1s\n","len(semeval_label_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KDOOtKHcUmX","executionInfo":{"status":"ok","timestamp":1668723532006,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"b0e670b0-cbba-42db-c2ec-52b50f4f3c40"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4012"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# construct text_list\n","semeval_label_text = semeval_label_causal['text']\n","semeval_label_text_list = semeval_label_text.tolist()"],"metadata":{"id":"usq9E-w7igwV","executionInfo":{"status":"ok","timestamp":1668723532006,"user_tz":300,"elapsed":6,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# construct dictionaries of word2id\n","semeval_label_word2id = {val : idx for idx, val in enumerate(set(semeval_label_args))}\n","len(semeval_label_word2id.keys()), len(semeval_label_args) # duplicates"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gbv6lpDcfWX","executionInfo":{"status":"ok","timestamp":1668723532006,"user_tz":300,"elapsed":6,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"e2919ef1-b810-4df9-948e-048bdc2648f8"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1131, 4012)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# construct dictionaries of id2word\n","semeval_label_id2word = {val: key for key, val in semeval_label_word2id.items()}"],"metadata":{"id":"6S-av33OkkMB","executionInfo":{"status":"ok","timestamp":1668723532006,"user_tz":300,"elapsed":5,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# contruct dictionaries of id2freq\n","semeval_label_id2freq = semeval_label_id2word.copy()\n","semeval_label_freq = [(arg, semeval_label_args.count(arg)) for arg in set(semeval_label_args)]\n","i = 0\n","for key, val in semeval_label_id2freq.items():\n","    semeval_label_id2freq[key] = semeval_label_freq[i][1]\n","    i += 1"],"metadata":{"id":"Yg_yh-YKidA8","executionInfo":{"status":"ok","timestamp":1668723532324,"user_tz":300,"elapsed":323,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# load the Phrase-BERT model through the sentence-BERT interface\n","model_path = \"/content/drive/MyDrive/Assignments/capstone/pooled_context_para_triples_p=0.8/\"\n","model = SentenceTransformer(model_path)"],"metadata":{"id":"9jGCi-J3cUi5","executionInfo":{"status":"ok","timestamp":1668723547915,"user_tz":300,"elapsed":15593,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# commented out because results are already saved and can be loaded from files\n","# compute phrase embeddings using Phrase-BERT\n","semeval_label_embs = model.encode(set(semeval_label_args), batch_size=8, show_progress_bar=True)\n","semeval_label_embs = np.asarray(semeval_label_embs)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"jLmaPKdmcUbR","executionInfo":{"status":"ok","timestamp":1668723547916,"user_tz":300,"elapsed":16,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"58865d39-65d4-406c-a342-532bbe3d7dcc"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# commented out because results are already saved and can be loaded from files\\n# compute phrase embeddings using Phrase-BERT\\nsemeval_label_embs = model.encode(set(semeval_label_args), batch_size=8, show_progress_bar=True)\\nsemeval_label_embs = np.asarray(semeval_label_embs)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["\"\"\"\n","# save the results\n","topic_model_data_path = \"/content/drive/MyDrive/Assignments/capstone/pntm/\"\n","np.save(os.path.join(topic_model_data_path, 'semeval_label_embs_matrix_np'), semeval_label_embs)\n","\"\"\""],"metadata":{"id":"lYONu0dqcfhK","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1668723547916,"user_tz":300,"elapsed":13,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"6aaa731b-c8e5-499a-caca-f16b9973d9d5"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# save the results\\ntopic_model_data_path = \"/content/drive/MyDrive/Assignments/capstone/pntm/\"\\nnp.save(os.path.join(topic_model_data_path, \\'semeval_label_embs_matrix_np\\'), semeval_label_embs)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# set seed\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wJn3cf9cfb0","executionInfo":{"status":"ok","timestamp":1668723547916,"user_tz":300,"elapsed":13,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"28f6fc33-2b02-4a07-e3d2-49b388c2bcd9"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f91bfd62f50>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["topic_model_data_path = \"/content/drive/MyDrive/Assignments/capstone/pntm/\""],"metadata":{"id":"bqPH2_B0fu2g","executionInfo":{"status":"ok","timestamp":1668723547917,"user_tz":300,"elapsed":9,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["semeval_label_embs_matrix_np = np.load(os.path.join(topic_model_data_path, f\"semeval_label_embs_matrix_np.npy\"))\n","print(f\"Loaded semeval labels word embedding from {topic_model_data_path}\")\n","print(f\"Loaded vocab size of {len(semeval_label_word2id)} (including phrases)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASAXPFRWcfZN","executionInfo":{"status":"ok","timestamp":1668723548494,"user_tz":300,"elapsed":586,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"8019ee44-01b6-4543-e0fd-5077c3b4c41f"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded semeval labels word embedding from /content/drive/MyDrive/Assignments/capstone/pntm/\n","Loaded vocab size of 1131 (including phrases)\n"]}]},{"cell_type":"code","source":["len(semeval_label_embs_matrix_np)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6eQ_6XDRuer","executionInfo":{"status":"ok","timestamp":1668723548494,"user_tz":300,"elapsed":8,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"9c632dad-0bb7-4e3c-eb0a-437afd796ba3"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1131"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["Below code is adapted from Phrase-Bert: https://github.com/sf-wa-326/phrase-bert-topic-model"],"metadata":{"id":"89H47gDLofNy"}},{"cell_type":"code","source":["# word frequency and filter info\n","\n","# compute the length (in n-grams)\n","# setting word_threshould really high to include every phrase, reset to lower value to remove longer phrases\n","word_threshold = 100\n","semeval_label_len_words = [0] * len(semeval_label_id2word)\n","for (id, word) in semeval_label_id2word.items():\n","    semeval_label_len_words[id] = len(word.split(' '))\n","semeval_label_indices_to_remove_based_on_len = [id \n","                                                for id, word_len \n","                                                in enumerate(semeval_label_len_words) \n","                                                if (word_len > word_threshold)]\n","\n","print(len(semeval_label_indices_to_remove_based_on_len)) # 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kd18tS-qcfUI","executionInfo":{"status":"ok","timestamp":1668723548495,"user_tz":300,"elapsed":5,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"816ee9ae-eace-49cd-dfb0-2b35ba232e36"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","source":["# keeping every token / word, reset freq_threshold to remove lower frequency words\n","freq_threshold = 0\n","\n","semeval_label_sorted_ids = [k for k, v in sorted(semeval_label_id2freq.items(), key=lambda item: item[1])]\n","semeval_label_sorted_ids.reverse()\n","semeval_label_indices_to_remove_based_on_freq = [k for k, v in semeval_label_id2freq.items() if v <= freq_threshold ]\n","semeval_label_to_be_removed = list(set(semeval_label_indices_to_remove_based_on_freq \n","                                       + semeval_label_indices_to_remove_based_on_len))"],"metadata":{"id":"CsKq9929cfHs","executionInfo":{"status":"ok","timestamp":1668723548495,"user_tz":300,"elapsed":4,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# encode the text_lists\n","semeval_label_text_rep_list = model.encode(semeval_label_text, batch_size = 8, show_progress_bar = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3d20a623558b43588a50e30c5ed09b69","5b2136199c0a4ae0845d991b0c2b0e43","12805ae18dc848a995793d2e61891968","b12c175ed602456bacd149364816b4cb","e5279feca6df4367b595ca9f327fb470","d27381b110b842ad9a6efa07339ea3bb","af7451f209fa403f8663cf99a5a2b5f6","34b3ffb209ea4e3cae83244f2e705ced","284bf98d36024ad6b5e1ea8db52699c8","9cf2b557449742559aabaab4563142f4","69d521fca2b4439687dd2aaed302d619"]},"id":"A4_e7zIjho2U","executionInfo":{"status":"ok","timestamp":1668723563477,"user_tz":300,"elapsed":14986,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"de13b90d-28f0-4640-f06b-b1d7cdb2d001"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/251 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d20a623558b43588a50e30c5ed09b69"}},"metadata":{}}]},{"cell_type":"code","source":["emb_model = \"phrase-bert\"\n","print(f\"Building sentence model by using {emb_model} as embedding model\")\n","\n","semeval_label_uid_input_vector_list = [(i, semeval_label_text_rep_list[i]) for i in range(len(semeval_label_text_rep_list))]\n","print(f\"Computed {len(semeval_label_uid_input_vector_list)} positive examples\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDvVzeTvB_9W","executionInfo":{"status":"ok","timestamp":1668723563478,"user_tz":300,"elapsed":11,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"588c9ff0-3635-43c3-8907-24373808434e"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Building sentence model by using phrase-bert as embedding model\n","Computed 2006 positive examples\n"]}]},{"cell_type":"code","source":["# setting the argument num_negative_samples for negative sampling\n","num_neg_samples = 5 # default in the original model\n","\n","semeval_label_uid_input_vector_list_neg = []\n","indices = list(range(len(semeval_label_uid_input_vector_list)))\n","for idx in range(len(semeval_label_uid_input_vector_list)):\n","    indices_candidate = indices\n","    neg_indices = random.sample(indices_candidate, num_neg_samples)\n","    neg_samples = [semeval_label_uid_input_vector_list[neg_i][1] for neg_i in neg_indices]\n","    neg_vector = np.mean(neg_samples, axis=0)\n","    semeval_label_uid_input_vector_list_neg.append(neg_vector)\n","print(f\"Computed {len(semeval_label_uid_input_vector_list_neg)} negative examples\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xsoht1hQikis","executionInfo":{"status":"ok","timestamp":1668723563478,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"1617f949-995b-4302-e192-4afcaba9de41"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Computed 2006 negative examples\n"]}]},{"cell_type":"code","source":["# set up hyperparameters\n","semeval_label_net_params = {}\n","semeval_label_net_params[\"mode\"] = \"bert\"\n","semeval_label_net_params[\"embedding\"] = semeval_label_embs_matrix_np\n","semeval_label_net_params[\"d_hid\"] = 100\n","semeval_label_net_params[\"num_rows\"] = 50  # number of topics\n","semeval_label_net_params[\"num_sub_topics\"] = 0\n","semeval_label_net_params[\"word_dropout_prob\"] = 0.2\n","semeval_label_net_params[\"vrev\"] = semeval_label_id2word  # idx to word map\n","semeval_label_net_params[\"device\"] = 'cuda'\n","semeval_label_net_params[\"pred_world\"] = False"],"metadata":{"id":"MipxoZ-gikf-","executionInfo":{"status":"ok","timestamp":1668723563479,"user_tz":300,"elapsed":5,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["semeval_label_net = DictionaryAutoencoder(net_params=semeval_label_net_params)\n","semeval_label_net.to('cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZaqIlCkikdX","executionInfo":{"status":"ok","timestamp":1668723563862,"user_tz":300,"elapsed":388,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"0c8dc160-87b6-463c-8a89-13969c1c342e"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DictionaryAutoencoder(\n","  (embeddings): Embedding(1131, 768)\n","  (W_proj): Linear(in_features=768, out_features=100, bias=True)\n","  (act): ReLU()\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (W_att): Linear(in_features=100, out_features=768, bias=True)\n","  (W_out): Linear(in_features=768, out_features=1131, bias=True)\n",")"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# training specs (default from original code)\n","num_epochs = 300\n","batch_size = 100\n","ortho_weight = 1e-5\n","world_clas_weight = 0.0\n","semeval_label_optim = torch.optim.Adam(semeval_label_net.parameters(), lr=1e-4)\n","interpret_interval = int(np.ceil(num_epochs / 10))\n","h_model = 2"],"metadata":{"id":"oeFCe0_4ikax","executionInfo":{"status":"ok","timestamp":1668723563863,"user_tz":300,"elapsed":9,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# iterating through batches\n","semeval_label_batch_intervals = [\n","    (start, start + batch_size)\n","    for start in range(0, len(semeval_label_uid_input_vector_list), batch_size)]\n","    # batch_intervals = batch_intervals[:100]\n","semeval_label_split = int(np.ceil(len(semeval_label_batch_intervals) * 0.9))\n","semeval_label_batch_intervals_train = semeval_label_batch_intervals[:semeval_label_split]\n","semeval_label_batch_intervals_valid = semeval_label_batch_intervals[semeval_label_split:]"],"metadata":{"id":"86GDPHebikYJ","executionInfo":{"status":"ok","timestamp":1668723563863,"user_tz":300,"elapsed":8,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["import argparse\n","parser = argparse.ArgumentParser()\n","\"\"\"parser.add_argument(\"--lr\", type=float, default=1e-4)\n","\"\"\"\n","args = parser.parse_args(args=[])\n","args.device = 'cuda:' + '0'\n","args.triplet_loss_margin = 1.0\n","args.triplet_loss_weight = 1.0\n","args.ortho_weight = 1e-5\n","args.neighbour_loss_weight = 1e-7\n","args.offset_loss_weight = 1e-4"],"metadata":{"id":"uqJ3ZrFhqwCx","executionInfo":{"status":"ok","timestamp":1668723563863,"user_tz":300,"elapsed":8,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# semeval label training\n","print(\"\\n\" + \"=\" * 70)\n","for epoch in range(num_epochs):\n","    # training\n","    semeval_label_net.train()\n","    train_mode = True\n","    print(f\"Epoch {epoch}\")\n","    run_epoch(semeval_label_net, semeval_label_optim, semeval_label_batch_intervals_train,\n","              semeval_label_uid_input_vector_list, semeval_label_uid_input_vector_list_neg,\n","              args, train_mode, h_model, epoch, 200)\n","\n","    # validation\n","    semeval_label_net.eval()\n","    train_mode = False\n","    with torch.no_grad():\n","        run_epoch(\n","                semeval_label_net,\n","                semeval_label_optim,\n","                semeval_label_batch_intervals_valid,\n","                semeval_label_uid_input_vector_list,\n","                semeval_label_uid_input_vector_list_neg,\n","                args, \n","                train_mode,\n","                h_model,\n","                epoch,\n","                200\n","        )\n","\n","    if (epoch + 1) % interpret_interval == 0:\n","        print(\"Topics with probability argmax\")\n","        topics_print_list = semeval_label_net.rank_vocab_for_topics(\n","            word_embedding_matrix=semeval_label_embs_matrix_np,\n","            to_be_removed=semeval_label_to_be_removed)\n","        print(\"=\" * 70)\n","\n","    print()\n","    print()\n","    print()\n","    print(\"=\" * 70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bb_TidWMqsfX","executionInfo":{"status":"ok","timestamp":1668723598613,"user_tz":300,"elapsed":34757,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"b63056f3-a3d8-4b79-9283-d56300e8438a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Epoch 0\n","[TRAIN] loss: 0.9694, 0.9694, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.30 s\n","[VALID] loss: 0.9551, 0.9551, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 1\n","[TRAIN] loss: 0.9127, 0.9127, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.9060, 0.9060, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 2\n","[TRAIN] loss: 0.8702, 0.8702, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.8664, 0.8664, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 3\n","[TRAIN] loss: 0.8363, 0.8363, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.8363, 0.8363, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 4\n","[TRAIN] loss: 0.8085, 0.8085, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.8113, 0.8113, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 5\n","[TRAIN] loss: 0.7851, 0.7851, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.7894, 0.7894, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 6\n","[TRAIN] loss: 0.7655, 0.7655, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.7735, 0.7735, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 7\n","[TRAIN] loss: 0.7486, 0.7486, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.7568, 0.7568, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 8\n","[TRAIN] loss: 0.7329, 0.7329, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.16 s\n","[VALID] loss: 0.7416, 0.7416, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 9\n","[TRAIN] loss: 0.7190, 0.7190, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.16 s\n","[VALID] loss: 0.7318, 0.7318, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 10\n","[TRAIN] loss: 0.7058, 0.7058, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.30 s\n","[VALID] loss: 0.7134, 0.7134, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.03 s\n","\n","\n","\n","======================================================================\n","Epoch 11\n","[TRAIN] loss: 0.6932, 0.6932, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.24 s\n","[VALID] loss: 0.6966, 0.6966, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 12\n","[TRAIN] loss: 0.6810, 0.6810, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.22 s\n","[VALID] loss: 0.6939, 0.6939, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 13\n","[TRAIN] loss: 0.6714, 0.6714, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.31 s\n","[VALID] loss: 0.6827, 0.6827, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 14\n","[TRAIN] loss: 0.6607, 0.6607, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.27 s\n","[VALID] loss: 0.6705, 0.6705, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 15\n","[TRAIN] loss: 0.6508, 0.6508, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.28 s\n","[VALID] loss: 0.6660, 0.6660, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 16\n","[TRAIN] loss: 0.6412, 0.6412, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.29 s\n","[VALID] loss: 0.6518, 0.6518, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.02 s\n","\n","\n","\n","======================================================================\n","Epoch 17\n","[TRAIN] loss: 0.6322, 0.6322, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.20 s\n","[VALID] loss: 0.6449, 0.6449, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 18\n","[TRAIN] loss: 0.6244, 0.6244, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.16 s\n","[VALID] loss: 0.6409, 0.6409, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 19\n","[TRAIN] loss: 0.6178, 0.6178, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.6337, 0.6337, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 20\n","[TRAIN] loss: 0.6104, 0.6104, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.6392, 0.6392, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 21\n","[TRAIN] loss: 0.6034, 0.6034, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.6319, 0.6319, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 22\n","[TRAIN] loss: 0.5989, 0.5989, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.6216, 0.6216, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 23\n","[TRAIN] loss: 0.5921, 0.5921, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.6155, 0.6155, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 24\n","[TRAIN] loss: 0.5866, 0.5866, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.6113, 0.6113, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 25\n","[TRAIN] loss: 0.5825, 0.5825, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.19 s\n","[VALID] loss: 0.6157, 0.6157, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 26\n","[TRAIN] loss: 0.5764, 0.5764, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.6068, 0.6068, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 27\n","[TRAIN] loss: 0.5736, 0.5736, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.6106, 0.6106, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 28\n","[TRAIN] loss: 0.5696, 0.5696, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5936, 0.5936, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 29\n","[TRAIN] loss: 0.5661, 0.5661, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.5949, 0.5949, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","Topics with probability argmax\n","50\n","50\n","[ 893  834  640  845 1128  632  582  797  432  970]\n","topic 0 : electricity, humidity, shortage, Addiction, hormonal changes, screen, deluge, Production, Sadness, consumption\n","[  54  759  941  782  315 1038   93  481   40 1059]\n","topic 1 : flood, programmer, influenza, repetition, alterations, conquests, improvement, competition, bite, bankruptcies\n","[ 367   17  589  893 1032  570  814   62  298   46]\n","topic 2 : deforestation, convergence, meningitis, electricity, discharge, distortion, preservation, bombs, Menopause, cyclone\n","[ 958  352 1021    1  400  110  832  791  970  629]\n","topic 3 : gases, threats, drain, rising unemployment rate, substance, wave, bills, cementation, consumption, anticipation\n","[ 815 1019  770  503  871  667  534  245  298  720]\n","topic 4 : loop, cancellation, dams, Malaria, crusaders, celebration, fighting, deterioration, Menopause, bleeding\n","[ 530  436  877  968  895  688  610  354 1024  652]\n","topic 5 : debate, boiling water, impact, failure, process, shoulder problems, Osteoporosis, sinking, particles, Trauma\n","[ 72 677 501 338  63 457 782 415 336 538]\n","topic 6 : execution, chemotherapy, design, maker, proof, drowning, repetition, downturn, compromises, Convulsions\n","[ 385  759 1067 1031  791  727  672  352  593  940]\n","topic 7 : preservatives, programmer, exhibition, ointment, cementation, symptoms, indigestion, threats, interest, joy\n","[ 947  970  636  998 1128  797  727  670  619  845]\n","topic 8 : lack, consumption, species, cancer, hormonal changes, Production, symptoms, experience, vocals, Addiction\n","[ 675  797  851  632  538  619  670  640 1128  655]\n","topic 9 : books, Production, increase, screen, Convulsions, vocals, experience, shortage, hormonal changes, heroin\n","[520  15 949  58 260 865 176 376  97 593]\n","topic 10 : calcification, pseudolesion, imbalances, overflow, recession, information, fireworks, pyrolysis, thump, interest\n","[ 824  443  714  697  298  842 1068  770  255  382]\n","topic 11 : thorns, excitement, hunting, whistle, Menopause, anxiety, calm, dams, cockroaches, bout\n","[ 110  670  791  672  723 1093  495  871  671  593]\n","topic 12 : wave, experience, cementation, indigestion, amplifier, profit, alarm, crusaders, gravitational force, interest\n","[473 245  39 331 567 690 813 832 720 871]\n","topic 13 : drug use, deterioration, corkscrew, inquiry, transmission, taxes, explosive, bills, bleeding, crusaders\n","[ 223  484  198  968  224 1019  262  879  181 1035]\n","topic 14 : meeting, person, visible light, failure, women, cancellation, fungi, problems, hardening, malfunctions\n","[832 791 962 495 671 720 947 871 224 262]\n","topic 15 : bills, cementation, slowdown, alarm, gravitational force, bleeding, lack, crusaders, women, fungi\n","[1112 1110  848  282  986  287  424 1065  625  368]\n","topic 16 : colds, rill, discomfort, arrests, irritation, tears, mentoring, Injury, man, eating\n","[1099   74  236   58 1003  555   43  528 1031   50]\n","topic 17 : singer, colitis, infections, overflow, toothache, immunizations, flashlight, crisis, ointment, dwarf\n","[720 153 455 496 986 263 292 871 567 262]\n","topic 18 : bleeding, addition, miscarriages, education, irritation, lorry, hardship, crusaders, transmission, fungi\n","[  62  654  131  232 1024  682  902  936  746  225]\n","topic 19 : bombs, insomnia, leak, personality, particles, movements, Weak ligaments, exposure, business, retention\n","[  3 353 372 828 733 969 274 202 582 803]\n","topic 20 : distress, ginseng, temperature, debt, disorders, stroke, activation, arterial blood pressure, deluge, media\n","[  49  333  664  645   40 1029   83 1059 1015  751]\n","topic 21 : resources, ecstasy, surplus, fuels, bite, breakouts, hull, bankruptcies, inspiration, sound\n","[ 246 1064  582  372  121  733  494   56    2  432]\n","topic 22 : sidewalk, smell, deluge, temperature, car, disorders, sale, pleasure, progress, Sadness\n","[1068  834 1019  926  971  436  251  356  809  601]\n","topic 23 : calm, humidity, cancellation, illness, scratch, boiling water, rubbing, tone, slugs, presence\n","[ 397  652  264  602  877  296  206 1008  530   21]\n","topic 24 : internship, Trauma, firing, election, impact, current, passage, launch, debate, consumers\n","[440 731 357 547 463 573 129 899 268 864]\n","topic 25 : famine, itchiness, gaps, device, breezes, currents, concealing, giving, absorption, product\n","[ 716  548  530 1056  284  879   12  852  797  242]\n","topic 26 : concerto, emergency, debate, legs, vibration, problems, swinging, decomposition reaction, Production, cavity\n","[1041  209  479  692  591 1011  262    3  822  473]\n","topic 27 : illnesses, tsunamis, fight, wire, avalanche, field, fungi, distress, conflicts, drug use\n","[264 632 780  95 694 296 629  94 374 845]\n","topic 28 : firing, screen, intersection, collision, resignation, current, anticipation, Birth defects, morbidity, Addiction\n","[1003  509  300 1102   93  288  282  609  917 1065]\n","topic 29 : toothache, lightning, guy, protozoa, improvement, Fatigue, arrests, poisoning, failures, Injury\n","[631 386 315 430  72 651 121 584  17 686]\n","topic 30 : silversmiths, terrorist, alterations, imbalance, execution, helicobacter, car, pills, convergence, properties\n","[ 869   40   51  121  582  633   83  748   82 1036]\n","topic 31 : fume, bite, militancy, car, deluge, science, hull, swelling, smoking, child abuse\n","[953 820 548 451 204 433 376 352 315 867]\n","topic 32 : band, indicator, emergency, burning, breeding, tornado track, pyrolysis, threats, alterations, dust\n","[243 672 381 394 671 353 496 791 720 262]\n","topic 33 : smog, indigestion, eyestrain, spark, gravitational force, ginseng, education, cementation, bleeding, fungi\n","[ 35 689 621 166 996 190 196 643 906 824]\n","topic 34 : burns, amphibolites, caffeine withdrawal, diseases, meteor shower, Lymphedema, dermatitis, microphone, meltdown, thorns\n","[479  63 172 415 229 478 664 336 248 384]\n","topic 35 : fight, proof, fog, downturn, vaccines, trial, surplus, compromises, distention, Dizziness\n","[ 307 1056  685  548 1129  683  936  713  790 1008]\n","topic 36 : worsening, legs, gun, emergency, washing, jams, exposure, weather, disability, launch\n","[ 388  904  810  737  632 1005  826  655  619  209]\n","topic 37 : ionization, braces, victory, faults, screen, bugs, drugs, heroin, vocals, tsunamis\n","[262 813 396 538  39 832 331 895 338 526]\n","topic 38 : fungi, explosive, wildfires, Convulsions, corkscrew, bills, inquiry, process, maker, burn\n","[ 121  432  202 1060 1043  430  877  857  807  797]\n","topic 39 : car, Sadness, arterial blood pressure, quake, sunrise, imbalance, impact, beverages, staring effect, Production\n","[ 785    2  117  616  346 1019 1112  625  121   17]\n","topic 40 : vibrations, progress, changes, molt, coughing, cancellation, colds, man, car, convergence\n","[ 34 355 496 871  99 791 671 181 567 262]\n","topic 41 : refinement, dumping, education, crusaders, arrest, cementation, gravitational force, hardening, transmission, fungi\n","[ 567  570  347  789  724  883  109    0  852 1129]\n","topic 42 : transmission, distortion, allergies, impeachment, defects, Water, scratches, amazement, decomposition reaction, washing\n","[  75  101  736   93   11  849  449  782 1038  916]\n","topic 43 : laser, hematoma, clock, improvement, optimism, migrations, overflows, repetition, conquests, change\n","[640 788  51 757 797 451 998 694 924 941]\n","topic 44 : shortage, pollutant, militancy, Estrogen dominance, Production, burning, cancer, resignation, Pinworm, influenza\n","[907 156 759  61  75 231 204 915 477 451]\n","topic 45 : appetite, war, programmer, commerce, laser, raisin bread, breeding, earthquakes, rebellion, burning\n","[ 762  107 1074  979   57  555  850 1022 1099 1005]\n","topic 46 : carnage, cow, floodlights, pathogens, cigarettes, immunizations, loss, emission, singer, bugs\n","[498  51 994 758 516 112 849 423 688 941]\n","topic 47 : cocaine, militancy, asteroid, activity, air, teaching, migrations, waterjet, shoulder problems, influenza\n","[ 224  970 1035  619  404  495  394  720  871  262]\n","topic 48 : women, consumption, malfunctions, vocals, biography, alarm, spark, bleeding, crusaders, fungi\n","[857 684 797 863 998   2 665 805 187 430]\n","topic 49 : beverages, presentation, Production, drill, cancer, progress, pressures, tests, blackout, imbalance\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 30\n","[TRAIN] loss: 0.5620, 0.5620, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.29 s\n","[VALID] loss: 0.6027, 0.6027, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 31\n","[TRAIN] loss: 0.5590, 0.5590, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.24 s\n","[VALID] loss: 0.5858, 0.5858, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 32\n","[TRAIN] loss: 0.5549, 0.5549, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5889, 0.5889, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 33\n","[TRAIN] loss: 0.5528, 0.5528, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.5937, 0.5937, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 34\n","[TRAIN] loss: 0.5495, 0.5495, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5850, 0.5850, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 35\n","[TRAIN] loss: 0.5470, 0.5470, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5871, 0.5871, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 36\n","[TRAIN] loss: 0.5452, 0.5452, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5884, 0.5884, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 37\n","[TRAIN] loss: 0.5426, 0.5426, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5786, 0.5786, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 38\n","[TRAIN] loss: 0.5391, 0.5391, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5906, 0.5906, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 39\n","[TRAIN] loss: 0.5380, 0.5380, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.19 s\n","[VALID] loss: 0.5755, 0.5755, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.02 s\n","\n","\n","\n","======================================================================\n","Epoch 40\n","[TRAIN] loss: 0.5352, 0.5352, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.18 s\n","[VALID] loss: 0.5696, 0.5696, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 41\n","[TRAIN] loss: 0.5334, 0.5334, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5708, 0.5708, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 42\n","[TRAIN] loss: 0.5322, 0.5322, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5759, 0.5759, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 43\n","[TRAIN] loss: 0.5302, 0.5302, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5814, 0.5814, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 44\n","[TRAIN] loss: 0.5271, 0.5271, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5703, 0.5703, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 45\n","[TRAIN] loss: 0.5256, 0.5256, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5606, 0.5606, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 46\n","[TRAIN] loss: 0.5256, 0.5256, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.18 s\n","[VALID] loss: 0.5612, 0.5612, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 47\n","[TRAIN] loss: 0.5228, 0.5228, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.22 s\n","[VALID] loss: 0.5647, 0.5647, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 48\n","[TRAIN] loss: 0.5207, 0.5207, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5646, 0.5646, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 49\n","[TRAIN] loss: 0.5200, 0.5200, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5579, 0.5579, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 50\n","[TRAIN] loss: 0.5176, 0.5176, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5640, 0.5640, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 51\n","[TRAIN] loss: 0.5167, 0.5167, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5666, 0.5666, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 52\n","[TRAIN] loss: 0.5146, 0.5146, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5697, 0.5697, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 53\n","[TRAIN] loss: 0.5143, 0.5143, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5582, 0.5582, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 54\n","[TRAIN] loss: 0.5127, 0.5127, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5679, 0.5679, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 55\n","[TRAIN] loss: 0.5101, 0.5101, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5603, 0.5603, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 56\n","[TRAIN] loss: 0.5094, 0.5094, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5665, 0.5665, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 57\n","[TRAIN] loss: 0.5089, 0.5089, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5595, 0.5595, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 58\n","[TRAIN] loss: 0.5075, 0.5075, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5652, 0.5652, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 59\n","[TRAIN] loss: 0.5072, 0.5072, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5510, 0.5510, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[1090  845  910  688  219  432  632  576  941  582]\n","topic 0 : stress, Addiction, discoloration, shoulder problems, signal, Sadness, screen, scandals, influenza, deluge\n","[147 740 941 451 909 719 156  54 497 481]\n","topic 1 : bill, assassination, influenza, burning, pustules, Properties, war, flood, malfunction, competition\n","[1031  288  223  674  925  999  367  194  740  391]\n","topic 2 : ointment, Fatigue, meeting, thought, excess, invasions, deforestation, gash, assassination, motion\n","[1093  406 1108  400  120    1  538  832  652  142]\n","topic 3 : profit, inflammation, antibiotics, substance, wheels, rising unemployment rate, Convulsions, bills, Trauma, plants\n","[ 667  145  436  345  538  432  582 1019  296  770]\n","topic 4 : celebration, vaccum, boiling water, development, Convulsions, Sadness, deluge, cancellation, current, dams\n","[ 911  759  610  634  305  814  335  182 1024  770]\n","topic 5 : dispute, programmer, Osteoporosis, extinction, landslides, preservation, rashes, dehydration, particles, dams\n","[ 266  183  336 1104  248  451  315  963  753   72]\n","topic 6 : Toxoplasmosis, winds, compromises, hormone, distention, burning, alterations, lorries, intoxication, execution\n","[ 759  235 1044  548  636  170  526  721  352 1073]\n","topic 7 : programmer, landmines, darkness, emergency, species, press, burn, inhalation, threats, toxin\n","[ 588  374  920    8 1128  655 1032  576  998 1090]\n","topic 8 : Anger, morbidity, Alcohol, abuse, hormonal changes, heroin, discharge, scandals, cancer, stress\n","[1090  780  970  961  497    3  910  619  998 1128]\n","topic 9 : stress, intersection, consumption, hitting, malfunction, distress, discoloration, vocals, cancer, hormonal changes\n","[ 315  943 1008   10   95   96   35  181  235  171]\n","topic 10 : alterations, electrons, launch, grief, collision, bomb explosion, burns, hardening, landmines, hemiplegia\n","[ 127  293  145  824 1019  534  273  254  255  298]\n","topic 11 : sinus pressure, conditions, vaccum, thorns, cancellation, fighting, roughness, Shrinkage, cockroaches, Menopause\n","[351 286 110 671 723 791 832 980 653 947]\n","topic 12 : infection, Acne, wave, gravitational force, amplifier, cementation, bills, seismic event, plate, lack\n","[195 813 783 672 490 255 682 293 727 322]\n","topic 13 : message, explosive, soaps, indigestion, reading, cockroaches, movements, conditions, symptoms, infestation\n","[ 857  459  268  790  198  109  296 1109  789  817]\n","topic 14 : beverages, explosion, absorption, disability, visible light, scratches, current, tornado, impeachment, chime\n","[317 871 351 690 495 224 262 619 947 566]\n","topic 15 : Yoga, crusaders, infection, taxes, alarm, women, fungi, vocals, lack, flooding\n","[ 137   70 1065 1129  879  282 1096  109 1112  510]\n","topic 16 : events, Progress, Injury, washing, problems, arrests, pyrotechnics, scratches, colds, widgets\n","[1099  376  844  947  188  655  639  705 1032  566]\n","topic 17 : singer, pyrolysis, vehicle, lack, satisfaction, heroin, Blinking, sorrow, discharge, flooding\n","[1106  394  947  186  707  292  351  720  566  495]\n","topic 18 : diode, spark, lack, plague, Fats, hardship, infection, bleeding, flooding, alarm\n","[ 922  804  828  902  223 1007  581 1122  908  179]\n","topic 19 : atmospheric pressure, expenditure, debt, Weak ligaments, meeting, Earthquakes, praise, asthma, deficits, quenches\n","[624 613  15 893 238  87 932 969 209 952]\n","topic 20 : Electrolysis, markets, pseudolesion, electricity, Smoke, cancellations, corrosion, stroke, tsunamis, ceasefire\n","[1015  664  645  326  190  821  884  185  269 1076]\n","topic 21 : inspiration, surplus, fuels, revellers, Lymphedema, heart attack, acne breakouts, success, pressure, attacks\n","[ 278 1021  863 1064   87 1026  624  952  893  908]\n","topic 22 : infraction, drain, drill, smell, cancellations, havoc, Electrolysis, ceasefire, electricity, deficits\n","[ 131  317  970 1056  296  738  530 1030  432   87]\n","topic 23 : leak, Yoga, consumption, legs, current, sun, debate, dancing, Sadness, cancellations\n","[  20  206  652 1109  452  770   21  749  530 1056]\n","topic 24 : rock, passage, Trauma, tornado, candle, dams, consumers, chord, debate, legs\n","[ 536  768 1099  639  496  274   45  738  351  720]\n","topic 25 : smiling, terror, singer, Blinking, education, activation, rainfall, sun, infection, bleeding\n","[ 576  998  967  452 1032    8  242  688   46  345]\n","topic 26 : scandals, cancer, source, candle, discharge, abuse, cavity, shoulder problems, cyclone, development\n","[959 949 662 266 496 209 351 394 495 822]\n","topic 27 : dandruff, imbalances, happiness, Toxoplasmosis, education, tsunamis, infection, spark, alarm, conflicts\n","[845 750  95 820 576 601 206 655 632 886]\n","topic 28 : Addiction, missile, collision, indicator, scandals, presence, passage, heroin, screen, applicants\n","[ 759 1065  857   12  849  730  917  423  842  789]\n","topic 29 : programmer, Injury, beverages, swinging, migrations, spiciness, failures, waterjet, anxiety, impeachment\n","[651 869 698 430 386   7  88 631 121 805]\n","topic 30 : helicobacter, fume, effect, imbalance, terrorist, floodwaters, opposition, silversmiths, car, tests\n","[  49  952  710  209 1096  719  419   78 1036 1104]\n","topic 31 : resources, ceasefire, clergy, tsunamis, pyrotechnics, Properties, increased pressure, Conflict, child abuse, hormone\n","[451 287 998 820 867 506 548 259 352 315]\n","topic 32 : burning, tears, cancer, indicator, dust, frenzy, emergency, neglect, threats, alterations\n","[ 353  377  189  662  306  224 1106  496  566  947]\n","topic 33 : ginseng, intubation, strike, happiness, tingling, women, diode, education, flooding, lack\n","[ 339  233   35  254  269 1028  745  687   24 1077]\n","topic 34 : infertility, flight, burns, Shrinkage, pressure, cell phones, people, evaporation, princess, strategies\n","[ 795  963  209 1039  238  384  318  960  855  183]\n","topic 35 : flammable liquids, lorries, tsunamis, withdrawal, Smoke, Dizziness, laughing, demise, drum, winds\n","[ 735  700  721  958 1008  541  790  911  447 1109]\n","topic 36 : alibi, Overpopulation, inhalation, gases, launch, tree, disability, dispute, helping, tornado\n","[ 961 1062  780  327  209 1058  589  372  910    8]\n","topic 37 : hitting, overheating, intersection, seas, tsunamis, aggressions, meningitis, temperature, discoloration, abuse\n","[195 523 538 183 682 895 770 813 526  63]\n","topic 38 : message, smile, Convulsions, winds, movements, process, dams, explosive, burn, proof\n","[807 259 831 843 121 732 911 194 430 301]\n","topic 39 : staring effect, neglect, resonance, injury, car, patriots, dispute, gash, imbalance, termination\n","[ 986  275  908  639  631  798  932  625  773 1112]\n","topic 40 : irritation, heat, deficits, Blinking, silversmiths, influx, corrosion, man, effects, colds\n","[776  14 947 409 355 566 306 707 567 871]\n","topic 41 : Vulvodynia, displacement, lack, power outage, dumping, flooding, tingling, Fats, transmission, crusaders\n","[ 263  704  843  476  377  804  735  109  790 1129]\n","topic 42 : lorry, pollution, injury, Entrepreneurship, intubation, expenditure, alibi, scratches, disability, washing\n","[ 500  112  323  177  287   11  870  211  849 1096]\n","topic 43 : disturbances, teaching, treatment, refusal, tears, optimism, Outbreaks, storms, migrations, pyrotechnics\n","[ 757  115  506  903 1118  655  588 1054  857  941]\n","topic 44 : Estrogen dominance, germs, frenzy, relaxation, work, heroin, Anger, boom, beverages, influenza\n","[269 252 788 757  95 309  35 341 530 352]\n","topic 45 : pressure, scandal, pollutant, Estrogen dominance, collision, tumor, burns, suffocation, debate, threats\n","[ 480 1099  795  304   50  546  910  450  653  275]\n","topic 46 : computers, singer, flammable liquids, Constipation, dwarf, crack, discoloration, policies, plate, heat\n","[960 663 156 423 661 112 994 857 903 849]\n","topic 47 : demise, pain killers, war, waterjet, kidnapping, teaching, asteroid, beverages, relaxation, migrations\n","[ 495  947 1035  170  619  871  970  690  941  409]\n","topic 48 : alarm, lack, malfunctions, press, vocals, crusaders, consumption, taxes, influenza, power outage\n","[846 857 863 941 998 688 674 187  88 210]\n","topic 49 : pie, beverages, drill, influenza, cancer, shoulder problems, thought, blackout, opposition, vortex-densities\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 60\n","[TRAIN] loss: 0.5049, 0.5049, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.11 s\n","[VALID] loss: 0.5584, 0.5584, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 61\n","[TRAIN] loss: 0.5044, 0.5044, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5587, 0.5587, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 62\n","[TRAIN] loss: 0.5027, 0.5027, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5538, 0.5538, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 63\n","[TRAIN] loss: 0.5017, 0.5017, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5386, 0.5386, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 64\n","[TRAIN] loss: 0.5016, 0.5016, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5501, 0.5501, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 65\n","[TRAIN] loss: 0.5003, 0.5003, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5417, 0.5417, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 66\n","[TRAIN] loss: 0.4996, 0.4996, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5444, 0.5444, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 67\n","[TRAIN] loss: 0.4981, 0.4981, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5550, 0.5550, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 68\n","[TRAIN] loss: 0.4959, 0.4959, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5426, 0.5426, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 69\n","[TRAIN] loss: 0.4957, 0.4957, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5461, 0.5461, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 70\n","[TRAIN] loss: 0.4948, 0.4948, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5580, 0.5580, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 71\n","[TRAIN] loss: 0.4950, 0.4950, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5303, 0.5303, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 72\n","[TRAIN] loss: 0.4940, 0.4940, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5509, 0.5509, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 73\n","[TRAIN] loss: 0.4921, 0.4921, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5409, 0.5409, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 74\n","[TRAIN] loss: 0.4927, 0.4927, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5581, 0.5581, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 75\n","[TRAIN] loss: 0.4913, 0.4913, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5445, 0.5445, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 76\n","[TRAIN] loss: 0.4895, 0.4895, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5391, 0.5391, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 77\n","[TRAIN] loss: 0.4901, 0.4901, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5325, 0.5325, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 78\n","[TRAIN] loss: 0.4882, 0.4882, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5515, 0.5515, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 79\n","[TRAIN] loss: 0.4880, 0.4880, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5480, 0.5480, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 80\n","[TRAIN] loss: 0.4876, 0.4876, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5362, 0.5362, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 81\n","[TRAIN] loss: 0.4851, 0.4851, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5373, 0.5373, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 82\n","[TRAIN] loss: 0.4850, 0.4850, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5317, 0.5317, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 83\n","[TRAIN] loss: 0.4845, 0.4845, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5346, 0.5346, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 84\n","[TRAIN] loss: 0.4838, 0.4838, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5438, 0.5438, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 85\n","[TRAIN] loss: 0.4833, 0.4833, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5288, 0.5288, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 86\n","[TRAIN] loss: 0.4829, 0.4829, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5388, 0.5388, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 87\n","[TRAIN] loss: 0.4817, 0.4817, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5331, 0.5331, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 88\n","[TRAIN] loss: 0.4808, 0.4808, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5296, 0.5296, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 89\n","[TRAIN] loss: 0.4803, 0.4803, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5379, 0.5379, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[582 372 941 296 890 178 778  95 846 432]\n","topic 0 : deluge, temperature, influenza, current, shingles, revenue, experiences, collision, pie, Sadness\n","[ 633  108  706  202  689  156  308   40 1077  315]\n","topic 1 : science, crashes, tugging, arterial blood pressure, amphibolites, war, driver, bite, strategies, alterations\n","[ 398  194 1026  968  799  372  418  899 1032  740]\n","topic 2 : affect, gash, havoc, failure, cycling, temperature, deregulation, giving, discharge, assassination\n","[ 142    1  231   95  342   88  406  400  988 1093]\n","topic 3 : plants, rising unemployment rate, raisin bread, collision, cars, opposition, inflammation, substance, transmitter, profit\n","[268 714 853 145 254 432  65 780 293 770]\n","topic 4 : absorption, hunting, Job dissatisfaction, vaccum, Shrinkage, Sadness, drainage, intersection, conditions, dams\n","[352 442 843 750 935 335 471 387 610 337]\n","topic 5 : threats, figure, injury, missile, beam, rashes, Scleroderma, insecurity, Osteoporosis, circulation\n","[ 315   50  795  736  677  336 1022  501 1009  266]\n","topic 6 : alterations, dwarf, flammable liquids, clock, chemotherapy, compromises, emission, design, abdominal pain, Toxoplasmosis\n","[ 444 1067  526  385  465  953 1093  284  342  727]\n","topic 7 : movement, exhibition, burn, preservatives, concern, band, profit, vibration, cars, symptoms\n","[  94  576  589  910  947  350 1032   13  100 1128]\n","topic 8 : Birth defects, scandals, meningitis, discoloration, lack, Inhibition, discharge, delusions, debris, hormonal changes\n","[ 998  262  576  846  970  538  497  910  619 1128]\n","topic 9 : cancer, fungi, scandals, pie, consumption, Convulsions, malfunction, discoloration, vocals, hormonal changes\n","[ 554   60  676  400  937   50 1077   58  943  150]\n","topic 10 : jaw problem, fever, light, substance, laws, dwarf, strategies, overflow, electrons, reduction\n","[ 298 1024  534  432  815  749  293  255  206  195]\n","topic 11 : Menopause, particles, fighting, Sadness, loop, chord, conditions, cockroaches, passage, message\n","[ 947  245  567 1111   24  848   96   52  455 1047]\n","topic 12 : lack, deterioration, transmission, eruption, princess, discomfort, bomb explosion, interference, miscarriages, extraction\n","[ 20 949 602 871 910 669 563 141 495 425]\n","topic 13 : rock, imbalances, election, crusaders, discoloration, memory loss, bang, bacterium, alarm, War\n","[298 401 443 476 316 459 917 883 817 534]\n","topic 14 : Menopause, anesthetic, excitement, Entrepreneurship, nerves, explosion, failures, Water, chime, fighting\n","[681 239 566 947 245 197 988 262 355 567]\n","topic 15 : jolliness, corruption, flooding, lack, deterioration, movie, transmitter, fungi, dumping, transmission\n","[ 937  405  137 1028  679  524 1049  986  368  258]\n","topic 16 : laws, birthmarks, events, cell phones, hole, decrease, mold, irritation, eating, tendinitis\n","[  68  350   58   66  932 1098  286  919 1032  589]\n","topic 17 : streaks, Inhibition, overflow, threat, corrosion, afterglow, Acne, devastations, discharge, meningitis\n","[1100  368  495  959  473  947  496  701  425  932]\n","topic 18 : immigration, eating, alarm, dandruff, drug use, lack, education, applicability, War, corrosion\n","[ 68 530 804 401 195 654 577 995 908 713]\n","topic 19 : streaks, debate, expenditure, anesthetic, message, insomnia, rainbow, constipation, deficits, weather\n","[ 229  117  246  209  372  178  702  296 1086  619]\n","topic 20 : vaccines, changes, sidewalk, tsunamis, temperature, revenue, attack, current, separation field, vocals\n","[ 544   73  326 1015  751 1105  106  854  734  645]\n","topic 21 : tardiness, losses, revellers, inspiration, sound, compromise, cold, acceleration, eyelids, fuels\n","[131 699 749 471 733 206 543 432  23 582]\n","topic 22 : leak, virus, chord, Scleroderma, disorders, passage, reboot, Sadness, pinkeye, deluge\n","[372 427 685 825 958 119 819 834 970  87]\n","topic 23 : temperature, Asthma, gun, candida, gases, Discomfort, structure, humidity, consumption, cancellations\n","[ 206 1024   21  105  895  778  749  458  195  264]\n","topic 24 : passage, particles, consumers, rains, process, experiences, chord, Germs, message, firing\n","[ 108   45  239  875 1098 1052   52 1058  351  190]\n","topic 25 : crashes, rainfall, corruption, filament, afterglow, Sinusitis, interference, aggressions, infection, Lymphedema\n","[ 890  716  852   65   46  542 1010  807  350  843]\n","topic 26 : shingles, concerto, decomposition reaction, drainage, cyclone, mobility, humiliation, staring effect, Inhibition, injury\n","[ 381  959  947  209  880 1014   50  262  473  266]\n","topic 27 : eyestrain, dandruff, lack, tsunamis, unconsciousness, pop, dwarf, fungi, drug use, Toxoplasmosis\n","[ 95 895 886 432 824 283 530 206 458 246]\n","topic 28 : collision, process, applicants, Sadness, thorns, burnt food, debate, passage, Germs, sidewalk\n","[ 308  772  205  534  116  986 1112  883  917  498]\n","topic 29 : driver, comet, diarrhea, fighting, negotiations, irritation, colds, Water, failures, cocaine\n","[430 952 202  72 584 543 465 715 121 805]\n","topic 30 : imbalance, ceasefire, arterial blood pressure, execution, pills, reboot, concern, bacteria, car, tests\n","[ 402 1015 1076  686  121  280 1054  481  719 1096]\n","topic 31 : burrows, inspiration, attacks, properties, car, Preeclampsia, boom, competition, Properties, pyrotechnics\n","[ 852  663  477  807  385  689  352  315 1077  259]\n","topic 32 : decomposition reaction, pain killers, rebellion, staring effect, preservatives, amphibolites, threats, alterations, strategies, neglect\n","[ 496  880  360  662  224  567  355 1106  409  262]\n","topic 33 : education, unconsciousness, suicide, happiness, women, transmission, dumping, diode, power outage, fungi\n","[ 234 1028  515  233  829   73  621   96 1077  906]\n","topic 34 : documentary, cell phones, rash, flight, impairment, losses, caffeine withdrawal, bomb explosion, strategies, meltdown\n","[1022   69  168  559  293  850 1062  314  847  540]\n","topic 35 : emission, perturbation, expansions, acupuncture, conditions, loss, overheating, Interpolation, factories, palpitations\n","[935 843 901 577 685 958 435 807 713  29]\n","topic 36 : beam, injury, frustration, rainbow, gun, gases, waste, staring effect, weather, producer\n","[495 641 350 765 798 100 372 603 640 497]\n","topic 37 : alarm, Famine, Inhibition, masts, influx, debris, temperature, shrinkage, shortage, malfunction\n","[404 961 195  63 183 293 264 912 338 526]\n","topic 38 : biography, hitting, message, proof, winds, conditions, firing, Bed sores, maker, burn\n","[935 665 194 877 831 584 648 121 843 807]\n","topic 39 : beam, pressures, gash, impact, resonance, pills, impression, car, injury, staring effect\n","[  68  778   71  368  257  205  260 1112  733 1110]\n","topic 40 : streaks, experiences, lamp, eating, construction, diarrhea, recession, colds, disorders, rill\n","[ 443  262  566 1112  848  937  484  376  567  355]\n","topic 41 : excitement, fungi, flooding, colds, discomfort, laws, person, pyrolysis, transmission, dumping\n","[ 43 584 211 735 843 405 790 883 137 298]\n","topic 42 : flashlight, pills, storms, alibi, injury, birthmarks, disability, Water, events, Menopause\n","[ 101  500 1085  623  575  156   16  112  849 1096]\n","topic 43 : hematoma, disturbances, lymphomas, aftershocks, snarl, war, emigration, teaching, migrations, pyrotechnics\n","[ 797  632  846  903  638  994 1036 1054   51  857]\n","topic 44 : Production, screen, pie, relaxation, harassment, asteroid, child abuse, boom, militancy, beverages\n","[ 883  442  352 1107  156  729  213  205  653 1121]\n","topic 45 : Water, figure, threats, hypertension, war, dogs, algorithm, diarrhea, plate, manakin\n","[ 795  555  910   93  111  395  275 1022  100  258]\n","topic 46 : flammable liquids, immunizations, discoloration, improvement, glands, developer, heat, emission, debris, tendinitis\n","[1078  432  156  382  688  112  471  383  308  498]\n","topic 47 : reasons, Sadness, war, bout, shoulder problems, teaching, Scleroderma, awareness, driver, cocaine\n","[ 566  260  727 1093  262  970  910  409  802  988]\n","topic 48 : flooding, recession, symptoms, profit, fungi, consumption, discoloration, power outage, massacre, transmitter\n","[712 941 841 863 674 846 805 187 121  88]\n","topic 49 : flows, influenza, delay, drill, thought, pie, tests, blackout, car, opposition\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 90\n","[TRAIN] loss: 0.4793, 0.4793, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5381, 0.5381, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 91\n","[TRAIN] loss: 0.4794, 0.4794, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5411, 0.5411, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 92\n","[TRAIN] loss: 0.4788, 0.4788, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5320, 0.5320, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 93\n","[TRAIN] loss: 0.4768, 0.4768, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5311, 0.5311, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 94\n","[TRAIN] loss: 0.4763, 0.4763, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5311, 0.5311, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 95\n","[TRAIN] loss: 0.4762, 0.4762, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5290, 0.5290, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 96\n","[TRAIN] loss: 0.4763, 0.4763, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5348, 0.5348, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 97\n","[TRAIN] loss: 0.4750, 0.4750, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5333, 0.5333, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 98\n","[TRAIN] loss: 0.4743, 0.4743, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5338, 0.5338, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 99\n","[TRAIN] loss: 0.4734, 0.4734, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5253, 0.5253, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 100\n","[TRAIN] loss: 0.4721, 0.4721, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5281, 0.5281, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 101\n","[TRAIN] loss: 0.4719, 0.4719, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5217, 0.5217, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 102\n","[TRAIN] loss: 0.4713, 0.4713, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5200, 0.5200, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 103\n","[TRAIN] loss: 0.4725, 0.4725, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5336, 0.5336, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 104\n","[TRAIN] loss: 0.4715, 0.4715, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5181, 0.5181, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 105\n","[TRAIN] loss: 0.4695, 0.4695, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5417, 0.5417, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 106\n","[TRAIN] loss: 0.4707, 0.4707, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5341, 0.5341, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 107\n","[TRAIN] loss: 0.4708, 0.4708, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5423, 0.5423, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 108\n","[TRAIN] loss: 0.4693, 0.4693, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5367, 0.5367, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 109\n","[TRAIN] loss: 0.4674, 0.4674, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5257, 0.5257, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 110\n","[TRAIN] loss: 0.4673, 0.4673, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5242, 0.5242, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 111\n","[TRAIN] loss: 0.4670, 0.4670, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5310, 0.5310, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 112\n","[TRAIN] loss: 0.4671, 0.4671, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5176, 0.5176, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 113\n","[TRAIN] loss: 0.4668, 0.4668, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5347, 0.5347, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 114\n","[TRAIN] loss: 0.4651, 0.4651, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5211, 0.5211, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 115\n","[TRAIN] loss: 0.4654, 0.4654, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5192, 0.5192, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 116\n","[TRAIN] loss: 0.4641, 0.4641, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.18 s\n","[VALID] loss: 0.5148, 0.5148, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 117\n","[TRAIN] loss: 0.4633, 0.4633, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.13 s\n","[VALID] loss: 0.5187, 0.5187, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 118\n","[TRAIN] loss: 0.4633, 0.4633, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5295, 0.5295, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 119\n","[TRAIN] loss: 0.4624, 0.4624, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.19 s\n","[VALID] loss: 0.5334, 0.5334, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","Topics with probability argmax\n","50\n","50\n","[ 202 1090  576  797  846  910  890  970  432  372]\n","topic 0 : arterial blood pressure, stress, scandals, Production, pie, discoloration, shingles, consumption, Sadness, temperature\n","[ 280 1088    5  408  719  689   40  633  146  638]\n","topic 1 : Preeclampsia, humans, laughter, nicotine, Properties, amphibolites, bite, science, germ, harassment\n","[ 316  967 1061  640  999   31  588 1051  510  509]\n","topic 2 : nerves, source, Sodium, shortage, invasions, evacuation, Anger, tip, widgets, lightning\n","[ 675  181 1056  237   66  120  652    1  406 1093]\n","topic 3 : books, hardening, legs, erosion, threat, wheels, Trauma, rising unemployment rate, inflammation, profit\n","[ 422  255  534  902  438  770 1118 1019  749  293]\n","topic 4 : condition, cockroaches, fighting, Weak ligaments, toll, dams, work, cancellation, chord, conditions\n","[807 895 759 387 843 264 548 281 936 354]\n","topic 5 : staring effect, process, programmer, insecurity, injury, firing, emergency, choking, exposure, sinking\n","[ 850  266   78   91  736 1044  315  511  554  501]\n","topic 6 : loss, Toxoplasmosis, Conflict, blasts, clock, darkness, alterations, Speeding, jaw problem, design\n","[998 759 120 593  66 832 665 385 204 352]\n","topic 7 : cancer, programmer, wheels, interest, threat, bills, pressures, preservatives, breeding, threats\n","[ 482  970  765  506  434  998   66  100   94 1128]\n","topic 8 : hurricanes, consumption, masts, frenzy, mistake, cancer, threat, debris, Birth defects, hormonal changes\n","[ 727  780  576  998  961  910  497  619 1090  970]\n","topic 9 : symptoms, intersection, scandals, cancer, hitting, discoloration, malfunction, vocals, stress, consumption\n","[ 647  562 1109  878  495  462   50  380  445 1113]\n","topic 10 : command, perfume, tornado, Fog, alarm, cancers, dwarf, disease, circumstances, anger\n","[206 547 644 770 701 534 255 749 293 195]\n","topic 11 : passage, device, software, dams, applicability, fighting, cockroaches, chord, conditions, message\n","[ 987  279 1094  286  780  689  317  638   96  745]\n","topic 12 : revenues, ingestion, collapse, Acne, intersection, amphibolites, Yoga, harassment, bomb explosion, people\n","[503 783 447 815 408 947 119 731 422 685]\n","topic 13 : Malaria, soaps, helping, loop, nicotine, lack, Discomfort, itchiness, condition, gun\n","[1109  405  286  476  268  883  202  652  534 1123]\n","topic 14 : tornado, birthmarks, Acne, Entrepreneurship, absorption, Water, arterial blood pressure, Trauma, fighting, crash\n","[484 961 593 791 355 872 170 224 317  66]\n","topic 15 : person, hitting, interest, cementation, dumping, Cities, press, women, Yoga, threat\n","[ 282  315  428  286  668 1049 1112  509   93  986]\n","topic 16 : arrests, alterations, colouration, Acne, radiation, mold, colds, lightning, improvement, irritation\n","[ 887  241 1099  480 1063  723  848  878   99 1032]\n","topic 17 : Poverty, Exposure, singer, computers, enzymes, amplifier, discomfort, Fog, arrest, discharge\n","[1041  189   66  292  880  725  701  425  376  495]\n","topic 18 : illnesses, strike, threat, hardship, unconsciousness, redistribution, applicability, War, pyrolysis, alarm\n","[167  71 639 900 141 696 624 179 922 875]\n","topic 19 : mess, lamp, Blinking, earthquake, bacterium, fleas, Electrolysis, quenches, atmospheric pressure, filament\n","[229 559 882 803 538 209  72  23 422 969]\n","topic 20 : vaccines, acupuncture, risk, media, Convulsions, tsunamis, execution, pinkeye, condition, stroke\n","[ 972  333  544  341 1036  734  321  645  190  269]\n","topic 21 : incubation, ecstasy, tardiness, suffocation, child abuse, eyelids, Discussion, fuels, Lymphedema, pressure\n","[162  69 543 131 432 202   2  23 969 863]\n","topic 22 : Oscillation, perturbation, reboot, leak, Sadness, arterial blood pressure, progress, pinkeye, stroke, drill\n","[ 988 1109  321  356  380  406  970  652  396  427]\n","topic 23 : transmitter, tornado, Discussion, tone, disease, inflammation, consumption, Trauma, wildfires, Asthma\n","[ 514  713  380  246  356  206   94  699  281 1109]\n","topic 24 : shooting, weather, disease, sidewalk, tone, passage, Birth defects, virus, choking, tornado\n","[ 404  949 1025  496  959  639 1027 1080  250  510]\n","topic 25 : biography, imbalances, steam, education, dandruff, Blinking, watering, Cardiovascular disease, longings, widgets\n","[145 327 846  94 242 548 482 888 998 576]\n","topic 26 : vaccum, seas, pie, Birth defects, cavity, emergency, hurricanes, clot, cancer, scandals\n","[  60  772 1100  880 1041  521  248  725 1106  495]\n","topic 27 : fever, comet, immigration, unconsciousness, illnesses, eruptions, distention, redistribution, diode, alarm\n","[888 749 114 138 221 601 846 890 982 206]\n","topic 28 : clot, chord, disruption, seismic arrival, scars, presence, pie, shingles, water, passage\n","[ 127  368  201  960  609  569 1121  534  917  423]\n","topic 29 : sinus pressure, eating, physician, demise, poisoning, proposal, manakin, fighting, failures, waterjet\n","[ 121  674  631 1013  805  527    7  828   17  733]\n","topic 30 : car, thought, silversmiths, speculation, tests, diabetes, floodwaters, debt, convergence, disorders\n","[ 516  177   40  972   51  202   23 1096  719  633]\n","topic 31 : air, refusal, bite, incubation, militancy, arterial blood pressure, pinkeye, pyrotechnics, Properties, science\n","[870 451 548 207 204 852 759 315 820 889]\n","topic 32 : Outbreaks, burning, emergency, ache, breeding, decomposition reaction, programmer, alterations, indicator, recurrence\n","[947  43 396 791 170 260  66 880 224 495]\n","topic 33 : lack, flashlight, wildfires, cementation, press, recession, threat, unconsciousness, women, alarm\n","[ 562 1061  135  745   84  358   35  687  906  233]\n","topic 34 : perfume, Sodium, institutions, people, acne, confidence, burns, evaporation, meltdown, flight\n","[449  92 493 432   2 358 850 733 543 501]\n","topic 35 : overflows, noises, breakthroughs, Sadness, progress, confidence, loss, disorders, reboot, design\n","[447 935 713 790 777 685 577 377 807   7]\n","topic 36 : helping, beam, weather, disability, viruses, gun, rainbow, intubation, staring effect, floodwaters\n","[ 845  434  588  640  826  324  100 1032  576  910]\n","topic 37 : Addiction, mistake, Anger, shortage, drugs, proliferation, debris, discharge, scandals, discoloration\n","[338 438 895 950 195 422 872  72 912 988]\n","topic 38 : maker, toll, process, Dehydration, message, condition, Cities, execution, Bed sores, transmitter\n","[ 843  852  323 1096  713  665  805  790  831  678]\n","topic 39 : injury, decomposition reaction, treatment, pyrotechnics, weather, pressures, tests, disability, resonance, behaviour\n","[ 868  865  882   68  537  380  773  639 1110 1112]\n","topic 40 : action, information, risk, streaks, suffering, disease, effects, Blinking, rill, colds\n","[ 389  376  428  353  224  484  937 1112  317  409]\n","topic 41 : buckling, pyrolysis, colouration, ginseng, women, person, laws, colds, Yoga, power outage\n","[1112  468  883  137  224 1129   43  735  791  678]\n","topic 42 : colds, mortality, Water, events, women, washing, flashlight, alibi, cementation, behaviour\n","[ 849  177  575  287  665  449  870  315  211 1096]\n","topic 43 : migrations, refusal, snarl, tears, pressures, overflows, Outbreaks, alterations, storms, pyrotechnics\n","[ 663 1054  151  854  846  759   51  477  458  588]\n","topic 44 : pain killers, boom, ulcer, acceleration, pie, programmer, militancy, rebellion, Germs, Anger\n","[1121  269  788  751  309   83  341  423   68  442]\n","topic 45 : manakin, pressure, pollutant, sound, tumor, hull, suffocation, waterjet, streaks, figure\n","[ 487   93  395  100  258  910  480  728 1014  229]\n","topic 46 : drinks, improvement, developer, debris, tendinitis, discoloration, computers, parasite, pop, vaccines\n","[ 498  558  112  753  633  289 1076  311  688  281]\n","topic 47 : cocaine, machine, teaching, intoxication, science, lavas, attacks, power, shoulder problems, choking\n","[690 846 998 566 432 484 872 961   1 970]\n","topic 48 : taxes, pie, cancer, flooding, Sadness, person, Cities, hitting, rising unemployment rate, consumption\n","[430 846 674 367 187 576 296 805 970  88]\n","topic 49 : imbalance, pie, thought, deforestation, blackout, scandals, current, tests, consumption, opposition\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 120\n","[TRAIN] loss: 0.4626, 0.4626, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.27 s\n","[VALID] loss: 0.5319, 0.5319, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 121\n","[TRAIN] loss: 0.4610, 0.4610, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5294, 0.5294, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 122\n","[TRAIN] loss: 0.4617, 0.4617, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.20 s\n","[VALID] loss: 0.5150, 0.5150, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 123\n","[TRAIN] loss: 0.4614, 0.4614, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5135, 0.5135, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 124\n","[TRAIN] loss: 0.4602, 0.4602, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.17 s\n","[VALID] loss: 0.5211, 0.5211, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 125\n","[TRAIN] loss: 0.4606, 0.4606, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.16 s\n","[VALID] loss: 0.5306, 0.5306, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 126\n","[TRAIN] loss: 0.4593, 0.4593, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.20 s\n","[VALID] loss: 0.5150, 0.5150, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 127\n","[TRAIN] loss: 0.4587, 0.4587, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5284, 0.5284, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 128\n","[TRAIN] loss: 0.4601, 0.4601, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.18 s\n","[VALID] loss: 0.5212, 0.5212, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 129\n","[TRAIN] loss: 0.4589, 0.4589, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.14 s\n","[VALID] loss: 0.5102, 0.5102, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 130\n","[TRAIN] loss: 0.4595, 0.4595, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.23 s\n","[VALID] loss: 0.5326, 0.5326, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 131\n","[TRAIN] loss: 0.4573, 0.4573, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.16 s\n","[VALID] loss: 0.5110, 0.5110, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 132\n","[TRAIN] loss: 0.4579, 0.4579, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.22 s\n","[VALID] loss: 0.5162, 0.5162, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 133\n","[TRAIN] loss: 0.4560, 0.4560, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.15 s\n","[VALID] loss: 0.5215, 0.5215, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 134\n","[TRAIN] loss: 0.4569, 0.4569, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5196, 0.5196, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 135\n","[TRAIN] loss: 0.4554, 0.4554, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5131, 0.5131, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 136\n","[TRAIN] loss: 0.4564, 0.4564, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5095, 0.5095, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 137\n","[TRAIN] loss: 0.4548, 0.4548, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5286, 0.5286, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 138\n","[TRAIN] loss: 0.4552, 0.4552, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5130, 0.5130, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 139\n","[TRAIN] loss: 0.4536, 0.4536, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5156, 0.5156, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 140\n","[TRAIN] loss: 0.4527, 0.4527, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5095, 0.5095, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 141\n","[TRAIN] loss: 0.4527, 0.4527, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5074, 0.5074, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 142\n","[TRAIN] loss: 0.4529, 0.4529, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5176, 0.5176, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 143\n","[TRAIN] loss: 0.4525, 0.4525, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5141, 0.5141, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 144\n","[TRAIN] loss: 0.4521, 0.4521, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5063, 0.5063, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 145\n","[TRAIN] loss: 0.4517, 0.4517, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5072, 0.5072, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 146\n","[TRAIN] loss: 0.4515, 0.4515, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5254, 0.5254, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 147\n","[TRAIN] loss: 0.4509, 0.4509, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.4993, 0.4993, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 148\n","[TRAIN] loss: 0.4508, 0.4508, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5082, 0.5082, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 149\n","[TRAIN] loss: 0.4505, 0.4505, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5211, 0.5211, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[863 582 688 970 740 910 372 251 882 432]\n","topic 0 : drill, deluge, shoulder problems, consumption, assassination, discoloration, temperature, rubbing, risk, Sadness\n","[ 79 146 759 637  93 580 756 903  40 904]\n","topic 1 : hormonal imbalance, germ, programmer, blaze, improvement, sensitization, anxiety reaction, relaxation, bite, braces\n","[628 100 640 925 722 350 999 917 899   8]\n","topic 2 : accumulation, debris, shortage, excess, rupture, Inhibition, invasions, failures, giving, abuse\n","[ 278  652  315  170    1  749  872  665  727 1093]\n","topic 3 : infraction, Trauma, alterations, press, rising unemployment rate, chord, Cities, pressures, symptoms, profit\n","[ 292  145  439  886 1019  251  305  770 1117  293]\n","topic 4 : hardship, vaccum, sizzle, applicants, cancellation, rubbing, landslides, dams, defeat, conditions\n","[1068  141  759  843  477  634 1024  922   21  281]\n","topic 5 : calm, bacterium, programmer, injury, rebellion, extinction, particles, atmospheric pressure, consumers, choking\n","[  72  847 1077 1014 1121  304  487  315  501  545]\n","topic 6 : execution, factories, strategies, pop, manakin, Constipation, drinks, alterations, design, backpressure\n","[1111  790  103  511 1093  170  352  385  636  526]\n","topic 7 : eruption, disability, breakdown, Speeding, profit, press, threats, preservatives, species, burn\n","[374 947 588 251 851 655 665 100 495 110]\n","topic 8 : morbidity, lack, Anger, rubbing, increase, heroin, pressures, debris, alarm, wave\n","[ 998    3  434  497  797  961  640  872 1128  910]\n","topic 9 : cancer, distress, mistake, malfunction, Production, hitting, shortage, Cities, hormonal changes, discoloration\n","[1092   57   50  404  400  627  703  593  562 1077]\n","topic 10 : mentality, cigarettes, dwarf, biography, substance, tension, hinge, interest, perfume, strategies\n","[145 895 770 195 824 995 503 749 962 255]\n","topic 11 : vaccum, process, dams, message, thorns, constipation, Malaria, chord, slowdown, cockroaches\n","[ 462  566 1111   40  189  259  496   24  745   96]\n","topic 12 : cancers, flooding, eruption, bite, strike, neglect, education, princess, people, bomb explosion\n","[ 419  360  351 1008  496  113  871   59  339  865]\n","topic 13 : increased pressure, suicide, infection, launch, education, sadness, crusaders, dizziness, infertility, information\n","[ 984  389  223  268 1035   70  789  962  202  476]\n","topic 14 : chemicals, buckling, meeting, absorption, malfunctions, Progress, impeachment, slowdown, arterial blood pressure, Entrepreneurship\n","[495 681 872 170 355 455 962 947  10 566]\n","topic 15 : alarm, jolliness, Cities, press, dumping, miscarriages, slowdown, lack, grief, flooding\n","[1077  937  306 1121  810  258   46   70  691  282]\n","topic 16 : strategies, laws, tingling, manakin, victory, tendinitis, cyclone, Progress, weathering, arrests\n","[ 566 1100 1074  310  376 1027  463  189  368  123]\n","topic 17 : flooding, immigration, floodlights, Zinc, pyrolysis, watering, breezes, strike, eating, range\n","[ 455  425  286  701 1033  566  292  473  495  496]\n","topic 18 : miscarriages, War, Acne, applicability, rendition, flooding, hardship, drug use, alarm, education\n","[785  83 803 183  70 839 179 827 141 654]\n","topic 19 : vibrations, hull, media, winds, Progress, acid, quenches, shock waves, bacterium, insomnia\n","[ 538  690  765  351  422  872 1104   39  209  432]\n","topic 20 : Convulsions, taxes, masts, infection, condition, Cities, hormone, corkscrew, tsunamis, Sadness\n","[269 645  73 533 884 341 515 333 190 544]\n","topic 21 : pressure, fuels, losses, stitches, acne breakouts, suffocation, rash, ecstasy, Lymphedema, tardiness\n","[654 179 595 733 346 246 624  23 631 969]\n","topic 22 : insomnia, quenches, moisture, disorders, coughing, sidewalk, Electrolysis, pinkeye, silversmiths, stroke\n","[  21  549   87 1014 1011  271  778  438  652 1008]\n","topic 23 : consumers, skin abnormality, cancellations, pop, field, airline, experiences, toll, Trauma, launch\n","[ 307  264  790  749  549  770 1056 1109   21  778]\n","topic 24 : worsening, firing, disability, chord, skin abnormality, dams, legs, tornado, consumers, experiences\n","[178 470 594 595 310 174  85 689 893 763]\n","topic 25 : revenue, pain, disappointment, moisture, Zinc, injection, feathers, amphibolites, electricity, outbreak\n","[703  22 688 327 576  46   8 251 530 998]\n","topic 26 : hinge, tides, shoulder problems, seas, scandals, cyclone, abuse, rubbing, debate, cancer\n","[ 276  153 1033  263  972  473 1041  353 1077  292]\n","topic 27 : fortune, addition, rendition, lorry, incubation, drug use, illnesses, ginseng, strategies, hardship\n","[503   1 778 246 807 251 889 749 970 886]\n","topic 28 : Malaria, rising unemployment rate, experiences, sidewalk, staring effect, rubbing, recurrence, chord, consumption, applicants\n","[ 127  423  112  128  917  288  937  789  663 1110]\n","topic 29 : sinus pressure, waterjet, teaching, gouge, failures, Fatigue, laws, impeachment, pain killers, rill\n","[733 804 543 584 386  17 839 430 631 121]\n","topic 30 : disorders, expenditure, reboot, pills, terrorist, convergence, acid, imbalance, silversmiths, car\n","[ 733 1127  543 1011 1096  560   40 1036  111  430]\n","topic 31 : disorders, Soya farming, reboot, field, pyrotechnics, blockage, bite, child abuse, glands, imbalance\n","[ 385 1077  759  477  807  204  889  636  315  281]\n","topic 32 : preservatives, strategies, programmer, rebellion, staring effect, breeding, recurrence, species, alterations, choking\n","[495 360 671 880 396 123  43 566 947 455]\n","topic 33 : alarm, suicide, gravitational force, unconsciousness, wildfires, range, flashlight, flooding, lack, miscarriages\n","[  30  234  515  643   84  115  867  190 1077  745]\n","topic 34 : magma, documentary, rash, microphone, acne, germs, dust, Lymphedema, strategies, people\n","[1121  628  562  117   69  850  957 1077  795   26]\n","topic 35 : manakin, accumulation, perfume, changes, perturbation, loss, revolution, strategies, flammable liquids, echoing\n","[861 683 743  13 852  22 843 935 839 958]\n","topic 36 : Coughing, jams, dissatisfaction, delusions, decomposition reaction, tides, injury, beam, acid, gases\n","[ 350 1090  765  727  497  100    8  910  715  640]\n","topic 37 : Inhibition, stress, masts, symptoms, malfunction, debris, abuse, discoloration, bacteria, shortage\n","[438 961  39 538 197  63 872 832 912 958]\n","topic 38 : toll, hitting, corkscrew, Convulsions, movie, proof, Cities, bills, Bed sores, gases\n","[259 430 636 935 807 790  46 599 967 733]\n","topic 39 : neglect, imbalance, species, beam, staring effect, disability, cyclone, genetic problems, source, disorders\n","[  68   35 1110  868  257  359 1014  624  631  773]\n","topic 40 : streaks, burns, rill, action, construction, reliance, pop, Electrolysis, silversmiths, effects\n","[567 306 366 353 937  34 181 260 947  66]\n","topic 41 : transmission, tingling, contamination, ginseng, laws, refinement, hardening, recession, lack, threat\n","[ 455 1129   46  123  386   70  735   14  223   54]\n","topic 42 : miscarriages, washing, cyclone, range, terrorist, Progress, alibi, displacement, meeting, flood\n","[ 575  101  849  870  759  623  112  449 1096 1038]\n","topic 43 : snarl, hematoma, migrations, Outbreaks, programmer, aftershocks, teaching, overflows, pyrotechnics, conquests\n","[388 941 903 636 156  51 797 741 576 904]\n","topic 44 : ionization, influenza, relaxation, species, war, militancy, Production, linearity, scandals, braces\n","[ 924 1121   51  751  784  309  653   95  763  269]\n","topic 45 : Pinworm, manakin, militancy, sound, riots, tumor, plate, collision, outbreak, pressure\n","[ 559   50  487  546  779  728  327 1099  450 1014]\n","topic 46 : acupuncture, dwarf, drinks, crack, excitation, parasite, seas, singer, policies, pop\n","[758 383 565 458 382 390 423 315 502 688]\n","topic 47 : activity, awareness, bombarding, Germs, bout, deprivation, waterjet, alterations, torsion, shoulder problems\n","[ 170  910  962  872 1093  947  251  970  496 1035]\n","topic 48 : press, discoloration, slowdown, Cities, profit, lack, rubbing, consumption, education, malfunctions\n","[846 733 712 684 543  88 941 778 644 430]\n","topic 49 : pie, disorders, flows, presentation, reboot, opposition, influenza, experiences, software, imbalance\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 150\n","[TRAIN] loss: 0.4500, 0.4500, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5224, 0.5224, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 151\n","[TRAIN] loss: 0.4497, 0.4497, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5008, 0.5008, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 152\n","[TRAIN] loss: 0.4493, 0.4493, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5125, 0.5125, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 153\n","[TRAIN] loss: 0.4492, 0.4492, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5096, 0.5096, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 154\n","[TRAIN] loss: 0.4477, 0.4477, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5078, 0.5078, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 155\n","[TRAIN] loss: 0.4488, 0.4488, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5159, 0.5159, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 156\n","[TRAIN] loss: 0.4479, 0.4479, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5102, 0.5102, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 157\n","[TRAIN] loss: 0.4481, 0.4481, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5053, 0.5053, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 158\n","[TRAIN] loss: 0.4465, 0.4465, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5025, 0.5025, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 159\n","[TRAIN] loss: 0.4482, 0.4482, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5220, 0.5220, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 160\n","[TRAIN] loss: 0.4465, 0.4465, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.5147, 0.5147, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 161\n","[TRAIN] loss: 0.4459, 0.4459, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5052, 0.5052, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 162\n","[TRAIN] loss: 0.4458, 0.4458, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5041, 0.5041, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 163\n","[TRAIN] loss: 0.4461, 0.4461, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5160, 0.5160, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 164\n","[TRAIN] loss: 0.4453, 0.4453, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5189, 0.5189, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 165\n","[TRAIN] loss: 0.4449, 0.4449, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5103, 0.5103, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 166\n","[TRAIN] loss: 0.4441, 0.4441, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5063, 0.5063, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 167\n","[TRAIN] loss: 0.4424, 0.4424, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5119, 0.5119, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 168\n","[TRAIN] loss: 0.4437, 0.4437, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5154, 0.5154, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 169\n","[TRAIN] loss: 0.4431, 0.4431, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5063, 0.5063, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 170\n","[TRAIN] loss: 0.4426, 0.4426, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5085, 0.5085, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 171\n","[TRAIN] loss: 0.4424, 0.4424, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.4939, 0.4939, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 172\n","[TRAIN] loss: 0.4420, 0.4420, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5118, 0.5118, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 173\n","[TRAIN] loss: 0.4431, 0.4431, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5033, 0.5033, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 174\n","[TRAIN] loss: 0.4423, 0.4423, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5169, 0.5169, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 175\n","[TRAIN] loss: 0.4416, 0.4416, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5069, 0.5069, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 176\n","[TRAIN] loss: 0.4401, 0.4401, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5122, 0.5122, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 177\n","[TRAIN] loss: 0.4404, 0.4404, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.4978, 0.4978, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 178\n","[TRAIN] loss: 0.4417, 0.4417, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5120, 0.5120, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 179\n","[TRAIN] loss: 0.4422, 0.4422, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5144, 0.5144, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[198 797 910 975 969 452 893 970 582 432]\n","topic 0 : visible light, Production, discoloration, officers, stroke, candle, electricity, consumption, deluge, Sadness\n","[  82  698  638 1121  887  401  497  156  740 1089]\n","topic 1 : smoking, effect, harassment, manakin, Poverty, anesthetic, malfunction, war, assassination, neutrons\n","[ 350  860   86 1052  391  482 1006  289   27 1032]\n","topic 2 : Inhibition, money, demolition, Sinusitis, motion, hurricanes, experiment, lavas, sensation, discharge\n","[ 317  755  988  181  727  699    1 1056 1128  593]\n","topic 3 : Yoga, accidents, transmitter, hardening, symptoms, virus, rising unemployment rate, legs, hormonal changes, interest\n","[1118  968  799  530  809  422  432  293  770  912]\n","topic 4 : work, failure, cycling, debate, slugs, condition, Sadness, conditions, dams, Bed sores\n","[ 471  169  721 1085  922  305  442  547  911 1024]\n","topic 5 : Scleroderma, gas leak, inhalation, lymphomas, atmospheric pressure, landslides, figure, device, dispute, particles\n","[963 415  26 266 336 248 554  91 753 501]\n","topic 6 : lorries, downturn, echoing, Toxoplasmosis, compromises, distention, jaw problem, blasts, intoxication, design\n","[ 998  317  506 1032  204  953  980  665  526  385]\n","topic 7 : cancer, Yoga, frenzy, discharge, breeding, band, seismic event, pressures, burn, preservatives\n","[ 998  653  148   94  970  807  588  374  665 1128]\n","topic 8 : cancer, plate, mycoses, Birth defects, consumption, staring effect, Anger, morbidity, pressures, hormonal changes\n","[ 576  229  497  619  670 1032  998  910 1090 1128]\n","topic 9 : scandals, vaccines, malfunction, vocals, experience, discharge, cancer, discoloration, stress, hormonal changes\n","[1109  621  445   60   50  566  150  462  492  562]\n","topic 10 : tornado, caffeine withdrawal, circumstances, fever, dwarf, flooding, reduction, cancers, radiation exposures, perfume\n","[ 87 255 503 273 305 534 436  30 995 293]\n","topic 11 : cancellations, cockroaches, Malaria, roughness, landslides, fighting, boiling water, magma, constipation, conditions\n","[ 350  222  190  317  422   52  189 1125 1098  454]\n","topic 12 : Inhibition, singing, Lymphedema, Yoga, condition, interference, strike, trend, afterglow, dryness\n","[ 573 1004  422   87  119  965  322  901  550  865]\n","topic 13 : currents, fuel, condition, cancellations, Discomfort, fire, infestation, frustration, Infection, information\n","[181  39 379 268 270 817 121  88 534 970]\n","topic 14 : hardening, corkscrew, outrage, absorption, deaths, chime, car, opposition, fighting, consumption\n","[526 170 286 260 245 324 317 613  99 593]\n","topic 15 : burn, press, Acne, recession, deterioration, proliferation, Yoga, markets, arrest, interest\n","[ 459  104  116  691 1049  297   70 1121  810  282]\n","topic 16 : explosion, surgery, negotiations, weathering, mold, antidepressants, Progress, manakin, victory, arrests\n","[ 953  310   19  327   79  350 1062 1032  241  277]\n","topic 17 : band, Zinc, tectonic forces, seas, hormonal imbalance, Inhibition, overheating, discharge, Exposure, drama\n","[ 262  705  286  986 1100  492  559  425  598 1041]\n","topic 18 : fungi, sorrow, Acne, irritation, immigration, radiation exposures, acupuncture, War, fevers, illnesses\n","[ 410  715  529  518  115  804  761   68  552 1043]\n","topic 19 : trouble, bacteria, contingent, Immigrants, germs, expenditure, incident, streaks, incidents, sunrise\n","[705 817 479 958 202 893 969  39 290 538]\n","topic 20 : sorrow, chime, fight, gases, arterial blood pressure, electricity, stroke, corkscrew, component, Convulsions\n","[ 341  190  544 1077 1015 1076  185  269  645  515]\n","topic 21 : suffocation, Lymphedema, tardiness, strategies, inspiration, attacks, success, pressure, fuels, rash\n","[432 740  23   2 969 804 529 733 202 543]\n","topic 22 : Sadness, assassination, pinkeye, progress, stroke, expenditure, contingent, disorders, arterial blood pressure, reboot\n","[549 438 926 406 271 988 523 317 976 970]\n","topic 23 : skin abnormality, toll, illness, inflammation, airline, transmitter, smile, Yoga, speech, consumption\n","[ 255 1008 1056 1113  252  293  523  307  778   95]\n","topic 24 : cockroaches, launch, legs, anger, scandal, conditions, smile, worsening, experiences, collision\n","[ 547  386  555 1052 1098   57  826  202    5  274]\n","topic 25 : device, terrorist, immunizations, Sinusitis, afterglow, cigarettes, drugs, arterial blood pressure, laughter, activation\n","[194 305 679 350 807 926 852 297 576 530]\n","topic 26 : gash, landslides, hole, Inhibition, staring effect, illness, decomposition reaction, antidepressants, scandals, debate\n","[ 425  423  260 1076  248  375  574 1041  292  321]\n","topic 27 : War, waterjet, recession, attacks, distention, divorce, overfertilizing, illnesses, hardship, Discussion\n","[926 159  16 293 749 807 432 886 530 305]\n","topic 28 : illness, flea, emigration, conditions, chord, staring effect, Sadness, applicants, debate, landslides\n","[ 917  960  161  625  772  336 1127  186  831  663]\n","topic 29 : failures, demise, exertion, man, comet, compromises, Soya farming, plague, resonance, pain killers\n","[ 121 1013   17  430  202  543    2  804  674  805]\n","topic 30 : car, speculation, convergence, imbalance, arterial blood pressure, reboot, progress, expenditure, thought, tests\n","[   2  990  481   88  710  202  121 1036  719 1096]\n","topic 31 : progress, wrinkles, competition, opposition, clergy, arterial blood pressure, car, child abuse, Properties, pyrotechnics\n","[506 385 562 889 759 259 812 451 204 315]\n","topic 32 : frenzy, preservatives, perfume, recurrence, programmer, neglect, claustrophobia, burning, breeding, alterations\n","[871 671 381 286 473 396 260 355 317 425]\n","topic 33 : crusaders, gravitational force, eyestrain, Acne, drug use, wildfires, recession, dumping, Yoga, War\n","[ 234  689  922 1111  745   73  190  515  621  906]\n","topic 34 : documentary, amphibolites, atmospheric pressure, eruption, people, losses, Lymphedema, rash, caffeine withdrawal, meltdown\n","[829 501 934  69 432 733 663 562 568 543]\n","topic 35 : impairment, design, hum, perturbation, Sadness, disorders, pain killers, perfume, law, reboot\n","[1085   18  256  525  704  743  807  665  790  896]\n","topic 36 : lymphomas, interactions, militia, terrorists, pollution, dissatisfaction, staring effect, pressures, disability, embers\n","[ 497  910 1032  640  845  350  999 1090  100    8]\n","topic 37 : malfunction, discoloration, discharge, shortage, Addiction, Inhibition, invasions, stress, debris, abuse\n","[598  39 912 273 397 568 438 958 183  63]\n","topic 38 : fevers, corkscrew, Bed sores, roughness, internship, law, toll, gases, winds, proof\n","[805  23 475 831 743 648 704 430 807 665]\n","topic 39 : tests, pinkeye, chemical changes, resonance, dissatisfaction, impression, pollution, imbalance, staring effect, pressures\n","[ 257  932  377 1062  505 1112   17    2  625  944]\n","topic 40 : construction, corrosion, intubation, overheating, leaks, colds, convergence, progress, man, system\n","[ 947 1121  396  625  742   14  262  409  260  355]\n","topic 41 : lack, manakin, wildfires, man, research, displacement, fungi, power outage, recession, dumping\n","[211  54 323 384 790 119 567 704 121 743]\n","topic 42 : storms, flood, treatment, Dizziness, disability, Discomfort, transmission, pollution, car, dissatisfaction\n","[ 987  211  347   11   70  807  623 1075  870 1096]\n","topic 43 : revenues, storms, allergies, optimism, Progress, staring effect, aftershocks, anesthetics, Outbreaks, pyrotechnics\n","[640 568 903 734 845 553  88 857 904 588]\n","topic 44 : shortage, law, relaxation, eyelids, Addiction, release, opposition, beverages, braces, Anger\n","[1121  653   60  442  588  269  309  196  751  812]\n","topic 45 : manakin, plate, fever, figure, Anger, pressure, tumor, dermatitis, sound, claustrophobia\n","[ 277  123  100  304   79  606  553 1112  910 1014]\n","topic 46 : drama, range, debris, Constipation, hormonal imbalance, damages, release, colds, discoloration, pop\n","[1005  759  390  382  104  383  289  281  688  994]\n","topic 47 : bugs, programmer, deprivation, bout, surgery, awareness, lavas, choking, shoulder problems, asteroid\n","[ 742 1035  988 1128  409  947   95  262  727  260]\n","topic 48 : research, malfunctions, transmitter, hormonal changes, power outage, lack, collision, fungi, symptoms, recession\n","[684 235 999 307 210 543 970 430 805  88]\n","topic 49 : presentation, landmines, invasions, worsening, vortex-densities, reboot, consumption, imbalance, tests, opposition\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 180\n","[TRAIN] loss: 0.4399, 0.4399, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.12 s\n","[VALID] loss: 0.5029, 0.5029, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 181\n","[TRAIN] loss: 0.4383, 0.4383, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.4991, 0.4991, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 182\n","[TRAIN] loss: 0.4396, 0.4396, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.4988, 0.4988, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 183\n","[TRAIN] loss: 0.4395, 0.4395, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.4960, 0.4960, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 184\n","[TRAIN] loss: 0.4394, 0.4394, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5197, 0.5197, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 185\n","[TRAIN] loss: 0.4383, 0.4383, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5051, 0.5051, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 186\n","[TRAIN] loss: 0.4386, 0.4386, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5049, 0.5049, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 187\n","[TRAIN] loss: 0.4383, 0.4383, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.4968, 0.4968, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 188\n","[TRAIN] loss: 0.4381, 0.4381, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.4915, 0.4915, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 189\n","[TRAIN] loss: 0.4376, 0.4376, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5020, 0.5020, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 190\n","[TRAIN] loss: 0.4379, 0.4379, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5030, 0.5030, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 191\n","[TRAIN] loss: 0.4367, 0.4367, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5078, 0.5078, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 192\n","[TRAIN] loss: 0.4374, 0.4374, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5143, 0.5143, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 193\n","[TRAIN] loss: 0.4369, 0.4369, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.4977, 0.4977, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 194\n","[TRAIN] loss: 0.4362, 0.4362, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5021, 0.5021, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 195\n","[TRAIN] loss: 0.4363, 0.4363, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5095, 0.5095, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 196\n","[TRAIN] loss: 0.4353, 0.4353, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5107, 0.5107, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 197\n","[TRAIN] loss: 0.4357, 0.4357, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5060, 0.5060, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 198\n","[TRAIN] loss: 0.4364, 0.4364, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.4996, 0.4996, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 199\n","[TRAIN] loss: 0.4349, 0.4349, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.5029, 0.5029, 0.0000, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 200\n","[TRAIN] loss: 0.5685, 0.4342, 0.1342, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6432, 0.5100, 0.1332, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 201\n","[TRAIN] loss: 0.5666, 0.4346, 0.1320, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6224, 0.4918, 0.1307, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 202\n","[TRAIN] loss: 0.5642, 0.4346, 0.1296, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6313, 0.5028, 0.1284, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 203\n","[TRAIN] loss: 0.5613, 0.4338, 0.1274, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6256, 0.4992, 0.1264, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 204\n","[TRAIN] loss: 0.5599, 0.4344, 0.1256, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6256, 0.5009, 0.1247, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 205\n","[TRAIN] loss: 0.5588, 0.4349, 0.1239, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6225, 0.4993, 0.1232, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 206\n","[TRAIN] loss: 0.5565, 0.4340, 0.1225, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6467, 0.5249, 0.1218, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 207\n","[TRAIN] loss: 0.5562, 0.4350, 0.1212, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6305, 0.5099, 0.1206, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 208\n","[TRAIN] loss: 0.5554, 0.4353, 0.1201, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6154, 0.4958, 0.1196, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 209\n","[TRAIN] loss: 0.5543, 0.4352, 0.1191, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6213, 0.5027, 0.1186, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[582 749 910 863 576 970 432 846 845 920]\n","topic 0 : deluge, chord, discoloration, drill, scandals, consumption, Sadness, pie, Addiction, Alcohol\n","[ 161  506  300  297 1036  146   54  280  979  887]\n","topic 1 : exertion, frenzy, guy, antidepressants, child abuse, germ, flood, Preeclampsia, pathogens, Poverty\n","[ 227  829  223  721 1061  288 1094   46   27  194]\n","topic 2 : blockages, impairment, meeting, inhalation, Sodium, Fatigue, collapse, cyclone, sensation, gash\n","[ 895  694  675 1090  918  742  181   10  727 1093]\n","topic 3 : process, resignation, books, stress, relief, research, hardening, grief, symptoms, profit\n","[ 351 1118  917  206  912   86  720  438  251  396]\n","topic 4 : infection, work, failures, passage, Bed sores, demolition, bleeding, toll, rubbing, wildfires\n","[105 398 293 889 471 610 335 585 634 721]\n","topic 5 : rains, affect, conditions, recurrence, Scleroderma, Osteoporosis, rashes, violence, extinction, inhalation\n","[ 753  352 1009  336  501  314   63  774  963  449]\n","topic 6 : intoxication, threats, abdominal pain, compromises, design, Interpolation, proof, convulsion, lorries, overflows\n","[889 301 727 526 548 188 940 342 212 385]\n","topic 7 : recurrence, termination, symptoms, burn, emergency, satisfaction, joy, cars, convulsions, preservatives\n","[970 532 142 511 688 720 655 548 831 588]\n","topic 8 : consumption, nausea, plants, Speeding, shoulder problems, bleeding, heroin, emergency, resonance, Anger\n","[ 619  454  497  576  845  780  970 1090 1128   13]\n","topic 9 : vocals, dryness, malfunction, scandals, Addiction, intersection, consumption, stress, hormonal changes, delusions\n","[ 120  693 1110  400  171 1092  562  480   91 1077]\n","topic 10 : wheels, discolouration, rill, substance, hemiplegia, mentality, perfume, computers, blasts, strategies\n","[273 195  87 749 372 824 298 115 221 206]\n","topic 11 : roughness, message, cancellations, chord, temperature, thorns, Menopause, germs, scars, passage\n","[1111  798   96   61  832  464  947  949  593  350]\n","topic 12 : eruption, influx, bomb explosion, commerce, bills, bruises, lack, imbalances, interest, Inhibition\n","[232 949 666 495 119 345 351 875 380 577]\n","topic 13 : personality, imbalances, Calluses, alarm, Discomfort, development, infection, filament, disease, rainbow\n","[742 795 459 251 883 405 476 790 379 970]\n","topic 14 : research, flammable liquids, explosion, rubbing, Water, birthmarks, Entrepreneurship, disability, outrage, consumption\n","[912 832 848 260 197 170 720 355 224  66]\n","topic 15 : Bed sores, bills, discomfort, recession, movie, press, bleeding, dumping, women, threat\n","[ 977  258  116  986 1069   79  668  810 1049  282]\n","topic 16 : melody, tendinitis, negotiations, irritation, criminals, hormonal imbalance, radiation, victory, mold, arrests\n","[ 555  880  126  277  710  873  350  241 1032  327]\n","topic 17 : immunizations, unconsciousness, castle, drama, clergy, drinking, Inhibition, Exposure, discharge, seas\n","[ 932  919   34  725 1033  705  425 1086  492  381]\n","topic 18 : corrosion, devastations, refinement, redistribution, rendition, sorrow, War, separation field, radiation exposures, eyestrain\n","[518 514 674 740  70 109 141 166  65 868]\n","topic 19 : Immigrants, shooting, thought, assassination, Progress, scratches, bacterium, diseases, drainage, action\n","[ 87  63 353 397 415 832 119 958 538 780]\n","topic 20 : cancellations, proof, ginseng, internship, downturn, bills, Discomfort, gases, Convulsions, intersection\n","[ 734 1001  751  821  535  441  269  333 1097  645]\n","topic 21 : eyelids, medication overuse, sound, heart attack, unity, killings, pressure, ecstasy, control, fuels\n","[131   2 804 543 450 427 890 202 748  23]\n","topic 22 : leak, progress, expenditure, reboot, policies, Asthma, shingles, arterial blood pressure, swelling, pinkeye\n","[1109  549 1112  131  491  271  438   87  480  372]\n","topic 23 : tornado, skin abnormality, colds, leak, dog, airline, toll, cancellations, computers, temperature\n","[ 434  514  790  971  759  680  458 1056  221  749]\n","topic 24 : mistake, shooting, disability, scratch, programmer, flu, Germs, legs, scars, chord\n","[ 299  528 1088  744  738  947  461  547   24  190]\n","topic 25 : colorants, crisis, humans, expansion, sun, lack, burst, device, princess, Lymphedema\n","[ 435  187  512 1032   46  548  327  194  846 1026]\n","topic 26 : waste, blackout, contraction, discharge, cyclone, emergency, seas, gash, pie, havoc\n","[1041  247  537  260 1077  496  645  495  933  725]\n","topic 27 : illnesses, gas, suffering, recession, strategies, education, fuels, alarm, Frustrations, redistribution\n","[ 873  166  889  221  114  433  530  888  630 1096]\n","topic 28 : drinking, diseases, recurrence, scars, disruption, tornado track, debate, clot, death, pyrotechnics\n","[ 285  423 1076  375 1072 1102  498  609  941  661]\n","topic 29 : poor sleeping, waterjet, attacks, divorce, ball, protozoa, cocaine, poisoning, influenza, kidnapping\n","[828 210 869 674 683  72 430  17 543 386]\n","topic 30 : debt, vortex-densities, fume, thought, jams, execution, imbalance, convergence, reboot, terrorist\n","[ 795  645  710  487  733  106 1005  754 1096  719]\n","topic 31 : flammable liquids, fuels, clergy, drinks, disorders, cold, bugs, complaint, pyrotechnics, Properties\n","[ 759  867  940  477  378 1028  506  756  385  889]\n","topic 32 : programmer, dust, joy, rebellion, Boils, cell phones, frenzy, anxiety reaction, preservatives, recurrence\n","[692 671 224 381 705 355 484 880 260 396]\n","topic 33 : wire, gravitational force, women, eyestrain, sorrow, dumping, person, unconsciousness, recession, wildfires\n","[ 885  214 1077  829   73  166  233  906  273  745]\n","topic 34 : pregnancy, spell, strategies, impairment, losses, diseases, flight, meltdown, roughness, people\n","[1062  327  559   69    2   46  901  255  432  733]\n","topic 35 : overheating, seas, acupuncture, perturbation, progress, cyclone, frustration, cockroaches, Sadness, disorders\n","[790 301  98 861 348 230 900 839 489 119]\n","topic 36 : disability, termination, migraines, Coughing, Suicide, buring, earthquake, acid, selling, Discomfort\n","[ 324 1090  453  873    8  727 1032  464  845  910]\n","topic 37 : proliferation, stress, ringworm, drinking, abuse, symptoms, discharge, bruises, Addiction, discoloration\n","[183 273 780 438 699 538 783 912 756  63]\n","topic 38 : winds, roughness, intersection, toll, virus, Convulsions, soaps, Bed sores, anxiety reaction, proof\n","[ 430   17   88   13 1026  377  648  743  371  665]\n","topic 39 : imbalance, convergence, opposition, delusions, havoc, intubation, impression, dissatisfaction, inhibition, pressures\n","[1109 1110  860  327  868 1099 1112 1014  257  944]\n","topic 40 : tornado, rill, money, seas, action, singer, colds, pop, construction, system\n","[ 353  567  336  389   66  484 1112  260  224  355]\n","topic 41 : ginseng, transmission, compromises, buckling, threat, person, colds, recession, women, dumping\n","[476 300 791 567 724 777 386 288 377 768]\n","topic 42 : Entrepreneurship, guy, cementation, transmission, defects, viruses, terrorist, Fatigue, intubation, terror\n","[ 500  524  633  578  782  112  308  139  831 1096]\n","topic 43 : disturbances, decrease, science, vomiting, repetition, teaching, driver, Flashes, resonance, pyrotechnics\n","[645 759 115 994  88 931 458 653 857 576]\n","topic 44 : fuels, programmer, germs, asteroid, opposition, dye, Germs, plate, beverages, scandals\n","[ 470  190  972 1076   70  812  269  553  309  196]\n","topic 45 : pain, Lymphedema, incubation, attacks, Progress, claustrophobia, pressure, release, tumor, dermatitis\n","[ 910  907  537  553  143 1014  984  957   93  487]\n","topic 46 : discoloration, appetite, suffering, release, marketing spending, pop, chemicals, revolution, improvement, drinks\n","[195 281 390 973 382 175 565 578 289 688]\n","topic 47 : message, choking, deprivation, controversy, bout, deficit, bombarding, vomiting, lavas, shoulder problems\n","[ 720  495 1090  846  262   34  947  251  222  970]\n","topic 48 : bleeding, alarm, stress, pie, fungi, refinement, lack, rubbing, singing, consumption\n","[674 505 235 210 430 187 920 970 846  17]\n","topic 49 : thought, leaks, landmines, vortex-densities, imbalance, blackout, Alcohol, consumption, pie, convergence\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 210\n","[TRAIN] loss: 0.5533, 0.4351, 0.1182, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6327, 0.5150, 0.1178, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 211\n","[TRAIN] loss: 0.5524, 0.4350, 0.1174, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6221, 0.5051, 0.1170, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 212\n","[TRAIN] loss: 0.5519, 0.4352, 0.1167, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6084, 0.4921, 0.1163, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 213\n","[TRAIN] loss: 0.5513, 0.4353, 0.1160, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6142, 0.4985, 0.1157, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 214\n","[TRAIN] loss: 0.5503, 0.4349, 0.1154, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6138, 0.4987, 0.1152, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 215\n","[TRAIN] loss: 0.5504, 0.4355, 0.1149, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6178, 0.5032, 0.1146, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 216\n","[TRAIN] loss: 0.5495, 0.4350, 0.1144, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6208, 0.5066, 0.1142, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 217\n","[TRAIN] loss: 0.5506, 0.4366, 0.1140, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6188, 0.5050, 0.1138, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 218\n","[TRAIN] loss: 0.5485, 0.4349, 0.1136, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6157, 0.5023, 0.1134, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 219\n","[TRAIN] loss: 0.5480, 0.4348, 0.1132, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6100, 0.4970, 0.1130, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 220\n","[TRAIN] loss: 0.5476, 0.4347, 0.1129, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6023, 0.4896, 0.1127, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 221\n","[TRAIN] loss: 0.5488, 0.4362, 0.1126, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6113, 0.4989, 0.1124, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 222\n","[TRAIN] loss: 0.5469, 0.4346, 0.1123, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6113, 0.4992, 0.1121, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 223\n","[TRAIN] loss: 0.5471, 0.4351, 0.1120, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6152, 0.5033, 0.1119, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 224\n","[TRAIN] loss: 0.5462, 0.4345, 0.1118, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6111, 0.4994, 0.1116, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 225\n","[TRAIN] loss: 0.5475, 0.4359, 0.1115, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6067, 0.4953, 0.1114, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 226\n","[TRAIN] loss: 0.5467, 0.4353, 0.1113, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6255, 0.5142, 0.1112, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 227\n","[TRAIN] loss: 0.5460, 0.4349, 0.1112, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6165, 0.5054, 0.1111, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 228\n","[TRAIN] loss: 0.5464, 0.4354, 0.1110, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6085, 0.4976, 0.1109, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 229\n","[TRAIN] loss: 0.5464, 0.4356, 0.1108, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6055, 0.4948, 0.1107, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 230\n","[TRAIN] loss: 0.5460, 0.4353, 0.1107, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6070, 0.4964, 0.1106, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 231\n","[TRAIN] loss: 0.5461, 0.4355, 0.1105, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6222, 0.5117, 0.1105, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 232\n","[TRAIN] loss: 0.5458, 0.4354, 0.1104, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6166, 0.5063, 0.1103, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 233\n","[TRAIN] loss: 0.5456, 0.4354, 0.1103, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6094, 0.4992, 0.1102, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 234\n","[TRAIN] loss: 0.5444, 0.4342, 0.1102, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6135, 0.5034, 0.1101, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 235\n","[TRAIN] loss: 0.5455, 0.4354, 0.1100, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6275, 0.5175, 0.1100, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 236\n","[TRAIN] loss: 0.5454, 0.4354, 0.1100, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6153, 0.5054, 0.1099, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 237\n","[TRAIN] loss: 0.5444, 0.4345, 0.1099, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6139, 0.5041, 0.1098, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 238\n","[TRAIN] loss: 0.5449, 0.4351, 0.1098, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6079, 0.4982, 0.1097, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 239\n","[TRAIN] loss: 0.5440, 0.4343, 0.1097, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6121, 0.5025, 0.1097, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[ 255  882  910  834  582 1090  749  920  372  576]\n","topic 0 : cockroaches, risk, discoloration, humidity, deluge, stress, chord, Alcohol, temperature, scandals\n","[1091  903  664  123   67  358  155  419  146  740]\n","topic 1 : fatty plaque, relaxation, surplus, range, growth, confidence, situation, increased pressure, germ, assassination\n","[ 391   27 1046  691  205  674  510  367  497  917]\n","topic 2 : motion, sensation, crowd, weathering, diarrhea, thought, widgets, deforestation, malfunction, failures\n","[ 652  629  918 1024   66   88  181    1 1093  727]\n","topic 3 : Trauma, anticipation, relief, particles, threat, opposition, hardening, rising unemployment rate, profit, symptoms\n","[293 495 542 538 422 749 195 356 452 770]\n","topic 4 : conditions, alarm, mobility, Convulsions, condition, chord, message, tone, candle, dams\n","[378 183 335 721 105 293 182 756  47 477]\n","topic 5 : Boils, winds, rashes, inhalation, rains, conditions, dehydration, anxiety reaction, disc, rebellion\n","[ 795  807  627  266  483   72  774 1009  314  540]\n","topic 6 : flammable liquids, staring effect, tension, Toxoplasmosis, insoles, execution, convulsion, abdominal pain, Interpolation, palpitations\n","[ 953  889  212  259 1093  385  807  833  315  820]\n","topic 7 : band, recurrence, convulsions, neglect, profit, preservatives, staring effect, rediscovery, alterations, indicator\n","[ 686  100 1010  511   94  837  374 1128  998  576]\n","topic 8 : properties, debris, humiliation, Speeding, Birth defects, fires, morbidity, hormonal changes, cancer, scandals\n","[1093  619  833  259  538  640 1128  497 1090  910]\n","topic 9 : profit, vocals, rediscovery, neglect, Convulsions, shortage, hormonal changes, malfunction, stress, discoloration\n","[ 593  703 1111 1028  400  973 1077 1125  562  457]\n","topic 10 : interest, hinge, eruption, cell phones, substance, controversy, strategies, trend, perfume, drowning\n","[254 601 206 227 298 770 293 962 815 824]\n","topic 11 : Shrinkage, presence, passage, blockages, Menopause, dams, conditions, slowdown, loop, thorns\n","[ 317  947 1098  358  367  506  462  689  190 1125]\n","topic 12 : Yoga, lack, afterglow, confidence, deforestation, frenzy, cancers, amphibolites, Lymphedema, trend\n","[1007 1004  664  682  839  153  685  293  119  865]\n","topic 13 : Earthquakes, fuel, surplus, movements, acid, addition, gun, conditions, Discomfort, information\n","[ 268  311  227  432  582  296 1123 1035  883  970]\n","topic 14 : absorption, power, blockages, Sadness, deluge, current, crash, malfunctions, Water, consumption\n","[912 319 400 720 961 566 262 947 455 791]\n","topic 15 : Bed sores, anemia, substance, bleeding, hitting, flooding, fungi, lack, miscarriages, cementation\n","[ 205  116  286  393 1125  368  691 1069  810   93]\n","topic 16 : diarrhea, negotiations, Acne, generator, trend, eating, weathering, criminals, victory, improvement\n","[  67   64  480  768  415  189 1003  723  241  655]\n","topic 17 : growth, disorder, computers, terror, downturn, strike, toothache, amplifier, Exposure, heroin\n","[  50  409 1086  826  701  496  292  865 1041  425]\n","topic 18 : dwarf, power outage, separation field, drugs, applicability, education, hardship, information, illnesses, War\n","[ 803  827 1068  529  581  639  798  902  183  654]\n","topic 19 : media, shock waves, calm, contingent, praise, Blinking, influx, Weak ligaments, winds, insomnia\n","[396  56  39 681   3  82 153 209 538 969]\n","topic 20 : wildfires, pleasure, corkscrew, jolliness, distress, smoking, addition, tsunamis, Convulsions, stroke\n","[1015  414   83  734  190  558  515 1077  269  645]\n","topic 21 : inspiration, high humidity, hull, eyelids, Lymphedema, machine, rash, strategies, pressure, fuels\n","[882 178   2 206 624 863 278 121 543  23]\n","topic 22 : risk, revenue, progress, passage, Electrolysis, drill, infraction, car, reboot, pinkeye\n","[ 582  538   21  246   87  976 1030 1109  549  760]\n","topic 23 : deluge, Convulsions, consumers, sidewalk, cancellations, speech, dancing, tornado, skin abnormality, fracture\n","[ 770 1056  530  749  206  759  105  434 1008  652]\n","topic 24 : dams, legs, debate, chord, passage, programmer, rains, mistake, launch, Trauma\n","[ 351   79  418  763 1004  547  946  722  877 1058]\n","topic 25 : infection, hormonal imbalance, deregulation, outbreak, fuel, device, deformation, rupture, impact, aggressions\n","[807 998 512  94 284 180 452 483 846 194]\n","topic 26 : staring effect, cancer, contraction, Birth defects, vibration, visit, candle, insoles, pie, gash\n","[1077  645  537  375  933  959  262 1041   72  496]\n","topic 27 : strategies, fuels, suffering, divorce, Frustrations, dandruff, fungi, illnesses, execution, education\n","[503 655 434 404 601 778 998 886 246 576]\n","topic 28 : Malaria, heroin, mistake, biography, presence, experiences, cancer, applicants, sidewalk, scandals\n","[ 161  428  688  663 1045  932   21 1052  645  831]\n","topic 29 : exertion, colouration, shoulder problems, pain killers, chaos, corrosion, consumers, Sinusitis, fuels, resonance\n","[210 683 949  23 828 543 367  72 121 674]\n","topic 30 : vortex-densities, jams, imbalances, pinkeye, debt, reboot, deforestation, execution, car, thought\n","[ 686  185 1054  280  633  782   66 1096  972  719]\n","topic 31 : properties, success, boom, Preeclampsia, science, repetition, threat, pyrotechnics, incubation, Properties\n","[ 433  315  284  385  807 1077  867  204  889  759]\n","topic 32 : tornado track, alterations, vibration, preservatives, staring effect, strategies, dust, breeding, recurrence, programmer\n","[880 355 224 292  43 495 791 455 353 566]\n","topic 33 : unconsciousness, dumping, women, hardship, flashlight, alarm, cementation, miscarriages, ginseng, flooding\n","[ 919 1061  233  621  286  829  739  906   77  190]\n","topic 34 : devastations, Sodium, flight, caffeine withdrawal, Acne, impairment, climate change, meltdown, photomultiplier, Lymphedema\n","[ 776  829  419  954 1009  389  562  847    2  568]\n","topic 35 : Vulvodynia, impairment, increased pressure, negligence, abdominal pain, buckling, perfume, factories, progress, law\n","[ 474  721 1085  713   16  991  922 1075  807 1109]\n","topic 36 : relocation, inhalation, lymphomas, weather, emigration, funding, atmospheric pressure, anesthetics, staring effect, tornado\n","[ 241  632  492  765  350  998 1113  910  239  497]\n","topic 37 : Exposure, screen, radiation exposures, masts, Inhibition, cancer, anger, discoloration, corruption, malfunction\n","[783 397 183  72  89 197 293 538 912  63]\n","topic 38 : soaps, internship, winds, execution, sensitivity, movie, conditions, Convulsions, Bed sores, proof\n","[ 600   16  732  648  599  430 1096  704   88  807]\n","topic 39 : refrigerator, emigration, patriots, impression, genetic problems, imbalance, pyrotechnics, pollution, opposition, staring effect\n","[1122  873  860  625 1062  944  528  868 1112  327]\n","topic 40 : asthma, drinking, money, man, overheating, system, crisis, action, colds, seas\n","[1039 1067  263  567  260  353  409 1008  224  355]\n","topic 41 : withdrawal, exhibition, lorry, transmission, recession, ginseng, power outage, launch, women, dumping\n","[ 435  768  967 1074  724  476  817  789  791   43]\n","topic 42 : waste, terror, source, floodlights, defects, Entrepreneurship, chime, impeachment, cementation, flashlight\n","[ 757  474   66  101  560  112  782 1038  849 1096]\n","topic 43 : Estrogen dominance, relocation, threat, hematoma, blockage, teaching, repetition, conquests, migrations, pyrotechnics\n","[303 895 100 759 854  47 645 419 588  88]\n","topic 44 : adhesion, process, debris, programmer, acceleration, disc, fuels, increased pressure, Anger, opposition\n","[  68  469  553 1052  100  902  718  972  269  812]\n","topic 45 : streaks, meteoroids, release, Sinusitis, debris, Weak ligaments, medication, incubation, pressure, claustrophobia\n","[ 984  553  641  495  808  933 1014  993  100   93]\n","topic 46 : chemicals, release, Famine, alarm, storm, Frustrations, pop, spilling, debris, improvement\n","[ 218  195  384  688  558  382 1074  663  390  383]\n","topic 47 : disasters, message, Dizziness, shoulder problems, machine, bout, floodlights, pain killers, deprivation, awareness\n","[1035  181  262  961  495  353  910  438  652  970]\n","topic 48 : malfunctions, hardening, fungi, hitting, alarm, ginseng, discoloration, toll, Trauma, consumption\n","[ 430  191  576 1056  235  543  665  846  187   88]\n","topic 49 : imbalance, aberrations, scandals, legs, landmines, reboot, pressures, pie, blackout, opposition\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 240\n","[TRAIN] loss: 0.5438, 0.4342, 0.1096, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.10 s\n","[VALID] loss: 0.6130, 0.5035, 0.1096, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 241\n","[TRAIN] loss: 0.5451, 0.4356, 0.1096, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6023, 0.4928, 0.1095, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 242\n","[TRAIN] loss: 0.5440, 0.4345, 0.1095, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6058, 0.4963, 0.1094, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 243\n","[TRAIN] loss: 0.5439, 0.4345, 0.1094, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6069, 0.4975, 0.1094, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 244\n","[TRAIN] loss: 0.5436, 0.4343, 0.1093, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6060, 0.4967, 0.1093, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 245\n","[TRAIN] loss: 0.5443, 0.4350, 0.1093, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6098, 0.5005, 0.1093, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 246\n","[TRAIN] loss: 0.5442, 0.4350, 0.1092, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6125, 0.5033, 0.1092, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 247\n","[TRAIN] loss: 0.5423, 0.4331, 0.1092, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6098, 0.5006, 0.1092, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 248\n","[TRAIN] loss: 0.5435, 0.4344, 0.1091, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6212, 0.5121, 0.1091, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 249\n","[TRAIN] loss: 0.5439, 0.4348, 0.1091, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6009, 0.4919, 0.1091, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 250\n","[TRAIN] loss: 0.5425, 0.4334, 0.1090, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6235, 0.5144, 0.1090, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 251\n","[TRAIN] loss: 0.5435, 0.4345, 0.1090, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6107, 0.5017, 0.1090, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 252\n","[TRAIN] loss: 0.5443, 0.4354, 0.1090, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6025, 0.4935, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 253\n","[TRAIN] loss: 0.5436, 0.4347, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6149, 0.5059, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 254\n","[TRAIN] loss: 0.5423, 0.4334, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6188, 0.5099, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 255\n","[TRAIN] loss: 0.5429, 0.4341, 0.1089, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6104, 0.5015, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 256\n","[TRAIN] loss: 0.5428, 0.4339, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6068, 0.4980, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 257\n","[TRAIN] loss: 0.5433, 0.4345, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6132, 0.5044, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 258\n","[TRAIN] loss: 0.5433, 0.4345, 0.1088, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6117, 0.5030, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 259\n","[TRAIN] loss: 0.5433, 0.4345, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6095, 0.5008, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 260\n","[TRAIN] loss: 0.5419, 0.4332, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6026, 0.4939, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 261\n","[TRAIN] loss: 0.5413, 0.4326, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6073, 0.4986, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 262\n","[TRAIN] loss: 0.5416, 0.4329, 0.1087, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6132, 0.5046, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 263\n","[TRAIN] loss: 0.5425, 0.4339, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6001, 0.4915, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 264\n","[TRAIN] loss: 0.5418, 0.4332, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6159, 0.5073, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 265\n","[TRAIN] loss: 0.5411, 0.4325, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6076, 0.4991, 0.1086, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 266\n","[TRAIN] loss: 0.5426, 0.4341, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6103, 0.5018, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 267\n","[TRAIN] loss: 0.5416, 0.4331, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6241, 0.5156, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 268\n","[TRAIN] loss: 0.5417, 0.4332, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6059, 0.4974, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 269\n","[TRAIN] loss: 0.5424, 0.4340, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5999, 0.4914, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[699 576 968 910 846 845 432 797 311 941]\n","topic 0 : virus, scandals, failure, discoloration, pie, Addiction, Sadness, Production, power, influenza\n","[358 740 904  93 719 812 979 706 280 291]\n","topic 1 : confidence, assassination, braces, improvement, Properties, claustrophobia, pathogens, tugging, Preeclampsia, damage\n","[733 767 601 508 402 367 223  44 439 598]\n","topic 2 : disorders, addiction, presence, swell, burrows, deforestation, meeting, wine, sizzle, fevers\n","[ 495  215  665  699 1093  918  632  832  727  652]\n","topic 3 : alarm, exports, pressures, virus, profit, relief, screen, bills, symptoms, Trauma\n","[ 749  422  947  225  223  799 1068  902  153  293]\n","topic 4 : chord, condition, lack, retention, meeting, cycling, calm, Weak ligaments, addition, conditions\n","[843 429 935 371 182 963 721 305 444 471]\n","topic 5 : injury, scintillator material, beam, inhibition, dehydration, lorries, inhalation, landslides, movement, Scleroderma\n","[479  97 281 855 172 963  63 116 540 665]\n","topic 6 : fight, thump, choking, drum, fog, lorries, proof, negotiations, palpitations, pressures\n","[ 488  231  352  403  698  953  170 1031  342  832]\n","topic 7 : Glaucoma, raisin bread, threats, conjunctivitis, effect, band, press, ointment, cars, bills\n","[ 110  665   94 1128  374  653  548  617  588  831]\n","topic 8 : wave, pressures, Birth defects, hormonal changes, morbidity, plate, emergency, studies, Anger, resonance\n","[ 640  872   72  259  947  362  632 1128 1093 1090]\n","topic 9 : shortage, Cities, execution, neglect, lack, hydraulic mining, screen, hormonal changes, profit, stress\n","[ 555  703  625  888 1028 1077  593   35  562  492]\n","topic 10 : immunizations, hinge, man, clot, cell phones, strategies, interest, burns, perfume, radiation exposures\n","[1068  749  688  293  824   87  815  701  995  273]\n","topic 11 : calm, chord, shoulder problems, conditions, thorns, cancellations, loop, applicability, constipation, roughness\n","[422 930 947 689 798 866 110 727 611  66]\n","topic 12 : condition, unemployment, lack, amphibolites, influx, confusion, wave, symptoms, body, threat\n","[448 713   4 871 742 603 345 875 119 861]\n","topic 13 : overdevelopment, weather, reaction, crusaders, research, shrinkage, development, filament, Discomfort, Coughing\n","[ 213  622  970  268  958  895  989 1035  790   70]\n","topic 14 : algorithm, cholesterol, consumption, absorption, gases, process, calamities, malfunctions, disability, Progress\n","[871 769 243 170 681 197 137 355 832 947]\n","topic 15 : crusaders, bike-accident, smog, press, jolliness, movie, events, dumping, bills, lack\n","[ 810  986  137  368 1112  282  201 1049 1028  258]\n","topic 16 : victory, irritation, events, eating, colds, arrests, physician, mold, cell phones, tendinitis\n","[953 537 705 579 798 286 779 450 241 793]\n","topic 17 : band, suffering, sorrow, gains, influx, Acne, excitation, policies, Exposure, charges\n","[ 154  473   58 1033  705  701  959  425  496  598]\n","topic 18 : shockwaves, drug use, overflow, rendition, sorrow, applicability, dandruff, War, education, fevers\n","[ 836  211 1060  990  339 1043  183  166  715  893]\n","topic 19 : clearance, storms, quake, wrinkles, infertility, sunrise, winds, diseases, bacteria, electricity\n","[ 803  958  582  893  817  890  479  572   87 1035]\n","topic 20 : media, gases, deluge, electricity, chime, shingles, fight, Viruses, cancellations, malfunctions\n","[ 168  515  441  751  291  281 1077  982  645  341]\n","topic 21 : expansions, rash, killings, sound, damage, choking, strategies, water, fuels, suffocation\n","[804 890 206 202 431 893  48   2 944  63]\n","topic 22 : expenditure, shingles, passage, arterial blood pressure, gunfire, electricity, fireplace, progress, system, proof\n","[ 435   91 1109  798  831  356 1008  559   87  749]\n","topic 23 : waste, blasts, tornado, influx, resonance, tone, launch, acupuncture, cancellations, chord\n","[ 458  831  576  902  434  921  749  206 1008  311]\n","topic 24 : Germs, resonance, scandals, Weak ligaments, mistake, elbow pain, chord, passage, launch, power\n","[ 351  155  875 1099  266   89  238  702  547  947]\n","topic 25 : infection, situation, filament, singer, Toxoplasmosis, sensitivity, Smoke, attack, device, lack\n","[ 191  506  206  305  374  194  548  679  588 1026]\n","topic 26 : aberrations, frenzy, passage, landslides, morbidity, gash, emergency, hole, Anger, havoc\n","[ 645  355 1106  772  925  174  959 1077  341 1041]\n","topic 27 : fuels, dumping, diode, comet, excess, injection, dandruff, strategies, suffocation, illnesses\n","[ 630 1096  452  888   94  432  846  356  434  886]\n","topic 28 : death, pyrotechnics, candle, clot, Birth defects, Sadness, pie, tone, mistake, applicants\n","[ 263 1112   68  256  423  201  149  736  688  505]\n","topic 29 : lorry, colds, streaks, militia, waterjet, physician, catastrophe, clock, shoulder problems, leaks\n","[674 584 367 869  72 430 543 631   2 121]\n","topic 30 : thought, pills, deforestation, fume, execution, imbalance, reboot, silversmiths, progress, car\n","[ 754  185  121  946  904  645  795  134   51 1096]\n","topic 31 : complaint, success, car, deformation, braces, fuels, flammable liquids, landslide, militancy, pyrotechnics\n","[ 610  204 1103  506  848 1077  889  252  451  953]\n","topic 32 : Osteoporosis, breeding, protest, frenzy, discomfort, strategies, recurrence, scandal, burning, band\n","[243 567 355 455 947 495 692 353 137 644]\n","topic 33 : smog, transmission, dumping, miscarriages, lack, alarm, wire, ginseng, events, software\n","[ 190  273  233  358  515  982  687 1077   73 1061]\n","topic 34 : Lymphedema, roughness, flight, confidence, rash, water, evaporation, strategies, losses, Sodium\n","[ 358  281  850 1014  559  948  318  888  432  201]\n","topic 35 : confidence, choking, loss, pop, acupuncture, celebs, laughing, clot, Sadness, physician\n","[1075  861  570  922  232  839  875  685  901 1008]\n","topic 36 : anesthetics, Coughing, distortion, atmospheric pressure, personality, acid, filament, gun, frustration, launch\n","[ 873  492 1090  846  710  532  999    8  715  910]\n","topic 37 : drinking, radiation exposures, stress, pie, clergy, nausea, invasions, abuse, bacteria, discoloration\n","[271 961 756 526 457 832  72 293 273  63]\n","topic 38 : airline, hitting, anxiety reaction, burn, drowning, bills, execution, conditions, roughness, proof\n","[1112  984 1013   13 1096  843  371  790  831  648]\n","topic 39 : colds, chemicals, speculation, delusions, pyrotechnics, injury, inhibition, disability, resonance, impression\n","[ 976  624 1112  375  944    7  627  257 1110  505]\n","topic 40 : speech, Electrolysis, colds, divorce, system, floodwaters, tension, construction, rill, leaks\n","[ 566  662  366  376 1083  324  947  353 1008  355]\n","topic 41 : flooding, happiness, contamination, pyrolysis, epidemic, proliferation, lack, ginseng, launch, dumping\n","[ 567  476  360  679 1129  123  137  879  424  644]\n","topic 42 : transmission, Entrepreneurship, suicide, hole, washing, range, events, problems, mentoring, software\n","[ 287  575  139  112 1075  648  849  623  870  831]\n","topic 43 : tears, snarl, Flashes, teaching, anesthetics, impression, migrations, aftershocks, Outbreaks, resonance\n","[797 183  51 759 793  88 854 846 576 588]\n","topic 44 : Production, winds, militancy, programmer, charges, opposition, acceleration, pie, scandals, Anger\n","[469 607  70 718 821 854 618 442 269  68]\n","topic 45 : meteoroids, headache, Progress, medication, heart attack, acceleration, panic, figure, pressure, streaks\n","[ 979  521  944  588  553  450  258  993  304 1099]\n","topic 46 : pathogens, eruptions, system, Anger, release, policies, tendinitis, spilling, Constipation, singer\n","[963 216 289 582 281 218 390 471 383 382]\n","topic 47 : lorries, lead, lavas, deluge, choking, disasters, deprivation, Scleroderma, awareness, bout\n","[749 988 970 262 353 910 153 495 832 947]\n","topic 48 : chord, transmitter, consumption, fungi, ginseng, discoloration, addition, alarm, bills, lack\n","[684 187  95 828  88 121 430 674 846 543]\n","topic 49 : presentation, blackout, collision, debt, opposition, car, imbalance, thought, pie, reboot\n","======================================================================\n","\n","\n","\n","======================================================================\n","Epoch 270\n","[TRAIN] loss: 0.5411, 0.4327, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6084, 0.5000, 0.1085, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 271\n","[TRAIN] loss: 0.5417, 0.4333, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6170, 0.5085, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 272\n","[TRAIN] loss: 0.5413, 0.4329, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6256, 0.5172, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 273\n","[TRAIN] loss: 0.5422, 0.4338, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6092, 0.5008, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 274\n","[TRAIN] loss: 0.5419, 0.4335, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6110, 0.5026, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 275\n","[TRAIN] loss: 0.5422, 0.4338, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6132, 0.5049, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 276\n","[TRAIN] loss: 0.5406, 0.4322, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6163, 0.5079, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 277\n","[TRAIN] loss: 0.5420, 0.4337, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6118, 0.5035, 0.1084, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 278\n","[TRAIN] loss: 0.5424, 0.4341, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6072, 0.4989, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 279\n","[TRAIN] loss: 0.5410, 0.4326, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.5996, 0.4913, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 280\n","[TRAIN] loss: 0.5415, 0.4332, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6087, 0.5004, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 281\n","[TRAIN] loss: 0.5410, 0.4327, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6100, 0.5017, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 282\n","[TRAIN] loss: 0.5400, 0.4317, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6004, 0.4922, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 283\n","[TRAIN] loss: 0.5407, 0.4325, 0.1083, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6014, 0.4931, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 284\n","[TRAIN] loss: 0.5406, 0.4324, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6069, 0.4987, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 285\n","[TRAIN] loss: 0.5414, 0.4331, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6072, 0.4990, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 286\n","[TRAIN] loss: 0.5416, 0.4334, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6016, 0.4934, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 287\n","[TRAIN] loss: 0.5407, 0.4325, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6134, 0.5052, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 288\n","[TRAIN] loss: 0.5416, 0.4334, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6010, 0.4928, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 289\n","[TRAIN] loss: 0.5410, 0.4328, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6101, 0.5019, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 290\n","[TRAIN] loss: 0.5413, 0.4331, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6098, 0.5017, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 291\n","[TRAIN] loss: 0.5398, 0.4316, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6069, 0.4987, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 292\n","[TRAIN] loss: 0.5399, 0.4317, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6148, 0.5067, 0.1082, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 293\n","[TRAIN] loss: 0.5405, 0.4324, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6017, 0.4935, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 294\n","[TRAIN] loss: 0.5410, 0.4328, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6112, 0.5030, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 295\n","[TRAIN] loss: 0.5402, 0.4321, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6197, 0.5116, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 296\n","[TRAIN] loss: 0.5407, 0.4326, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6085, 0.5003, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 297\n","[TRAIN] loss: 0.5393, 0.4311, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.08 s\n","[VALID] loss: 0.6100, 0.5019, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.01 s\n","\n","\n","\n","======================================================================\n","Epoch 298\n","[TRAIN] loss: 0.5410, 0.4329, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6038, 0.4957, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","\n","\n","\n","======================================================================\n","Epoch 299\n","[TRAIN] loss: 0.5398, 0.4318, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.09 s\n","[VALID] loss: 0.6041, 0.4960, 0.1081, 0.0000, 0.0000 (all, tri, or, off, nei), time: 0.00 s\n","Topics with probability argmax\n","50\n","50\n","[624 910 841 614 970 432 688 372 857 834]\n","topic 0 : Electrolysis, discoloration, delay, invasion, consumption, Sadness, shoulder problems, temperature, beverages, humidity\n","[  93  528  908  300  740  887  274  183  132 1094]\n","topic 1 : improvement, crisis, deficits, guy, assassination, Poverty, activation, winds, battle, collapse\n","[ 999  407 1005 1094  864  640  316  547  691  227]\n","topic 2 : invasions, bending, bugs, collapse, product, shortage, nerves, device, weathering, blockages\n","[ 363  170  901 1128  215  675  400  231 1093  442]\n","topic 3 : policy, press, frustration, hormonal changes, exports, books, substance, raisin bread, profit, figure\n","[1086  632  227   87  547  255 1118  296  193   39]\n","topic 4 : separation field, screen, blockages, cancellations, device, cockroaches, work, current, operations, corkscrew\n","[354 913 547 852 339 365 878 958 398 169]\n","topic 5 : sinking, swing, device, decomposition reaction, infertility, fluctuations, Fog, gases, affect, gas leak\n","[788 168 748  97 923 771 415 172 479 314]\n","topic 6 : pollutant, expansions, swelling, thump, conflict, belief, downturn, fog, fight, Interpolation\n","[ 921  953  342  548 1031  511  942  820  698  352]\n","topic 7 : elbow pain, band, cars, emergency, ointment, Speeding, poverty, indicator, effect, threats\n","[ 511  259 1128  675 1031  710  576 1032  374  998]\n","topic 8 : Speeding, neglect, hormonal changes, books, ointment, clergy, scandals, discharge, morbidity, cancer\n","[ 632  970  998  765  619 1090  941  910 1128  538]\n","topic 9 : screen, consumption, cancer, masts, vocals, stress, influenza, discoloration, hormonal changes, Convulsions\n","[1028 1111  627  150   50  703  171  992  492  562]\n","topic 10 : cell phones, eruption, tension, reduction, dwarf, hinge, hemiplegia, chopsticks, radiation exposures, perfume\n","[140 770 311 547 206 255 293  87  30 195]\n","topic 11 : buss, dams, power, device, passage, cockroaches, conditions, cancellations, magma, message\n","[ 998  947  566  887   24  414  291 1111 1090  316]\n","topic 12 : cancer, lack, flooding, Poverty, princess, high humidity, damage, eruption, stress, nerves\n","[ 746  685  490  422  248   59  865  910  154 1104]\n","topic 13 : business, gun, reading, condition, distention, dizziness, information, discoloration, shockwaves, hormone\n","[ 817  379  534  985  337 1035  995  958  121  760]\n","topic 14 : chime, outrage, fighting, overdose, circulation, malfunctions, constipation, gases, car, fracture\n","[593 958  15 400 404 566 224 791 103 355]\n","topic 15 : interest, gases, pseudolesion, substance, biography, flooding, women, cementation, breakdown, dumping\n","[ 286  575   52  864   93  881 1112 1069  509   70]\n","topic 16 : Acne, snarl, interference, product, improvement, cathode, colds, criminals, lightning, Progress\n","[993 277 819 986 407 327 189 697 878 873]\n","topic 17 : spilling, drama, structure, irritation, bending, seas, strike, whistle, Fog, drinking\n","[ 692 1098   34   99  705  496  690  189  473  380]\n","topic 18 : wire, afterglow, refinement, arrest, sorrow, education, taxes, strike, drug use, disease\n","[1020  179  579  835  814   65  777  141  939  654]\n","topic 19 : profits, quenches, gains, aftershock, preservation, drainage, viruses, bacterium, taste, insomnia\n","[949  87 810  72 624 760 229  82  39 538]\n","topic 20 : imbalances, cancellations, victory, execution, Electrolysis, fracture, vaccines, smoking, corkscrew, Convulsions\n","[1076  341  303   51  734 1077  544  441  533  190]\n","topic 21 : attacks, suffocation, adhesion, militancy, eyelids, strategies, tardiness, killings, stitches, Lymphedema\n","[804  56 939 923  39 748 733  23 293 206]\n","topic 22 : expenditure, pleasure, taste, conflict, corkscrew, swelling, disorders, pinkeye, conditions, passage\n","[ 388  396  749  296  377  372  760  380  293 1008]\n","topic 23 : ionization, wildfires, chord, current, intubation, temperature, fracture, disease, conditions, launch\n","[562 514  21  95 458 749 374 576 434 206]\n","topic 24 : perfume, shooting, consumers, collision, Germs, chord, morbidity, scandals, mistake, passage\n","[ 184   57 1100  826  110 1040  785  547  552  828]\n","topic 25 : neuroma, cigarettes, immigration, drugs, wave, paralysis, vibrations, device, incidents, debt\n","[1051  683  732  838  297  227   46    8  807 1032]\n","topic 26 : tip, jams, patriots, desires, antidepressants, blockages, cyclone, abuse, staring effect, discharge\n","[ 645  355   45  248  690 1105  266  229  351 1100]\n","topic 27 : fuels, dumping, rainfall, distention, taxes, compromise, Toxoplasmosis, vaccines, infection, immigration\n","[434 998 886 193 530 851 246  16 127 970]\n","topic 28 : mistake, cancer, applicants, operations, debate, increase, sidewalk, emigration, sinus pressure, consumption\n","[1102  609 1045  688  932  366  730  308 1127  368]\n","topic 29 : protozoa, poisoning, chaos, shoulder problems, corrosion, contamination, spiciness, driver, Soya farming, eating\n","[785  17 386  72 828 494 543 715 631 121]\n","topic 30 : vibrations, convergence, terrorist, execution, debt, sale, reboot, bacteria, silversmiths, car\n","[ 100 1076  121  684  751   51  734  623  719 1096]\n","topic 31 : debris, attacks, car, presentation, sound, militancy, eyelids, aftershocks, Properties, pyrotechnics\n","[ 281  698  689  852  942 1103  112  889  252  352]\n","topic 32 : choking, effect, amphibolites, decomposition reaction, poverty, protest, teaching, recurrence, scandal, threats\n","[189 681  43 484 705  10 355 791 224 360]\n","topic 33 : strike, jolliness, flashlight, person, sorrow, grief, dumping, cementation, women, suicide\n","[ 906   73  562  161 1105  233  919   24  190 1077]\n","topic 34 : meltdown, losses, perfume, exertion, compromise, flight, devastations, princess, Lymphedema, strategies\n","[187 358 131 372 862 246 172 210 560 562]\n","topic 35 : blackout, confidence, leak, temperature, writing, sidewalk, fog, vortex-densities, blockage, perfume\n","[339 839 713 911  98 474 807 193 958 377]\n","topic 36 : infertility, acid, weather, dispute, migraines, relocation, staring effect, operations, gases, intubation\n","[ 241   95  873  464  970  324 1032  350 1090  910]\n","topic 37 : Exposure, collision, drinking, bruises, consumption, proliferation, discharge, Inhibition, stress, discoloration\n","[ 338 1025  780  988  749  614  197  183  425  436]\n","topic 38 : maker, steam, intersection, transmitter, chord, invasion, movie, winds, War, boiling water\n","[807 935 514 984 852 732  17 121  46  13]\n","topic 39 : staring effect, beam, shooting, chemicals, decomposition reaction, patriots, convergence, car, cyclone, delusions\n","[ 993  944  810  505 1110  528   88  427  631 1112]\n","topic 40 : spilling, system, victory, leaks, rill, crisis, opposition, Asthma, silversmiths, colds\n","[ 224  776  663  657  958  189  222  355 1112  735]\n","topic 41 : women, Vulvodynia, pain killers, fumes, gases, strike, singing, dumping, colds, alibi\n","[ 312  437  828  986  790  377  768  879 1129  735]\n","topic 42 : boiler, papillomavirus, debt, irritation, disability, intubation, terror, problems, washing, alibi\n","[1038  578  347  287  831  870   11 1096  112  139]\n","topic 43 : conquests, vomiting, allergies, tears, resonance, Outbreaks, optimism, pyrotechnics, teaching, Flashes\n","[374 664 576 663 797 845 994 588 857  51]\n","topic 44 : morbidity, surplus, scandals, pain killers, Production, Addiction, asteroid, Anger, beverages, militancy\n","[1076  772  812  718  374  269  100  751  352 1113]\n","topic 45 : attacks, comet, claustrophobia, medication, morbidity, pressure, debris, sound, threats, anger\n","[ 641  546  655  272  993  229  304 1033  649  957]\n","topic 46 : Famine, crack, heroin, running, spilling, vaccines, Constipation, rendition, floods, revolution\n","[ 51 308 281 775 390  11 558  63 289 688]\n","topic 47 : militancy, driver, choking, fuel leak, deprivation, optimism, machine, proof, lavas, shoulder problems\n","[ 970  742  404  473  180  947  652  632 1035  222]\n","topic 48 : consumption, research, biography, drug use, visit, lack, Trauma, screen, malfunctions, singing\n","[ 684   23  210  857   17 1128  975   88  733  187]\n","topic 49 : presentation, pinkeye, vortex-densities, beverages, convergence, hormonal changes, officers, opposition, disorders, blackout\n","======================================================================\n","\n","\n","\n","======================================================================\n"]}]},{"cell_type":"code","source":["# semeval label print topic list\n","print(\"Finally after training\")\n","semeval_label_net.eval()\n","\n","print(\"Topics with probability argmax\")\n","prob_over_vocab_np, topics_print_list = semeval_label_net.rank_vocab_for_topics(\n","            word_embedding_matrix=semeval_label_embs_matrix_np, to_be_removed=semeval_label_to_be_removed\n","    )\n","print(\"=\" * 70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhkEIXPtrSf6","executionInfo":{"status":"ok","timestamp":1668723598613,"user_tz":300,"elapsed":29,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"b69f1ba9-88b8-48c4-c3eb-7a48bfb1598e"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Finally after training\n","Topics with probability argmax\n","50\n","50\n","[372 797 845 941 688 582 576 846 970 432]\n","topic 0 : temperature, Production, Addiction, influenza, shoulder problems, deluge, scandals, pie, consumption, Sadness\n","[481 419 664  54  40 979 300 638 887 903]\n","topic 1 : competition, increased pressure, surplus, flood, bite, pathogens, guy, harassment, Poverty, relaxation\n","[ 350 1061  497  948 1094  316  259    8  194 1032]\n","topic 2 : Inhibition, Sodium, malfunction, celebs, collapse, nerves, neglect, abuse, gash, discharge\n","[ 694 1128 1056  400  652  675    1  629  727 1093]\n","topic 3 : resignation, hormonal changes, legs, substance, Trauma, books, rising unemployment rate, anticipation, symptoms, profit\n","[ 251  438   86  769  293 1019  422 1086  912  770]\n","topic 4 : rubbing, toll, demolition, bike-accident, conditions, cancellation, condition, separation field, Bed sores, dams\n","[ 264  378 1024  634  610  913  335  398  337  471]\n","topic 5 : firing, Boils, particles, extinction, Osteoporosis, swing, rashes, affect, circulation, Scleroderma\n","[1009  771  795  554  449  963  172  540  774  314]\n","topic 6 : abdominal pain, belief, flammable liquids, jaw problem, overflows, lorries, fog, palpitations, convulsion, Interpolation\n","[ 342  204 1031  315  807  940  636  820  352  385]\n","topic 7 : cars, breeding, ointment, alterations, staring effect, joy, species, indicator, threats, preservatives\n","[ 675  100  374  588  653  617   94  548  665 1128]\n","topic 8 : books, debris, morbidity, Anger, plate, studies, Birth defects, emergency, pressures, hormonal changes\n","[ 910  872  497  538  988  961  362 1090  619 1128]\n","topic 9 : discoloration, Cities, malfunction, Convulsions, transmitter, hitting, hydraulic mining, stress, vocals, hormonal changes\n","[1028  480   35 1111  150   50 1125   96  562 1077]\n","topic 10 : cell phones, computers, burns, eruption, reduction, dwarf, trend, bomb explosion, perfume, strategies\n","[ 87 962 770 503 145 749 206 293 255 824]\n","topic 11 : cancellations, slowdown, dams, Malaria, vaccum, chord, passage, conditions, cockroaches, thorns\n","[ 170 1090  190   24 1069 1111  887   96 1125  745]\n","topic 12 : press, stress, Lymphedema, princess, criminals, eruption, Poverty, bomb explosion, trend, people\n","[ 408   34 1104  119  685  875  949  154  339  865]\n","topic 13 : nicotine, refinement, hormone, Discomfort, gun, filament, imbalances, shockwaves, infertility, information\n","[379 116 181 298 984 817 476 534 268 970]\n","topic 14 : outrage, negotiations, hardening, Menopause, chemicals, chime, Entrepreneurship, fighting, absorption, consumption\n","[962 526 262 400 947 495 566 224 170 355]\n","topic 15 : slowdown, burn, fungi, substance, lack, alarm, flooding, women, press, dumping\n","[1069  368  986   93  282   79  258 1049  810   70]\n","topic 16 : criminals, eating, irritation, improvement, arrests, hormonal imbalance, tendinitis, mold, victory, Progress\n","[1027 1003  798  589  492 1014  310  241  647 1032]\n","topic 17 : watering, toothache, influx, meningitis, radiation exposures, pop, Zinc, Exposure, command, discharge\n","[ 690  919  380  292  705   99 1041  473  496  425]\n","topic 18 : taxes, devastations, disease, hardship, sorrow, arrest, illnesses, drug use, education, War\n","[ 581   71 1043  814   65  827  114  803  141  183]\n","topic 19 : praise, lamp, sunrise, preservation, drainage, shock waves, disruption, media, bacterium, winds\n","[202 624  39 780 893  82 803 969 958 538]\n","topic 20 : arterial blood pressure, Electrolysis, corkscrew, intersection, electricity, smoking, media, stroke, gases, Convulsions\n","[  73   83  734 1015  168  269  341  533 1077  645]\n","topic 21 : losses, hull, eyelids, inspiration, expansions, pressure, suffocation, stitches, strategies, fuels\n","[893 863  17 890 202   2 733 543 206  23]\n","topic 22 : electricity, drill, convergence, shingles, arterial blood pressure, progress, disorders, reboot, passage, pinkeye\n","[ 406  427  438   87  926  372 1008  131 1109  271]\n","topic 23 : inflammation, Asthma, toll, cancellations, illness, temperature, launch, leak, tornado, airline\n","[  21  562  588  458  206  434  824 1008  749  778]\n","topic 24 : consumers, perfume, Anger, Germs, passage, mistake, thorns, launch, chord, experiences\n","[1025  331  785 1058  595  828  547  684  190  512]\n","topic 25 : steam, inquiry, vibrations, aggressions, moisture, debt, device, presentation, Lymphedema, contraction\n","[ 576  998   94  846 1026  548 1032    8  194  888]\n","topic 26 : scandals, cancer, Birth defects, pie, havoc, emergency, discharge, abuse, gash, clot\n","[ 292  247 1077  725 1041 1100  375  266 1076  645]\n","topic 27 : hardship, gas, strategies, redistribution, illnesses, immigration, divorce, Toxoplasmosis, attacks, fuels\n","[807 356 576 749 206 432 530 251 630 886]\n","topic 28 : staring effect, tone, scandals, chord, passage, Sadness, debate, rubbing, death, applicants\n","[1102  688  917  201  831  730  405  498  772  423]\n","topic 29 : protozoa, shoulder problems, failures, physician, resonance, spiciness, birthmarks, cocaine, comet, waterjet\n","[805  72   2 733 804 430 386 631  17 121]\n","topic 30 : tests, execution, progress, disorders, expenditure, imbalance, terrorist, silversmiths, convergence, car\n","[1015  481  686  280  633   51 1036   40  719 1096]\n","topic 31 : inspiration, competition, properties, Preeclampsia, science, militancy, child abuse, bite, Properties, pyrotechnics\n","[909 204 315 637 867 385 759 259 352 889]\n","topic 32 : pustules, breeding, alterations, blaze, dust, preservatives, programmer, neglect, threats, recurrence\n","[566 484 671 260 381 396 567 495 947 355]\n","topic 33 : flooding, person, gravitational force, recession, eyestrain, wildfires, transmission, alarm, lack, dumping\n","[ 687 1061  233  621   24   73  190 1077  745  906]\n","topic 34 : evaporation, Sodium, flight, caffeine withdrawal, princess, losses, Lymphedema, strategies, people, meltdown\n","[501 957  69 568  26 210 559 318 562 888]\n","topic 35 : design, revolution, perturbation, law, echoing, vortex-densities, acupuncture, laughing, perfume, clot\n","[ 790  935  377  843   29  901 1008   98  713  807]\n","topic 36 : disability, beam, intubation, injury, producer, frustration, launch, migraines, weather, staring effect\n","[ 873   95  715 1032  350  492  241  497  324  910]\n","topic 37 : drinking, collision, bacteria, discharge, Inhibition, radiation exposures, Exposure, malfunction, proliferation, discoloration\n","[950 958 183 895 438 526 273 832 912  63]\n","topic 38 : Dehydration, gases, winds, process, toll, burn, roughness, bills, Bed sores, proof\n","[ 683  121  831 1096  807  665  430  704  648  790]\n","topic 39 : jams, car, resonance, pyrotechnics, staring effect, pressures, imbalance, pollution, impression, disability\n","[ 944  528  631  505    7  327  625 1110  773 1112]\n","topic 40 : system, crisis, silversmiths, leaks, floodwaters, seas, man, rill, effects, colds\n","[396 181 260 484 353 224 360 662 355 567]\n","topic 41 : wildfires, hardening, recession, person, ginseng, women, suicide, happiness, dumping, transmission\n","[ 476  567 1129  790  123  678  435  791  377  735]\n","topic 42 : Entrepreneurship, transmission, washing, disability, range, behaviour, waste, cementation, intubation, alibi\n","[ 101  500 1075  623 1038  870  112  849  831 1096]\n","topic 43 : hematoma, disturbances, anesthetics, aftershocks, conquests, Outbreaks, teaching, migrations, resonance, pyrotechnics\n","[663 645 797  47 994  51 857 588 846 759]\n","topic 44 : pain killers, fuels, Production, disc, asteroid, militancy, beverages, Anger, pie, programmer\n","[309 213  70 653 196  68 442 751 812 269]\n","topic 45 : tumor, algorithm, Progress, plate, dermatitis, streaks, figure, sound, claustrophobia, pressure\n","[ 450  993  258  123   50   93  487  553 1014  910]\n","topic 46 : policies, spilling, tendinitis, range, dwarf, improvement, drinks, release, pop, discoloration\n","[173 281 218 471 383 390 688 311 289 382]\n","topic 47 : Rabies, choking, disasters, Scleroderma, awareness, deprivation, shoulder problems, power, lavas, bout\n","[ 409 1090  619  262  632  988  961  947  495  970]\n","topic 48 : power outage, stress, vocals, fungi, screen, transmitter, hitting, lack, alarm, consumption\n","[941 674 631 430 684 846  17 210 187  88]\n","topic 49 : influenza, thought, silversmiths, imbalance, presentation, pie, convergence, vortex-densities, blackout, opposition\n","======================================================================\n"]}]},{"cell_type":"code","source":["# after training we evaluate all the topics percentage in the dataset and rank the topics by percentage\n","uid_list, vector_list = zip(*semeval_label_uid_input_vector_list)\n","topic_pred_list = text_to_topic(vector_list, semeval_label_net, 'cuda')\n","\n","topic_id_ranked, topic_percentage_ranked = rank_topics_by_percentage( topic_pred_list )\n","\n","for rank, (topic_id, topic_percentage) in enumerate( zip(topic_id_ranked, topic_percentage_ranked)):\n","    print(\n","            f\"Rank: {rank}, Topic_id: {topic_id}, Topic Words: {topics_print_list[topic_id]}, \\\n","            Topic Percentage: {topic_percentage}\"\n","        )\n"],"metadata":{"id":"lrua2j6ZVqlM","executionInfo":{"status":"ok","timestamp":1668723598935,"user_tz":300,"elapsed":326,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3e64c8d-5961-472b-e007-206606db7a9a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:00<00:00, 488.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Rank: 0, Topic_id: 48, Topic Words: topic 48 : power outage, stress, vocals, fungi, screen, transmitter, hitting, lack, alarm, consumption,             Topic Percentage: 9.67\n","Rank: 1, Topic_id: 31, Topic Words: topic 31 : inspiration, competition, properties, Preeclampsia, science, militancy, child abuse, bite, Properties, pyrotechnics,             Topic Percentage: 7.58\n","Rank: 2, Topic_id: 26, Topic Words: topic 26 : scandals, cancer, Birth defects, pie, havoc, emergency, discharge, abuse, gash, clot,             Topic Percentage: 7.38\n","Rank: 3, Topic_id: 22, Topic Words: topic 22 : electricity, drill, convergence, shingles, arterial blood pressure, progress, disorders, reboot, passage, pinkeye,             Topic Percentage: 7.08\n","Rank: 4, Topic_id: 15, Topic Words: topic 15 : slowdown, burn, fungi, substance, lack, alarm, flooding, women, press, dumping,             Topic Percentage: 6.28\n","Rank: 5, Topic_id: 38, Topic Words: topic 38 : Dehydration, gases, winds, process, toll, burn, roughness, bills, Bed sores, proof,             Topic Percentage: 6.18\n","Rank: 6, Topic_id: 32, Topic Words: topic 32 : pustules, breeding, alterations, blaze, dust, preservatives, programmer, neglect, threats, recurrence,             Topic Percentage: 4.79\n","Rank: 7, Topic_id: 45, Topic Words: topic 45 : tumor, algorithm, Progress, plate, dermatitis, streaks, figure, sound, claustrophobia, pressure,             Topic Percentage: 4.29\n","Rank: 8, Topic_id: 33, Topic Words: topic 33 : flooding, person, gravitational force, recession, eyestrain, wildfires, transmission, alarm, lack, dumping,             Topic Percentage: 3.19\n","Rank: 9, Topic_id: 39, Topic Words: topic 39 : jams, car, resonance, pyrotechnics, staring effect, pressures, imbalance, pollution, impression, disability,             Topic Percentage: 3.09\n","Rank: 10, Topic_id: 16, Topic Words: topic 16 : criminals, eating, irritation, improvement, arrests, hormonal imbalance, tendinitis, mold, victory, Progress,             Topic Percentage: 2.99\n","Rank: 11, Topic_id: 6, Topic Words: topic 6 : abdominal pain, belief, flammable liquids, jaw problem, overflows, lorries, fog, palpitations, convulsion, Interpolation,             Topic Percentage: 2.79\n","Rank: 12, Topic_id: 0, Topic Words: topic 0 : temperature, Production, Addiction, influenza, shoulder problems, deluge, scandals, pie, consumption, Sadness,             Topic Percentage: 2.79\n","Rank: 13, Topic_id: 4, Topic Words: topic 4 : rubbing, toll, demolition, bike-accident, conditions, cancellation, condition, separation field, Bed sores, dams,             Topic Percentage: 2.59\n","Rank: 14, Topic_id: 9, Topic Words: topic 9 : discoloration, Cities, malfunction, Convulsions, transmitter, hitting, hydraulic mining, stress, vocals, hormonal changes,             Topic Percentage: 2.29\n","Rank: 15, Topic_id: 18, Topic Words: topic 18 : taxes, devastations, disease, hardship, sorrow, arrest, illnesses, drug use, education, War,             Topic Percentage: 2.09\n","Rank: 16, Topic_id: 46, Topic Words: topic 46 : policies, spilling, tendinitis, range, dwarf, improvement, drinks, release, pop, discoloration,             Topic Percentage: 1.99\n","Rank: 17, Topic_id: 34, Topic Words: topic 34 : evaporation, Sodium, flight, caffeine withdrawal, princess, losses, Lymphedema, strategies, people, meltdown,             Topic Percentage: 1.89\n","Rank: 18, Topic_id: 19, Topic Words: topic 19 : praise, lamp, sunrise, preservation, drainage, shock waves, disruption, media, bacterium, winds,             Topic Percentage: 1.69\n","Rank: 19, Topic_id: 44, Topic Words: topic 44 : pain killers, fuels, Production, disc, asteroid, militancy, beverages, Anger, pie, programmer,             Topic Percentage: 1.69\n","Rank: 20, Topic_id: 35, Topic Words: topic 35 : design, revolution, perturbation, law, echoing, vortex-densities, acupuncture, laughing, perfume, clot,             Topic Percentage: 1.6\n","Rank: 21, Topic_id: 14, Topic Words: topic 14 : outrage, negotiations, hardening, Menopause, chemicals, chime, Entrepreneurship, fighting, absorption, consumption,             Topic Percentage: 1.6\n","Rank: 22, Topic_id: 40, Topic Words: topic 40 : system, crisis, silversmiths, leaks, floodwaters, seas, man, rill, effects, colds,             Topic Percentage: 1.3\n","Rank: 23, Topic_id: 41, Topic Words: topic 41 : wildfires, hardening, recession, person, ginseng, women, suicide, happiness, dumping, transmission,             Topic Percentage: 1.3\n","Rank: 24, Topic_id: 10, Topic Words: topic 10 : cell phones, computers, burns, eruption, reduction, dwarf, trend, bomb explosion, perfume, strategies,             Topic Percentage: 1.3\n","Rank: 25, Topic_id: 11, Topic Words: topic 11 : cancellations, slowdown, dams, Malaria, vaccum, chord, passage, conditions, cockroaches, thorns,             Topic Percentage: 1.1\n","Rank: 26, Topic_id: 36, Topic Words: topic 36 : disability, beam, intubation, injury, producer, frustration, launch, migraines, weather, staring effect,             Topic Percentage: 1.0\n","Rank: 27, Topic_id: 29, Topic Words: topic 29 : protozoa, shoulder problems, failures, physician, resonance, spiciness, birthmarks, cocaine, comet, waterjet,             Topic Percentage: 0.9\n","Rank: 28, Topic_id: 30, Topic Words: topic 30 : tests, execution, progress, disorders, expenditure, imbalance, terrorist, silversmiths, convergence, car,             Topic Percentage: 0.9\n","Rank: 29, Topic_id: 12, Topic Words: topic 12 : press, stress, Lymphedema, princess, criminals, eruption, Poverty, bomb explosion, trend, people,             Topic Percentage: 0.9\n","Rank: 30, Topic_id: 43, Topic Words: topic 43 : hematoma, disturbances, anesthetics, aftershocks, conquests, Outbreaks, teaching, migrations, resonance, pyrotechnics,             Topic Percentage: 0.9\n","Rank: 31, Topic_id: 13, Topic Words: topic 13 : nicotine, refinement, hormone, Discomfort, gun, filament, imbalances, shockwaves, infertility, information,             Topic Percentage: 0.8\n","Rank: 32, Topic_id: 27, Topic Words: topic 27 : hardship, gas, strategies, redistribution, illnesses, immigration, divorce, Toxoplasmosis, attacks, fuels,             Topic Percentage: 0.7\n","Rank: 33, Topic_id: 8, Topic Words: topic 8 : books, debris, morbidity, Anger, plate, studies, Birth defects, emergency, pressures, hormonal changes,             Topic Percentage: 0.6\n","Rank: 34, Topic_id: 7, Topic Words: topic 7 : cars, breeding, ointment, alterations, staring effect, joy, species, indicator, threats, preservatives,             Topic Percentage: 0.5\n","Rank: 35, Topic_id: 47, Topic Words: topic 47 : Rabies, choking, disasters, Scleroderma, awareness, deprivation, shoulder problems, power, lavas, bout,             Topic Percentage: 0.5\n","Rank: 36, Topic_id: 5, Topic Words: topic 5 : firing, Boils, particles, extinction, Osteoporosis, swing, rashes, affect, circulation, Scleroderma,             Topic Percentage: 0.4\n","Rank: 37, Topic_id: 1, Topic Words: topic 1 : competition, increased pressure, surplus, flood, bite, pathogens, guy, harassment, Poverty, relaxation,             Topic Percentage: 0.4\n","Rank: 38, Topic_id: 23, Topic Words: topic 23 : inflammation, Asthma, toll, cancellations, illness, temperature, launch, leak, tornado, airline,             Topic Percentage: 0.4\n","Rank: 39, Topic_id: 24, Topic Words: topic 24 : consumers, perfume, Anger, Germs, passage, mistake, thorns, launch, chord, experiences,             Topic Percentage: 0.4\n","Rank: 40, Topic_id: 42, Topic Words: topic 42 : Entrepreneurship, transmission, washing, disability, range, behaviour, waste, cementation, intubation, alibi,             Topic Percentage: 0.1\n","Rank: 41, Topic_id: 3, Topic Words: topic 3 : resignation, hormonal changes, legs, substance, Trauma, books, rising unemployment rate, anticipation, symptoms, profit,             Topic Percentage: 0.1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["prob_over_vocab_df = pd.DataFrame(prob_over_vocab_np) # shape: 50 x 1131\n","prob_over_vocab_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"gFbPwR6wPy-i","executionInfo":{"status":"ok","timestamp":1668723598936,"user_tz":300,"elapsed":10,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"3480b37d-7533-4421-e235-3c280c48d0d1"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       0         1         2         3         4         5         6     \\\n","0  0.000874  0.001002  0.001406  0.001139  0.000745  0.000712  0.000837   \n","1  0.000965  0.000786  0.000933  0.001010  0.000944  0.001202  0.000856   \n","2  0.000930  0.000553  0.000979  0.000761  0.001020  0.000920  0.000899   \n","3  0.000906  0.001484  0.000851  0.001184  0.000700  0.000715  0.001018   \n","4  0.000783  0.000980  0.000763  0.000896  0.001073  0.000870  0.000911   \n","\n","       7         8         9     ...      1121      1122      1123      1124  \\\n","0  0.000529  0.001343  0.000847  ...  0.000684  0.000562  0.000787  0.000835   \n","1  0.000855  0.000639  0.000770  ...  0.001037  0.000811  0.000794  0.000997   \n","2  0.000889  0.001199  0.000974  ...  0.000785  0.000790  0.000975  0.001023   \n","3  0.000902  0.000920  0.000879  ...  0.000880  0.000874  0.000552  0.000576   \n","4  0.000952  0.000758  0.000918  ...  0.000552  0.000962  0.000895  0.001043   \n","\n","       1125      1126      1127      1128      1129      1130  \n","0  0.000704  0.001145  0.000804  0.001376  0.000634  0.000930  \n","1  0.000994  0.000813  0.000796  0.000787  0.000685  0.000951  \n","2  0.000793  0.000977  0.000774  0.000817  0.000863  0.000932  \n","3  0.000884  0.000881  0.000667  0.001408  0.000664  0.001035  \n","4  0.000766  0.000925  0.000817  0.000656  0.000901  0.001124  \n","\n","[5 rows x 1131 columns]"],"text/html":["\n","  <div id=\"df-04ccfa9c-7ac5-45c1-8182-6b4aacbf29ee\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>1121</th>\n","      <th>1122</th>\n","      <th>1123</th>\n","      <th>1124</th>\n","      <th>1125</th>\n","      <th>1126</th>\n","      <th>1127</th>\n","      <th>1128</th>\n","      <th>1129</th>\n","      <th>1130</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000874</td>\n","      <td>0.001002</td>\n","      <td>0.001406</td>\n","      <td>0.001139</td>\n","      <td>0.000745</td>\n","      <td>0.000712</td>\n","      <td>0.000837</td>\n","      <td>0.000529</td>\n","      <td>0.001343</td>\n","      <td>0.000847</td>\n","      <td>...</td>\n","      <td>0.000684</td>\n","      <td>0.000562</td>\n","      <td>0.000787</td>\n","      <td>0.000835</td>\n","      <td>0.000704</td>\n","      <td>0.001145</td>\n","      <td>0.000804</td>\n","      <td>0.001376</td>\n","      <td>0.000634</td>\n","      <td>0.000930</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000965</td>\n","      <td>0.000786</td>\n","      <td>0.000933</td>\n","      <td>0.001010</td>\n","      <td>0.000944</td>\n","      <td>0.001202</td>\n","      <td>0.000856</td>\n","      <td>0.000855</td>\n","      <td>0.000639</td>\n","      <td>0.000770</td>\n","      <td>...</td>\n","      <td>0.001037</td>\n","      <td>0.000811</td>\n","      <td>0.000794</td>\n","      <td>0.000997</td>\n","      <td>0.000994</td>\n","      <td>0.000813</td>\n","      <td>0.000796</td>\n","      <td>0.000787</td>\n","      <td>0.000685</td>\n","      <td>0.000951</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000930</td>\n","      <td>0.000553</td>\n","      <td>0.000979</td>\n","      <td>0.000761</td>\n","      <td>0.001020</td>\n","      <td>0.000920</td>\n","      <td>0.000899</td>\n","      <td>0.000889</td>\n","      <td>0.001199</td>\n","      <td>0.000974</td>\n","      <td>...</td>\n","      <td>0.000785</td>\n","      <td>0.000790</td>\n","      <td>0.000975</td>\n","      <td>0.001023</td>\n","      <td>0.000793</td>\n","      <td>0.000977</td>\n","      <td>0.000774</td>\n","      <td>0.000817</td>\n","      <td>0.000863</td>\n","      <td>0.000932</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000906</td>\n","      <td>0.001484</td>\n","      <td>0.000851</td>\n","      <td>0.001184</td>\n","      <td>0.000700</td>\n","      <td>0.000715</td>\n","      <td>0.001018</td>\n","      <td>0.000902</td>\n","      <td>0.000920</td>\n","      <td>0.000879</td>\n","      <td>...</td>\n","      <td>0.000880</td>\n","      <td>0.000874</td>\n","      <td>0.000552</td>\n","      <td>0.000576</td>\n","      <td>0.000884</td>\n","      <td>0.000881</td>\n","      <td>0.000667</td>\n","      <td>0.001408</td>\n","      <td>0.000664</td>\n","      <td>0.001035</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000783</td>\n","      <td>0.000980</td>\n","      <td>0.000763</td>\n","      <td>0.000896</td>\n","      <td>0.001073</td>\n","      <td>0.000870</td>\n","      <td>0.000911</td>\n","      <td>0.000952</td>\n","      <td>0.000758</td>\n","      <td>0.000918</td>\n","      <td>...</td>\n","      <td>0.000552</td>\n","      <td>0.000962</td>\n","      <td>0.000895</td>\n","      <td>0.001043</td>\n","      <td>0.000766</td>\n","      <td>0.000925</td>\n","      <td>0.000817</td>\n","      <td>0.000656</td>\n","      <td>0.000901</td>\n","      <td>0.001124</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1131 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04ccfa9c-7ac5-45c1-8182-6b4aacbf29ee')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-04ccfa9c-7ac5-45c1-8182-6b4aacbf29ee button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-04ccfa9c-7ac5-45c1-8182-6b4aacbf29ee');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["prob_over_vocab_df.idxmax() # for each column, find the row number of the max"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5hblaiGNXW0","executionInfo":{"status":"ok","timestamp":1668723598936,"user_tz":300,"elapsed":7,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"5edfbd99-60b8-4c81-e58e-2c605427de6c"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       43\n","1        3\n","2       22\n","3        9\n","4       38\n","        ..\n","1126    22\n","1127    39\n","1128     9\n","1129    42\n","1130    22\n","Length: 1131, dtype: int64"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["df = pd.DataFrame(prob_over_vocab_df.idxmax())\n","df = df.reset_index()\n","df"],"metadata":{"id":"qVAvNw7FROdp","executionInfo":{"status":"ok","timestamp":1668723646925,"user_tz":300,"elapsed":3,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"84a263b7-2b6a-4c7e-d525-489fe2447dd2"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      index   0\n","0         0  43\n","1         1   3\n","2         2  22\n","3         3   9\n","4         4  38\n","...     ...  ..\n","1126   1126  22\n","1127   1127  39\n","1128   1128   9\n","1129   1129  42\n","1130   1130  22\n","\n","[1131 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-c5772ba5-a9bb-4cff-abc3-123e51192b03\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1126</th>\n","      <td>1126</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>1127</th>\n","      <td>1127</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>1128</th>\n","      <td>1128</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1129</th>\n","      <td>1129</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>1130</th>\n","      <td>1130</td>\n","      <td>22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1131 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5772ba5-a9bb-4cff-abc3-123e51192b03')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c5772ba5-a9bb-4cff-abc3-123e51192b03 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c5772ba5-a9bb-4cff-abc3-123e51192b03');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# construct a dictionary with arg ids as keys and topic ids as values\n","argid2topic_dict = df[0].to_dict()"],"metadata":{"id":"nhVYKdvvrkul","executionInfo":{"status":"ok","timestamp":1668723660778,"user_tz":300,"elapsed":1,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# construct a dictionary with topic ids as keys and arg ids as values\n","# where only one topic is assigned to each argument\n","topic2argid_dict = df.groupby(0)['index'].apply(list).to_dict()"],"metadata":{"id":"7FRsR6Odrmed","executionInfo":{"status":"ok","timestamp":1668723666654,"user_tz":300,"elapsed":399,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["topic2argid_dict"],"metadata":{"id":"BfOtpXULtltf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668723668588,"user_tz":300,"elapsed":1,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"cd6d1158-e4b0-442c-b379-491c061a81da"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [12,\n","  48,\n","  95,\n","  131,\n","  138,\n","  155,\n","  178,\n","  198,\n","  219,\n","  246,\n","  251,\n","  296,\n","  305,\n","  311,\n","  346,\n","  372,\n","  373,\n","  378,\n","  399,\n","  432,\n","  434,\n","  452,\n","  502,\n","  529,\n","  530,\n","  576,\n","  582,\n","  601,\n","  614,\n","  624,\n","  640,\n","  654,\n","  655,\n","  688,\n","  694,\n","  699,\n","  740,\n","  749,\n","  778,\n","  797,\n","  803,\n","  834,\n","  841,\n","  845,\n","  846,\n","  857,\n","  863,\n","  882,\n","  890,\n","  893,\n","  910,\n","  920,\n","  926,\n","  941,\n","  954,\n","  968,\n","  969,\n","  970,\n","  971,\n","  975,\n","  1051,\n","  1064,\n","  1095,\n","  1118],\n"," 1: [5, 132, 392, 419, 520, 638, 664, 806, 887, 903, 979],\n"," 2: [316, 391, 508, 1094],\n"," 3: [1,\n","  125,\n","  142,\n","  159,\n","  215,\n","  231,\n","  237,\n","  278,\n","  307,\n","  416,\n","  493,\n","  583,\n","  629,\n","  652,\n","  657,\n","  675,\n","  702,\n","  727,\n","  741,\n","  755,\n","  801,\n","  884,\n","  997,\n","  1012,\n","  1024,\n","  1056,\n","  1093],\n"," 4: [27, 86, 609, 722, 767, 799, 809, 1052, 1086, 1117],\n"," 5: [20,\n","  21,\n","  102,\n","  105,\n","  169,\n","  182,\n","  225,\n","  226,\n","  264,\n","  335,\n","  337,\n","  354,\n","  387,\n","  398,\n","  471,\n","  585,\n","  610,\n","  634,\n","  660,\n","  708,\n","  721,\n","  750,\n","  830,\n","  913,\n","  1043],\n"," 6: [78,\n","  91,\n","  97,\n","  111,\n","  157,\n","  172,\n","  249,\n","  290,\n","  295,\n","  314,\n","  328,\n","  340,\n","  397,\n","  415,\n","  446,\n","  449,\n","  478,\n","  479,\n","  501,\n","  531,\n","  540,\n","  545,\n","  554,\n","  627,\n","  650,\n","  656,\n","  676,\n","  677,\n","  753,\n","  762,\n","  771,\n","  774,\n","  786,\n","  795,\n","  800,\n","  847,\n","  938,\n","  963,\n","  965,\n","  982,\n","  983,\n","  1002,\n","  1009,\n","  1022,\n","  1088,\n","  1107],\n"," 7: [148, 163, 212, 279, 344, 403, 488, 596, 820, 853, 953, 1031, 1073],\n"," 8: [37, 589, 617, 921],\n"," 9: [3,\n","  13,\n","  274,\n","  362,\n","  454,\n","  497,\n","  532,\n","  538,\n","  539,\n","  597,\n","  613,\n","  619,\n","  670,\n","  684,\n","  710,\n","  765,\n","  780,\n","  826,\n","  837,\n","  844,\n","  851,\n","  872,\n","  998,\n","  1082,\n","  1083,\n","  1090,\n","  1128],\n"," 10: [35,\n","  50,\n","  60,\n","  74,\n","  90,\n","  96,\n","  150,\n","  165,\n","  171,\n","  184,\n","  214,\n","  244,\n","  302,\n","  320,\n","  445,\n","  461,\n","  480,\n","  537,\n","  551,\n","  557,\n","  562,\n","  651,\n","  703,\n","  736,\n","  811,\n","  876,\n","  885,\n","  966,\n","  992,\n","  1040,\n","  1077,\n","  1092,\n","  1111,\n","  1119,\n","  1125],\n"," 11: [30,\n","  81,\n","  87,\n","  115,\n","  127,\n","  140,\n","  145,\n","  166,\n","  195,\n","  254,\n","  255,\n","  293,\n","  467,\n","  503,\n","  547,\n","  770,\n","  815,\n","  824,\n","  902,\n","  995,\n","  1006,\n","  1068,\n","  1084],\n"," 12: [611],\n"," 13: [32,\n","  154,\n","  158,\n","  326,\n","  339,\n","  408,\n","  448,\n","  456,\n","  485,\n","  563,\n","  602,\n","  731,\n","  746,\n","  865,\n","  875,\n","  928,\n","  981,\n","  1004,\n","  1101,\n","  1104],\n"," 14: [42,\n","  107,\n","  116,\n","  268,\n","  270,\n","  298,\n","  379,\n","  401,\n","  405,\n","  459,\n","  468,\n","  476,\n","  534,\n","  622,\n","  726,\n","  784,\n","  787,\n","  789,\n","  817,\n","  842,\n","  956,\n","  984,\n","  989,\n","  1037,\n","  1123],\n"," 15: [15,\n","  66,\n","  67,\n","  103,\n","  170,\n","  197,\n","  245,\n","  319,\n","  325,\n","  400,\n","  526,\n","  550,\n","  587,\n","  593,\n","  672,\n","  681,\n","  723,\n","  769,\n","  848,\n","  866,\n","  914,\n","  949,\n","  962,\n","  980,\n","  985,\n","  1067],\n"," 16: [52,\n","  70,\n","  79,\n","  93,\n","  104,\n","  109,\n","  137,\n","  147,\n","  201,\n","  209,\n","  229,\n","  258,\n","  275,\n","  282,\n","  297,\n","  300,\n","  306,\n","  389,\n","  418,\n","  428,\n","  439,\n","  462,\n","  509,\n","  510,\n","  513,\n","  524,\n","  569,\n","  616,\n","  642,\n","  668,\n","  691,\n","  754,\n","  810,\n","  840,\n","  860,\n","  864,\n","  881,\n","  899,\n","  917,\n","  930,\n","  955,\n","  974,\n","  977,\n","  1039,\n","  1049,\n","  1069,\n","  1074,\n","  1078,\n","  1091],\n"," 17: [19, 277, 310, 411, 482, 579, 615, 697, 798, 819, 1003, 1027, 1063],\n"," 18: [34, 58, 108, 313, 380, 521, 603, 816, 919, 1041, 1098],\n"," 19: [9, 33, 114, 141, 179, 299, 410, 518, 552, 581, 626, 827, 1020],\n"," 20: [82],\n"," 21: [73,\n","  83,\n","  126,\n","  164,\n","  168,\n","  276,\n","  291,\n","  333,\n","  341,\n","  349,\n","  364,\n","  414,\n","  441,\n","  472,\n","  483,\n","  515,\n","  533,\n","  544,\n","  594,\n","  645,\n","  695,\n","  734,\n","  821,\n","  854,\n","  972,\n","  1001,\n","  1015,\n","  1029,\n","  1050,\n","  1097,\n","  1105],\n"," 22: [2,\n","  23,\n","  56,\n","  65,\n","  117,\n","  144,\n","  202,\n","  206,\n","  238,\n","  334,\n","  412,\n","  427,\n","  431,\n","  494,\n","  519,\n","  543,\n","  595,\n","  717,\n","  733,\n","  748,\n","  814,\n","  825,\n","  855,\n","  858,\n","  908,\n","  923,\n","  939,\n","  952,\n","  1011,\n","  1087,\n","  1126,\n","  1130],\n"," 23: [136, 491, 549, 639, 1030],\n"," 25: [792, 1080],\n"," 26: [8,\n","  31,\n","  46,\n","  94,\n","  129,\n","  180,\n","  191,\n","  194,\n","  221,\n","  242,\n","  345,\n","  407,\n","  512,\n","  679,\n","  744,\n","  747,\n","  888,\n","  1010,\n","  1026,\n","  1032,\n","  1071],\n"," 27: [128, 247, 248, 266, 321, 375, 555, 725, 737, 818, 925, 945, 1076, 1100],\n"," 28: [356, 886],\n"," 29: [369, 423, 486, 498, 586, 605, 666, 730, 772, 1072],\n"," 30: [17,\n","  62,\n","  71,\n","  72,\n","  121,\n","  192,\n","  332,\n","  367,\n","  386,\n","  460,\n","  465,\n","  489,\n","  490,\n","  527,\n","  584,\n","  604,\n","  631,\n","  635,\n","  669,\n","  674,\n","  785,\n","  804,\n","  805,\n","  828,\n","  838,\n","  869,\n","  999,\n","  1013],\n"," 31: [40,\n","  44,\n","  49,\n","  54,\n","  106,\n","  156,\n","  160,\n","  177,\n","  185,\n","  280,\n","  402,\n","  481,\n","  556,\n","  633,\n","  719,\n","  904,\n","  990,\n","  1000,\n","  1005,\n","  1036],\n"," 32: [75,\n","  120,\n","  124,\n","  146,\n","  151,\n","  161,\n","  176,\n","  188,\n","  204,\n","  207,\n","  228,\n","  235,\n","  259,\n","  281,\n","  283,\n","  284,\n","  315,\n","  342,\n","  352,\n","  358,\n","  376,\n","  385,\n","  429,\n","  433,\n","  444,\n","  451,\n","  477,\n","  504,\n","  506,\n","  511,\n","  548,\n","  568,\n","  637,\n","  658,\n","  689,\n","  693,\n","  698,\n","  706,\n","  716,\n","  756,\n","  759,\n","  764,\n","  766,\n","  776,\n","  781,\n","  823,\n","  833,\n","  859,\n","  862,\n","  867,\n","  874,\n","  889,\n","  892,\n","  909,\n","  931,\n","  936,\n","  937,\n","  940,\n","  942,\n","  943,\n","  960,\n","  973,\n","  978,\n","  1028,\n","  1034,\n","  1045,\n","  1057,\n","  1079,\n","  1089,\n","  1102,\n","  1103,\n","  1108,\n","  1116],\n"," 33: [10,\n","  43,\n","  45,\n","  76,\n","  99,\n","  113,\n","  123,\n","  189,\n","  203,\n","  224,\n","  243,\n","  260,\n","  286,\n","  292,\n","  317,\n","  353,\n","  355,\n","  360,\n","  366,\n","  368,\n","  370,\n","  381,\n","  394,\n","  396,\n","  424,\n","  425,\n","  443,\n","  455,\n","  473,\n","  484,\n","  495,\n","  496,\n","  522,\n","  536,\n","  566,\n","  567,\n","  574,\n","  591,\n","  644,\n","  662,\n","  671,\n","  692,\n","  701,\n","  705,\n","  707,\n","  720,\n","  768,\n","  791,\n","  794,\n","  871,\n","  879,\n","  880,\n","  894,\n","  898,\n","  932,\n","  947,\n","  986,\n","  1065,\n","  1070,\n","  1106],\n"," 34: [24,\n","  25,\n","  41,\n","  77,\n","  84,\n","  135,\n","  190,\n","  220,\n","  233,\n","  234,\n","  571,\n","  618,\n","  621,\n","  643,\n","  687,\n","  709,\n","  745,\n","  829,\n","  906,\n","  1061],\n"," 35: [26, 69, 318, 357, 426, 628, 850, 1120],\n"," 36: [18,\n","  22,\n","  29,\n","  98,\n","  119,\n","  122,\n","  167,\n","  193,\n","  199,\n","  230,\n","  232,\n","  256,\n","  312,\n","  348,\n","  447,\n","  474,\n","  499,\n","  525,\n","  541,\n","  561,\n","  570,\n","  577,\n","  600,\n","  659,\n","  685,\n","  700,\n","  713,\n","  835,\n","  836,\n","  839,\n","  861,\n","  896,\n","  900,\n","  901,\n","  922,\n","  927,\n","  929,\n","  935,\n","  991,\n","  1007,\n","  1008,\n","  1085,\n","  1109,\n","  1114,\n","  1115],\n"," 37: [239, 241, 324, 350, 420, 453, 464, 492, 715, 779, 873, 1059],\n"," 38: [4,\n","  39,\n","  55,\n","  63,\n","  64,\n","  89,\n","  183,\n","  253,\n","  271,\n","  273,\n","  331,\n","  338,\n","  351,\n","  365,\n","  413,\n","  417,\n","  422,\n","  436,\n","  438,\n","  440,\n","  457,\n","  463,\n","  572,\n","  598,\n","  682,\n","  760,\n","  783,\n","  813,\n","  832,\n","  895,\n","  912,\n","  950,\n","  951,\n","  958,\n","  996,\n","  1016,\n","  1017,\n","  1021,\n","  1025,\n","  1044],\n"," 39: [6,\n","  7,\n","  14,\n","  16,\n","  80,\n","  200,\n","  267,\n","  301,\n","  323,\n","  371,\n","  393,\n","  430,\n","  437,\n","  458,\n","  475,\n","  514,\n","  599,\n","  636,\n","  648,\n","  665,\n","  680,\n","  683,\n","  686,\n","  704,\n","  711,\n","  732,\n","  743,\n","  790,\n","  807,\n","  843,\n","  852,\n","  856,\n","  877,\n","  911,\n","  964,\n","  967,\n","  1018,\n","  1023,\n","  1048,\n","  1060,\n","  1066,\n","  1127],\n"," 40: [53,\n","  208,\n","  250,\n","  257,\n","  327,\n","  359,\n","  505,\n","  528,\n","  625,\n","  696,\n","  773,\n","  897,\n","  944,\n","  1062,\n","  1110,\n","  1112,\n","  1122],\n"," 41: [186, 263, 336, 1047],\n"," 42: [223,\n","  227,\n","  288,\n","  377,\n","  435,\n","  466,\n","  590,\n","  661,\n","  667,\n","  678,\n","  724,\n","  735,\n","  777,\n","  883,\n","  946,\n","  1129],\n"," 43: [0,\n","  11,\n","  28,\n","  36,\n","  101,\n","  112,\n","  134,\n","  139,\n","  152,\n","  211,\n","  287,\n","  343,\n","  347,\n","  363,\n","  500,\n","  560,\n","  564,\n","  575,\n","  612,\n","  623,\n","  782,\n","  831,\n","  849,\n","  870,\n","  916,\n","  987,\n","  1038,\n","  1075,\n","  1096],\n"," 44: [47, 51, 303, 588, 592, 620, 663, 793, 891, 994],\n"," 45: [38,\n","  68,\n","  100,\n","  130,\n","  149,\n","  162,\n","  196,\n","  205,\n","  213,\n","  217,\n","  236,\n","  252,\n","  261,\n","  269,\n","  309,\n","  329,\n","  374,\n","  388,\n","  442,\n","  469,\n","  470,\n","  507,\n","  517,\n","  542,\n","  580,\n","  607,\n","  647,\n","  653,\n","  718,\n","  729,\n","  739,\n","  751,\n","  761,\n","  763,\n","  788,\n","  812,\n","  868,\n","  878,\n","  915,\n","  924,\n","  976,\n","  1054,\n","  1113,\n","  1121],\n"," 46: [57,\n","  61,\n","  85,\n","  92,\n","  118,\n","  133,\n","  143,\n","  265,\n","  272,\n","  304,\n","  361,\n","  395,\n","  421,\n","  450,\n","  487,\n","  516,\n","  546,\n","  553,\n","  559,\n","  606,\n","  641,\n","  646,\n","  649,\n","  673,\n","  728,\n","  757,\n","  808,\n","  822,\n","  905,\n","  907,\n","  933,\n","  957,\n","  993,\n","  1014,\n","  1033,\n","  1046,\n","  1053,\n","  1055,\n","  1099,\n","  1124],\n"," 47: [173,\n","  175,\n","  216,\n","  218,\n","  240,\n","  285,\n","  289,\n","  294,\n","  308,\n","  330,\n","  382,\n","  383,\n","  384,\n","  390,\n","  535,\n","  558,\n","  565,\n","  578,\n","  608,\n","  752,\n","  758,\n","  775,\n","  796,\n","  934,\n","  948,\n","  1081],\n"," 48: [59,\n","  110,\n","  153,\n","  174,\n","  181,\n","  222,\n","  262,\n","  322,\n","  404,\n","  406,\n","  409,\n","  523,\n","  573,\n","  630,\n","  632,\n","  690,\n","  714,\n","  738,\n","  742,\n","  802,\n","  918,\n","  959,\n","  961,\n","  988,\n","  1019,\n","  1035,\n","  1042,\n","  1058],\n"," 49: [88, 187, 210, 712]}"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["argid2topic_dict"],"metadata":{"id":"63Ju35yntmTP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668723669315,"user_tz":300,"elapsed":1,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"37681121-6f94-426c-cdaf-36fe06604245"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 43,\n"," 1: 3,\n"," 2: 22,\n"," 3: 9,\n"," 4: 38,\n"," 5: 1,\n"," 6: 39,\n"," 7: 39,\n"," 8: 26,\n"," 9: 19,\n"," 10: 33,\n"," 11: 43,\n"," 12: 0,\n"," 13: 9,\n"," 14: 39,\n"," 15: 15,\n"," 16: 39,\n"," 17: 30,\n"," 18: 36,\n"," 19: 17,\n"," 20: 5,\n"," 21: 5,\n"," 22: 36,\n"," 23: 22,\n"," 24: 34,\n"," 25: 34,\n"," 26: 35,\n"," 27: 4,\n"," 28: 43,\n"," 29: 36,\n"," 30: 11,\n"," 31: 26,\n"," 32: 13,\n"," 33: 19,\n"," 34: 18,\n"," 35: 10,\n"," 36: 43,\n"," 37: 8,\n"," 38: 45,\n"," 39: 38,\n"," 40: 31,\n"," 41: 34,\n"," 42: 14,\n"," 43: 33,\n"," 44: 31,\n"," 45: 33,\n"," 46: 26,\n"," 47: 44,\n"," 48: 0,\n"," 49: 31,\n"," 50: 10,\n"," 51: 44,\n"," 52: 16,\n"," 53: 40,\n"," 54: 31,\n"," 55: 38,\n"," 56: 22,\n"," 57: 46,\n"," 58: 18,\n"," 59: 48,\n"," 60: 10,\n"," 61: 46,\n"," 62: 30,\n"," 63: 38,\n"," 64: 38,\n"," 65: 22,\n"," 66: 15,\n"," 67: 15,\n"," 68: 45,\n"," 69: 35,\n"," 70: 16,\n"," 71: 30,\n"," 72: 30,\n"," 73: 21,\n"," 74: 10,\n"," 75: 32,\n"," 76: 33,\n"," 77: 34,\n"," 78: 6,\n"," 79: 16,\n"," 80: 39,\n"," 81: 11,\n"," 82: 20,\n"," 83: 21,\n"," 84: 34,\n"," 85: 46,\n"," 86: 4,\n"," 87: 11,\n"," 88: 49,\n"," 89: 38,\n"," 90: 10,\n"," 91: 6,\n"," 92: 46,\n"," 93: 16,\n"," 94: 26,\n"," 95: 0,\n"," 96: 10,\n"," 97: 6,\n"," 98: 36,\n"," 99: 33,\n"," 100: 45,\n"," 101: 43,\n"," 102: 5,\n"," 103: 15,\n"," 104: 16,\n"," 105: 5,\n"," 106: 31,\n"," 107: 14,\n"," 108: 18,\n"," 109: 16,\n"," 110: 48,\n"," 111: 6,\n"," 112: 43,\n"," 113: 33,\n"," 114: 19,\n"," 115: 11,\n"," 116: 14,\n"," 117: 22,\n"," 118: 46,\n"," 119: 36,\n"," 120: 32,\n"," 121: 30,\n"," 122: 36,\n"," 123: 33,\n"," 124: 32,\n"," 125: 3,\n"," 126: 21,\n"," 127: 11,\n"," 128: 27,\n"," 129: 26,\n"," 130: 45,\n"," 131: 0,\n"," 132: 1,\n"," 133: 46,\n"," 134: 43,\n"," 135: 34,\n"," 136: 23,\n"," 137: 16,\n"," 138: 0,\n"," 139: 43,\n"," 140: 11,\n"," 141: 19,\n"," 142: 3,\n"," 143: 46,\n"," 144: 22,\n"," 145: 11,\n"," 146: 32,\n"," 147: 16,\n"," 148: 7,\n"," 149: 45,\n"," 150: 10,\n"," 151: 32,\n"," 152: 43,\n"," 153: 48,\n"," 154: 13,\n"," 155: 0,\n"," 156: 31,\n"," 157: 6,\n"," 158: 13,\n"," 159: 3,\n"," 160: 31,\n"," 161: 32,\n"," 162: 45,\n"," 163: 7,\n"," 164: 21,\n"," 165: 10,\n"," 166: 11,\n"," 167: 36,\n"," 168: 21,\n"," 169: 5,\n"," 170: 15,\n"," 171: 10,\n"," 172: 6,\n"," 173: 47,\n"," 174: 48,\n"," 175: 47,\n"," 176: 32,\n"," 177: 31,\n"," 178: 0,\n"," 179: 19,\n"," 180: 26,\n"," 181: 48,\n"," 182: 5,\n"," 183: 38,\n"," 184: 10,\n"," 185: 31,\n"," 186: 41,\n"," 187: 49,\n"," 188: 32,\n"," 189: 33,\n"," 190: 34,\n"," 191: 26,\n"," 192: 30,\n"," 193: 36,\n"," 194: 26,\n"," 195: 11,\n"," 196: 45,\n"," 197: 15,\n"," 198: 0,\n"," 199: 36,\n"," 200: 39,\n"," 201: 16,\n"," 202: 22,\n"," 203: 33,\n"," 204: 32,\n"," 205: 45,\n"," 206: 22,\n"," 207: 32,\n"," 208: 40,\n"," 209: 16,\n"," 210: 49,\n"," 211: 43,\n"," 212: 7,\n"," 213: 45,\n"," 214: 10,\n"," 215: 3,\n"," 216: 47,\n"," 217: 45,\n"," 218: 47,\n"," 219: 0,\n"," 220: 34,\n"," 221: 26,\n"," 222: 48,\n"," 223: 42,\n"," 224: 33,\n"," 225: 5,\n"," 226: 5,\n"," 227: 42,\n"," 228: 32,\n"," 229: 16,\n"," 230: 36,\n"," 231: 3,\n"," 232: 36,\n"," 233: 34,\n"," 234: 34,\n"," 235: 32,\n"," 236: 45,\n"," 237: 3,\n"," 238: 22,\n"," 239: 37,\n"," 240: 47,\n"," 241: 37,\n"," 242: 26,\n"," 243: 33,\n"," 244: 10,\n"," 245: 15,\n"," 246: 0,\n"," 247: 27,\n"," 248: 27,\n"," 249: 6,\n"," 250: 40,\n"," 251: 0,\n"," 252: 45,\n"," 253: 38,\n"," 254: 11,\n"," 255: 11,\n"," 256: 36,\n"," 257: 40,\n"," 258: 16,\n"," 259: 32,\n"," 260: 33,\n"," 261: 45,\n"," 262: 48,\n"," 263: 41,\n"," 264: 5,\n"," 265: 46,\n"," 266: 27,\n"," 267: 39,\n"," 268: 14,\n"," 269: 45,\n"," 270: 14,\n"," 271: 38,\n"," 272: 46,\n"," 273: 38,\n"," 274: 9,\n"," 275: 16,\n"," 276: 21,\n"," 277: 17,\n"," 278: 3,\n"," 279: 7,\n"," 280: 31,\n"," 281: 32,\n"," 282: 16,\n"," 283: 32,\n"," 284: 32,\n"," 285: 47,\n"," 286: 33,\n"," 287: 43,\n"," 288: 42,\n"," 289: 47,\n"," 290: 6,\n"," 291: 21,\n"," 292: 33,\n"," 293: 11,\n"," 294: 47,\n"," 295: 6,\n"," 296: 0,\n"," 297: 16,\n"," 298: 14,\n"," 299: 19,\n"," 300: 16,\n"," 301: 39,\n"," 302: 10,\n"," 303: 44,\n"," 304: 46,\n"," 305: 0,\n"," 306: 16,\n"," 307: 3,\n"," 308: 47,\n"," 309: 45,\n"," 310: 17,\n"," 311: 0,\n"," 312: 36,\n"," 313: 18,\n"," 314: 6,\n"," 315: 32,\n"," 316: 2,\n"," 317: 33,\n"," 318: 35,\n"," 319: 15,\n"," 320: 10,\n"," 321: 27,\n"," 322: 48,\n"," 323: 39,\n"," 324: 37,\n"," 325: 15,\n"," 326: 13,\n"," 327: 40,\n"," 328: 6,\n"," 329: 45,\n"," 330: 47,\n"," 331: 38,\n"," 332: 30,\n"," 333: 21,\n"," 334: 22,\n"," 335: 5,\n"," 336: 41,\n"," 337: 5,\n"," 338: 38,\n"," 339: 13,\n"," 340: 6,\n"," 341: 21,\n"," 342: 32,\n"," 343: 43,\n"," 344: 7,\n"," 345: 26,\n"," 346: 0,\n"," 347: 43,\n"," 348: 36,\n"," 349: 21,\n"," 350: 37,\n"," 351: 38,\n"," 352: 32,\n"," 353: 33,\n"," 354: 5,\n"," 355: 33,\n"," 356: 28,\n"," 357: 35,\n"," 358: 32,\n"," 359: 40,\n"," 360: 33,\n"," 361: 46,\n"," 362: 9,\n"," 363: 43,\n"," 364: 21,\n"," 365: 38,\n"," 366: 33,\n"," 367: 30,\n"," 368: 33,\n"," 369: 29,\n"," 370: 33,\n"," 371: 39,\n"," 372: 0,\n"," 373: 0,\n"," 374: 45,\n"," 375: 27,\n"," 376: 32,\n"," 377: 42,\n"," 378: 0,\n"," 379: 14,\n"," 380: 18,\n"," 381: 33,\n"," 382: 47,\n"," 383: 47,\n"," 384: 47,\n"," 385: 32,\n"," 386: 30,\n"," 387: 5,\n"," 388: 45,\n"," 389: 16,\n"," 390: 47,\n"," 391: 2,\n"," 392: 1,\n"," 393: 39,\n"," 394: 33,\n"," 395: 46,\n"," 396: 33,\n"," 397: 6,\n"," 398: 5,\n"," 399: 0,\n"," 400: 15,\n"," 401: 14,\n"," 402: 31,\n"," 403: 7,\n"," 404: 48,\n"," 405: 14,\n"," 406: 48,\n"," 407: 26,\n"," 408: 13,\n"," 409: 48,\n"," 410: 19,\n"," 411: 17,\n"," 412: 22,\n"," 413: 38,\n"," 414: 21,\n"," 415: 6,\n"," 416: 3,\n"," 417: 38,\n"," 418: 16,\n"," 419: 1,\n"," 420: 37,\n"," 421: 46,\n"," 422: 38,\n"," 423: 29,\n"," 424: 33,\n"," 425: 33,\n"," 426: 35,\n"," 427: 22,\n"," 428: 16,\n"," 429: 32,\n"," 430: 39,\n"," 431: 22,\n"," 432: 0,\n"," 433: 32,\n"," 434: 0,\n"," 435: 42,\n"," 436: 38,\n"," 437: 39,\n"," 438: 38,\n"," 439: 16,\n"," 440: 38,\n"," 441: 21,\n"," 442: 45,\n"," 443: 33,\n"," 444: 32,\n"," 445: 10,\n"," 446: 6,\n"," 447: 36,\n"," 448: 13,\n"," 449: 6,\n"," 450: 46,\n"," 451: 32,\n"," 452: 0,\n"," 453: 37,\n"," 454: 9,\n"," 455: 33,\n"," 456: 13,\n"," 457: 38,\n"," 458: 39,\n"," 459: 14,\n"," 460: 30,\n"," 461: 10,\n"," 462: 16,\n"," 463: 38,\n"," 464: 37,\n"," 465: 30,\n"," 466: 42,\n"," 467: 11,\n"," 468: 14,\n"," 469: 45,\n"," 470: 45,\n"," 471: 5,\n"," 472: 21,\n"," 473: 33,\n"," 474: 36,\n"," 475: 39,\n"," 476: 14,\n"," 477: 32,\n"," 478: 6,\n"," 479: 6,\n"," 480: 10,\n"," 481: 31,\n"," 482: 17,\n"," 483: 21,\n"," 484: 33,\n"," 485: 13,\n"," 486: 29,\n"," 487: 46,\n"," 488: 7,\n"," 489: 30,\n"," 490: 30,\n"," 491: 23,\n"," 492: 37,\n"," 493: 3,\n"," 494: 22,\n"," 495: 33,\n"," 496: 33,\n"," 497: 9,\n"," 498: 29,\n"," 499: 36,\n"," 500: 43,\n"," 501: 6,\n"," 502: 0,\n"," 503: 11,\n"," 504: 32,\n"," 505: 40,\n"," 506: 32,\n"," 507: 45,\n"," 508: 2,\n"," 509: 16,\n"," 510: 16,\n"," 511: 32,\n"," 512: 26,\n"," 513: 16,\n"," 514: 39,\n"," 515: 21,\n"," 516: 46,\n"," 517: 45,\n"," 518: 19,\n"," 519: 22,\n"," 520: 1,\n"," 521: 18,\n"," 522: 33,\n"," 523: 48,\n"," 524: 16,\n"," 525: 36,\n"," 526: 15,\n"," 527: 30,\n"," 528: 40,\n"," 529: 0,\n"," 530: 0,\n"," 531: 6,\n"," 532: 9,\n"," 533: 21,\n"," 534: 14,\n"," 535: 47,\n"," 536: 33,\n"," 537: 10,\n"," 538: 9,\n"," 539: 9,\n"," 540: 6,\n"," 541: 36,\n"," 542: 45,\n"," 543: 22,\n"," 544: 21,\n"," 545: 6,\n"," 546: 46,\n"," 547: 11,\n"," 548: 32,\n"," 549: 23,\n"," 550: 15,\n"," 551: 10,\n"," 552: 19,\n"," 553: 46,\n"," 554: 6,\n"," 555: 27,\n"," 556: 31,\n"," 557: 10,\n"," 558: 47,\n"," 559: 46,\n"," 560: 43,\n"," 561: 36,\n"," 562: 10,\n"," 563: 13,\n"," 564: 43,\n"," 565: 47,\n"," 566: 33,\n"," 567: 33,\n"," 568: 32,\n"," 569: 16,\n"," 570: 36,\n"," 571: 34,\n"," 572: 38,\n"," 573: 48,\n"," 574: 33,\n"," 575: 43,\n"," 576: 0,\n"," 577: 36,\n"," 578: 47,\n"," 579: 17,\n"," 580: 45,\n"," 581: 19,\n"," 582: 0,\n"," 583: 3,\n"," 584: 30,\n"," 585: 5,\n"," 586: 29,\n"," 587: 15,\n"," 588: 44,\n"," 589: 8,\n"," 590: 42,\n"," 591: 33,\n"," 592: 44,\n"," 593: 15,\n"," 594: 21,\n"," 595: 22,\n"," 596: 7,\n"," 597: 9,\n"," 598: 38,\n"," 599: 39,\n"," 600: 36,\n"," 601: 0,\n"," 602: 13,\n"," 603: 18,\n"," 604: 30,\n"," 605: 29,\n"," 606: 46,\n"," 607: 45,\n"," 608: 47,\n"," 609: 4,\n"," 610: 5,\n"," 611: 12,\n"," 612: 43,\n"," 613: 9,\n"," 614: 0,\n"," 615: 17,\n"," 616: 16,\n"," 617: 8,\n"," 618: 34,\n"," 619: 9,\n"," 620: 44,\n"," 621: 34,\n"," 622: 14,\n"," 623: 43,\n"," 624: 0,\n"," 625: 40,\n"," 626: 19,\n"," 627: 6,\n"," 628: 35,\n"," 629: 3,\n"," 630: 48,\n"," 631: 30,\n"," 632: 48,\n"," 633: 31,\n"," 634: 5,\n"," 635: 30,\n"," 636: 39,\n"," 637: 32,\n"," 638: 1,\n"," 639: 23,\n"," 640: 0,\n"," 641: 46,\n"," 642: 16,\n"," 643: 34,\n"," 644: 33,\n"," 645: 21,\n"," 646: 46,\n"," 647: 45,\n"," 648: 39,\n"," 649: 46,\n"," 650: 6,\n"," 651: 10,\n"," 652: 3,\n"," 653: 45,\n"," 654: 0,\n"," 655: 0,\n"," 656: 6,\n"," 657: 3,\n"," 658: 32,\n"," 659: 36,\n"," 660: 5,\n"," 661: 42,\n"," 662: 33,\n"," 663: 44,\n"," 664: 1,\n"," 665: 39,\n"," 666: 29,\n"," 667: 42,\n"," 668: 16,\n"," 669: 30,\n"," 670: 9,\n"," 671: 33,\n"," 672: 15,\n"," 673: 46,\n"," 674: 30,\n"," 675: 3,\n"," 676: 6,\n"," 677: 6,\n"," 678: 42,\n"," 679: 26,\n"," 680: 39,\n"," 681: 15,\n"," 682: 38,\n"," 683: 39,\n"," 684: 9,\n"," 685: 36,\n"," 686: 39,\n"," 687: 34,\n"," 688: 0,\n"," 689: 32,\n"," 690: 48,\n"," 691: 16,\n"," 692: 33,\n"," 693: 32,\n"," 694: 0,\n"," 695: 21,\n"," 696: 40,\n"," 697: 17,\n"," 698: 32,\n"," 699: 0,\n"," 700: 36,\n"," 701: 33,\n"," 702: 3,\n"," 703: 10,\n"," 704: 39,\n"," 705: 33,\n"," 706: 32,\n"," 707: 33,\n"," 708: 5,\n"," 709: 34,\n"," 710: 9,\n"," 711: 39,\n"," 712: 49,\n"," 713: 36,\n"," 714: 48,\n"," 715: 37,\n"," 716: 32,\n"," 717: 22,\n"," 718: 45,\n"," 719: 31,\n"," 720: 33,\n"," 721: 5,\n"," 722: 4,\n"," 723: 15,\n"," 724: 42,\n"," 725: 27,\n"," 726: 14,\n"," 727: 3,\n"," 728: 46,\n"," 729: 45,\n"," 730: 29,\n"," 731: 13,\n"," 732: 39,\n"," 733: 22,\n"," 734: 21,\n"," 735: 42,\n"," 736: 10,\n"," 737: 27,\n"," 738: 48,\n"," 739: 45,\n"," 740: 0,\n"," 741: 3,\n"," 742: 48,\n"," 743: 39,\n"," 744: 26,\n"," 745: 34,\n"," 746: 13,\n"," 747: 26,\n"," 748: 22,\n"," 749: 0,\n"," 750: 5,\n"," 751: 45,\n"," 752: 47,\n"," 753: 6,\n"," 754: 16,\n"," 755: 3,\n"," 756: 32,\n"," 757: 46,\n"," 758: 47,\n"," 759: 32,\n"," 760: 38,\n"," 761: 45,\n"," 762: 6,\n"," 763: 45,\n"," 764: 32,\n"," 765: 9,\n"," 766: 32,\n"," 767: 4,\n"," 768: 33,\n"," 769: 15,\n"," 770: 11,\n"," 771: 6,\n"," 772: 29,\n"," 773: 40,\n"," 774: 6,\n"," 775: 47,\n"," 776: 32,\n"," 777: 42,\n"," 778: 0,\n"," 779: 37,\n"," 780: 9,\n"," 781: 32,\n"," 782: 43,\n"," 783: 38,\n"," 784: 14,\n"," 785: 30,\n"," 786: 6,\n"," 787: 14,\n"," 788: 45,\n"," 789: 14,\n"," 790: 39,\n"," 791: 33,\n"," 792: 25,\n"," 793: 44,\n"," 794: 33,\n"," 795: 6,\n"," 796: 47,\n"," 797: 0,\n"," 798: 17,\n"," 799: 4,\n"," 800: 6,\n"," 801: 3,\n"," 802: 48,\n"," 803: 0,\n"," 804: 30,\n"," 805: 30,\n"," 806: 1,\n"," 807: 39,\n"," 808: 46,\n"," 809: 4,\n"," 810: 16,\n"," 811: 10,\n"," 812: 45,\n"," 813: 38,\n"," 814: 22,\n"," 815: 11,\n"," 816: 18,\n"," 817: 14,\n"," 818: 27,\n"," 819: 17,\n"," 820: 7,\n"," 821: 21,\n"," 822: 46,\n"," 823: 32,\n"," 824: 11,\n"," 825: 22,\n"," 826: 9,\n"," 827: 19,\n"," 828: 30,\n"," 829: 34,\n"," 830: 5,\n"," 831: 43,\n"," 832: 38,\n"," 833: 32,\n"," 834: 0,\n"," 835: 36,\n"," 836: 36,\n"," 837: 9,\n"," 838: 30,\n"," 839: 36,\n"," 840: 16,\n"," 841: 0,\n"," 842: 14,\n"," 843: 39,\n"," 844: 9,\n"," 845: 0,\n"," 846: 0,\n"," 847: 6,\n"," 848: 15,\n"," 849: 43,\n"," 850: 35,\n"," 851: 9,\n"," 852: 39,\n"," 853: 7,\n"," 854: 21,\n"," 855: 22,\n"," 856: 39,\n"," 857: 0,\n"," 858: 22,\n"," 859: 32,\n"," 860: 16,\n"," 861: 36,\n"," 862: 32,\n"," 863: 0,\n"," 864: 16,\n"," 865: 13,\n"," 866: 15,\n"," 867: 32,\n"," 868: 45,\n"," 869: 30,\n"," 870: 43,\n"," 871: 33,\n"," 872: 9,\n"," 873: 37,\n"," 874: 32,\n"," 875: 13,\n"," 876: 10,\n"," 877: 39,\n"," 878: 45,\n"," 879: 33,\n"," 880: 33,\n"," 881: 16,\n"," 882: 0,\n"," 883: 42,\n"," 884: 3,\n"," 885: 10,\n"," 886: 28,\n"," 887: 1,\n"," 888: 26,\n"," 889: 32,\n"," 890: 0,\n"," 891: 44,\n"," 892: 32,\n"," 893: 0,\n"," 894: 33,\n"," 895: 38,\n"," 896: 36,\n"," 897: 40,\n"," 898: 33,\n"," 899: 16,\n"," 900: 36,\n"," 901: 36,\n"," 902: 11,\n"," 903: 1,\n"," 904: 31,\n"," 905: 46,\n"," 906: 34,\n"," 907: 46,\n"," 908: 22,\n"," 909: 32,\n"," 910: 0,\n"," 911: 39,\n"," 912: 38,\n"," 913: 5,\n"," 914: 15,\n"," 915: 45,\n"," 916: 43,\n"," 917: 16,\n"," 918: 48,\n"," 919: 18,\n"," 920: 0,\n"," 921: 8,\n"," 922: 36,\n"," 923: 22,\n"," 924: 45,\n"," 925: 27,\n"," 926: 0,\n"," 927: 36,\n"," 928: 13,\n"," 929: 36,\n"," 930: 16,\n"," 931: 32,\n"," 932: 33,\n"," 933: 46,\n"," 934: 47,\n"," 935: 36,\n"," 936: 32,\n"," 937: 32,\n"," 938: 6,\n"," 939: 22,\n"," 940: 32,\n"," 941: 0,\n"," 942: 32,\n"," 943: 32,\n"," 944: 40,\n"," 945: 27,\n"," 946: 42,\n"," 947: 33,\n"," 948: 47,\n"," 949: 15,\n"," 950: 38,\n"," 951: 38,\n"," 952: 22,\n"," 953: 7,\n"," 954: 0,\n"," 955: 16,\n"," 956: 14,\n"," 957: 46,\n"," 958: 38,\n"," 959: 48,\n"," 960: 32,\n"," 961: 48,\n"," 962: 15,\n"," 963: 6,\n"," 964: 39,\n"," 965: 6,\n"," 966: 10,\n"," 967: 39,\n"," 968: 0,\n"," 969: 0,\n"," 970: 0,\n"," 971: 0,\n"," 972: 21,\n"," 973: 32,\n"," 974: 16,\n"," 975: 0,\n"," 976: 45,\n"," 977: 16,\n"," 978: 32,\n"," 979: 1,\n"," 980: 15,\n"," 981: 13,\n"," 982: 6,\n"," 983: 6,\n"," 984: 14,\n"," 985: 15,\n"," 986: 33,\n"," 987: 43,\n"," 988: 48,\n"," 989: 14,\n"," 990: 31,\n"," 991: 36,\n"," 992: 10,\n"," 993: 46,\n"," 994: 44,\n"," 995: 11,\n"," 996: 38,\n"," 997: 3,\n"," 998: 9,\n"," 999: 30,\n"," ...}"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["Construct a new dataframe with columns: sentence, arg0, arg1, arg0id, arg1id, arg0topicid, arg1topicid"],"metadata":{"id":"5h7FCuTAYdR0"}},{"cell_type":"code","source":["semeval_label_topic_df = semeval_label_causal.copy()\n","semeval_label_topic_df.head(5)"],"metadata":{"id":"1zNYIpsttr62","colab":{"base_uri":"https://localhost:8080/","height":635},"executionInfo":{"status":"ok","timestamp":1668723680336,"user_tz":300,"elapsed":305,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"55a9f2b4-d3c8-4466-fb42-d77609ede947"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           corpus      doc_id  sent_id  eg_id                          index  \\\n","6   semeval2010t8  train.json        6      0   semeval2010t8_train.json_6_0   \n","13  semeval2010t8  train.json       13      0  semeval2010t8_train.json_13_0   \n","22  semeval2010t8  train.json       22      0  semeval2010t8_train.json_22_0   \n","26  semeval2010t8  train.json       26      0  semeval2010t8_train.json_26_0   \n","31  semeval2010t8  train.json       31      0  semeval2010t8_train.json_31_0   \n","\n","                                                 text  \\\n","6   The current view is that the chronic inflammat...   \n","13  The burst has been caused by water hammer pres...   \n","22  The singer , who performed three of the nomina...   \n","26  Suicide is one of the leading causes of death ...   \n","31  He had chest pains and headaches from mold in ...   \n","\n","                                         text_w_pairs  seq_label  pair_label  \\\n","6   The current view is that the chronic <ARG1>inf...          1           1   \n","13  The <ARG1>burst</ARG1> has been caused by wate...          1           1   \n","22  The <ARG0>singer</ARG0> , who performed three ...          1           1   \n","26  <ARG0>Suicide</ARG0> is one of the leading cau...          1           1   \n","31  He had chest pains and <ARG1>headaches</ARG1> ...          1           1   \n","\n","    context  num_sents  \n","6       NaN          1  \n","13      NaN          1  \n","22      NaN          1  \n","26      NaN          1  \n","31      NaN          1  "],"text/html":["\n","  <div id=\"df-1911ca9f-664b-4d94-994e-fb2b89576048\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus</th>\n","      <th>doc_id</th>\n","      <th>sent_id</th>\n","      <th>eg_id</th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>text_w_pairs</th>\n","      <th>seq_label</th>\n","      <th>pair_label</th>\n","      <th>context</th>\n","      <th>num_sents</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_6_0</td>\n","      <td>The current view is that the chronic inflammat...</td>\n","      <td>The current view is that the chronic &lt;ARG1&gt;inf...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_13_0</td>\n","      <td>The burst has been caused by water hammer pres...</td>\n","      <td>The &lt;ARG1&gt;burst&lt;/ARG1&gt; has been caused by wate...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_22_0</td>\n","      <td>The singer , who performed three of the nomina...</td>\n","      <td>The &lt;ARG0&gt;singer&lt;/ARG0&gt; , who performed three ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_26_0</td>\n","      <td>Suicide is one of the leading causes of death ...</td>\n","      <td>&lt;ARG0&gt;Suicide&lt;/ARG0&gt; is one of the leading cau...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_31_0</td>\n","      <td>He had chest pains and headaches from mold in ...</td>\n","      <td>He had chest pains and &lt;ARG1&gt;headaches&lt;/ARG1&gt; ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1911ca9f-664b-4d94-994e-fb2b89576048')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1911ca9f-664b-4d94-994e-fb2b89576048 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1911ca9f-664b-4d94-994e-fb2b89576048');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# modify the original extract_args function to differentiate which text arguments belong to\n","def extract_args2(dataset):\n","    arg0s = []\n","    arg1s = []\n","    for textwpair in dataset:\n","        arg0 = re.findall(r\"<ARG0>(.*?)</ARG0>\", textwpair) # list of all argument0s in string textwpair\n","        arg1 = re.findall(r\"<ARG1>(.*?)</ARG1>\", textwpair) # list of all argument1s in string textwpair\n","        arg0s.append(arg0)\n","        arg1s.append(arg1)\n","    return arg0s, arg1s"],"metadata":{"id":"uMLKNsiPYeJg","executionInfo":{"status":"ok","timestamp":1668723683241,"user_tz":300,"elapsed":307,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# create new columns for lists of arg0s and arg1s for each text\n","semeval_label_topic_df['arg0'], semeval_label_topic_df['arg1'] = extract_args2(semeval_label_textwpairs)"],"metadata":{"id":"l1t5-LoQXIZe","executionInfo":{"status":"ok","timestamp":1668723684309,"user_tz":300,"elapsed":2,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# create arg0_id and arg1_id columns\n","semeval_label_arg0id_list = []\n","for arg0s in semeval_label_topic_df['arg0']:\n","    temp = []\n","    for arg0 in arg0s:\n","        temp.append(semeval_label_word2id[arg0])\n","    semeval_label_arg0id_list.append(temp)\n","semeval_label_topic_df['arg0_id'] = semeval_label_arg0id_list\n","\n","semeval_label_arg1id_list = []\n","for arg1s in semeval_label_topic_df['arg1']:\n","    temp = []\n","    for arg1 in arg1s:\n","        temp.append(semeval_label_word2id[arg1])\n","    semeval_label_arg1id_list.append(temp)\n","semeval_label_topic_df['arg1_id'] = semeval_label_arg1id_list"],"metadata":{"id":"3-2kx4FrdMqZ","executionInfo":{"status":"ok","timestamp":1668723685260,"user_tz":300,"elapsed":4,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# create columns for arg0_topicid and arg1_topicid\n","semeval_label_arg0topic_list = []\n","for arg0s in semeval_label_arg0id_list:\n","    temp = []\n","    for arg0 in arg0s:\n","        temp.append(argid2topic_dict[arg0])\n","    semeval_label_arg0topic_list.append(temp)\n","semeval_label_topic_df['arg0_topicid'] = semeval_label_arg0topic_list\n","\n","semeval_label_arg1topic_list = []\n","for arg1s in semeval_label_arg1id_list:\n","    temp = []\n","    for arg1 in arg1s:\n","        temp.append(argid2topic_dict[arg1])\n","    semeval_label_arg1topic_list.append(temp)\n","semeval_label_topic_df['arg1_topicid'] = semeval_label_arg1topic_list"],"metadata":{"id":"ANzgDUETgaae","executionInfo":{"status":"ok","timestamp":1668723687545,"user_tz":300,"elapsed":2,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["semeval_label_topic_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":635},"id":"36fgeIfPgc6G","executionInfo":{"status":"ok","timestamp":1668723689127,"user_tz":300,"elapsed":6,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"b66eace1-3ec6-42ff-ce9e-e2ec8f47d5ec"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           corpus      doc_id  sent_id  eg_id                          index  \\\n","6   semeval2010t8  train.json        6      0   semeval2010t8_train.json_6_0   \n","13  semeval2010t8  train.json       13      0  semeval2010t8_train.json_13_0   \n","22  semeval2010t8  train.json       22      0  semeval2010t8_train.json_22_0   \n","26  semeval2010t8  train.json       26      0  semeval2010t8_train.json_26_0   \n","31  semeval2010t8  train.json       31      0  semeval2010t8_train.json_31_0   \n","\n","                                                 text  \\\n","6   The current view is that the chronic inflammat...   \n","13  The burst has been caused by water hammer pres...   \n","22  The singer , who performed three of the nomina...   \n","26  Suicide is one of the leading causes of death ...   \n","31  He had chest pains and headaches from mold in ...   \n","\n","                                         text_w_pairs  seq_label  pair_label  \\\n","6   The current view is that the chronic <ARG1>inf...          1           1   \n","13  The <ARG1>burst</ARG1> has been caused by wate...          1           1   \n","22  The <ARG0>singer</ARG0> , who performed three ...          1           1   \n","26  <ARG0>Suicide</ARG0> is one of the leading cau...          1           1   \n","31  He had chest pains and <ARG1>headaches</ARG1> ...          1           1   \n","\n","    context  num_sents         arg0            arg1 arg0_id arg1_id  \\\n","6       NaN          1  [infection]  [inflammation]   [351]   [406]   \n","13      NaN          1   [pressure]         [burst]   [269]   [461]   \n","22      NaN          1     [singer]     [commotion]  [1099]   [163]   \n","26      NaN          1    [Suicide]         [death]   [348]   [630]   \n","31      NaN          1       [mold]     [headaches]  [1049]   [472]   \n","\n","   arg0_topicid arg1_topicid  \n","6          [38]         [48]  \n","13         [45]         [10]  \n","22         [46]          [7]  \n","26         [36]         [48]  \n","31         [16]         [21]  "],"text/html":["\n","  <div id=\"df-5cc26888-5026-4da1-a7d4-151f16bef4d7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus</th>\n","      <th>doc_id</th>\n","      <th>sent_id</th>\n","      <th>eg_id</th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>text_w_pairs</th>\n","      <th>seq_label</th>\n","      <th>pair_label</th>\n","      <th>context</th>\n","      <th>num_sents</th>\n","      <th>arg0</th>\n","      <th>arg1</th>\n","      <th>arg0_id</th>\n","      <th>arg1_id</th>\n","      <th>arg0_topicid</th>\n","      <th>arg1_topicid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_6_0</td>\n","      <td>The current view is that the chronic inflammat...</td>\n","      <td>The current view is that the chronic &lt;ARG1&gt;inf...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[infection]</td>\n","      <td>[inflammation]</td>\n","      <td>[351]</td>\n","      <td>[406]</td>\n","      <td>[38]</td>\n","      <td>[48]</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_13_0</td>\n","      <td>The burst has been caused by water hammer pres...</td>\n","      <td>The &lt;ARG1&gt;burst&lt;/ARG1&gt; has been caused by wate...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[pressure]</td>\n","      <td>[burst]</td>\n","      <td>[269]</td>\n","      <td>[461]</td>\n","      <td>[45]</td>\n","      <td>[10]</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_22_0</td>\n","      <td>The singer , who performed three of the nomina...</td>\n","      <td>The &lt;ARG0&gt;singer&lt;/ARG0&gt; , who performed three ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[singer]</td>\n","      <td>[commotion]</td>\n","      <td>[1099]</td>\n","      <td>[163]</td>\n","      <td>[46]</td>\n","      <td>[7]</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_26_0</td>\n","      <td>Suicide is one of the leading causes of death ...</td>\n","      <td>&lt;ARG0&gt;Suicide&lt;/ARG0&gt; is one of the leading cau...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[Suicide]</td>\n","      <td>[death]</td>\n","      <td>[348]</td>\n","      <td>[630]</td>\n","      <td>[36]</td>\n","      <td>[48]</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_31_0</td>\n","      <td>He had chest pains and headaches from mold in ...</td>\n","      <td>He had chest pains and &lt;ARG1&gt;headaches&lt;/ARG1&gt; ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[mold]</td>\n","      <td>[headaches]</td>\n","      <td>[1049]</td>\n","      <td>[472]</td>\n","      <td>[16]</td>\n","      <td>[21]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cc26888-5026-4da1-a7d4-151f16bef4d7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5cc26888-5026-4da1-a7d4-151f16bef4d7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5cc26888-5026-4da1-a7d4-151f16bef4d7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["\"\"\"\n","# save the dataframe as a csv file\n","topic_model_data_path = \"/content/drive/MyDrive/Assignments/capstone/pntm/\"\n","np.save(os.path.join(topic_model_data_path, 'semeval_label_topic_df'), semeval_label_topic_df)\n","\"\"\""],"metadata":{"id":"YusM8XIhpu2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","with open( os.path.join(topic_model_data_path, 'semeval_label_topic_model_50.pt'), \"wb\") as f:\n","    torch.save(semeval_label_net, f)\n","    print(f\"Saved model at { os.path.join(topic_model_data_path) }\")\n","\"\"\""],"metadata":{"id":"tt_nun3vVqfe","executionInfo":{"status":"ok","timestamp":1668656253309,"user_tz":300,"elapsed":14,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1134439d-a5c0-498e-fc23-5dce84b90c0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model at /content/drive/MyDrive/Assignments/capstone/pntm/\n"]}]},{"cell_type":"markdown","source":["construct a table to keep track of how many times each topic of cause has caused each topic of effect"],"metadata":{"id":"8HHe3CDSr1UU"}},{"cell_type":"code","source":["semeval_label_topic_count_df = semeval_label_topic_df.copy()\n","semeval_label_topic_count_df.tail(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"id":"34lrSGokrGlq","executionInfo":{"status":"ok","timestamp":1668726237971,"user_tz":300,"elapsed":2,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"1a31c877-6dbb-4a99-8293-89a096bc42d6"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             corpus      doc_id  sent_id  eg_id  \\\n","7983  semeval2010t8  train.json     7958      0   \n","7984  semeval2010t8  train.json     7959      0   \n","7986  semeval2010t8  train.json     7961      0   \n","7987  semeval2010t8  train.json     7962      0   \n","7992  semeval2010t8  train.json     7967      0   \n","\n","                                index  \\\n","7983  semeval2010t8_train.json_7958_0   \n","7984  semeval2010t8_train.json_7959_0   \n","7986  semeval2010t8_train.json_7961_0   \n","7987  semeval2010t8_train.json_7962_0   \n","7992  semeval2010t8_train.json_7967_0   \n","\n","                                                   text  \\\n","7983  Hand creams counteract dryness from exposure t...   \n","7984  Eye discomfort from this staring effect is exa...   \n","7986  The transmitter emits a constant radio signal ...   \n","7987  Parents also experience anxiety from fear of t...   \n","7992  In chemical lasers the inversion is produced b...   \n","\n","                                           text_w_pairs  seq_label  \\\n","7983  Hand creams counteract <ARG1>dryness</ARG1> fr...          1   \n","7984  Eye <ARG1>discomfort</ARG1> from this <ARG0>st...          1   \n","7986  The <ARG0>transmitter</ARG0> emits a constant ...          1   \n","7987  Parents also experience <ARG1>anxiety</ARG1> f...          1   \n","7992  In chemical lasers the <ARG1>inversion</ARG1> ...          1   \n","\n","      pair_label  context  num_sents              arg0          arg1 arg0_id  \\\n","7983           1      NaN          1        [exposure]     [dryness]   [936]   \n","7984           1      NaN          1  [staring effect]  [discomfort]   [807]   \n","7986           1      NaN          1     [transmitter]      [signal]   [988]   \n","7987           1      NaN          1            [fear]     [anxiety]   [399]   \n","7992           1      NaN          1        [reaction]   [inversion]     [4]   \n","\n","     arg1_id arg0_topicid arg1_topicid  \n","7983   [454]         [32]          [9]  \n","7984   [848]         [39]         [15]  \n","7986   [219]         [48]          [0]  \n","7987   [842]          [0]         [14]  \n","7992   [446]         [38]          [6]  "],"text/html":["\n","  <div id=\"df-511b9e3b-473d-495a-a765-bbc048899b0a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corpus</th>\n","      <th>doc_id</th>\n","      <th>sent_id</th>\n","      <th>eg_id</th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>text_w_pairs</th>\n","      <th>seq_label</th>\n","      <th>pair_label</th>\n","      <th>context</th>\n","      <th>num_sents</th>\n","      <th>arg0</th>\n","      <th>arg1</th>\n","      <th>arg0_id</th>\n","      <th>arg1_id</th>\n","      <th>arg0_topicid</th>\n","      <th>arg1_topicid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7983</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>7958</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_7958_0</td>\n","      <td>Hand creams counteract dryness from exposure t...</td>\n","      <td>Hand creams counteract &lt;ARG1&gt;dryness&lt;/ARG1&gt; fr...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[exposure]</td>\n","      <td>[dryness]</td>\n","      <td>[936]</td>\n","      <td>[454]</td>\n","      <td>[32]</td>\n","      <td>[9]</td>\n","    </tr>\n","    <tr>\n","      <th>7984</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>7959</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_7959_0</td>\n","      <td>Eye discomfort from this staring effect is exa...</td>\n","      <td>Eye &lt;ARG1&gt;discomfort&lt;/ARG1&gt; from this &lt;ARG0&gt;st...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[staring effect]</td>\n","      <td>[discomfort]</td>\n","      <td>[807]</td>\n","      <td>[848]</td>\n","      <td>[39]</td>\n","      <td>[15]</td>\n","    </tr>\n","    <tr>\n","      <th>7986</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>7961</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_7961_0</td>\n","      <td>The transmitter emits a constant radio signal ...</td>\n","      <td>The &lt;ARG0&gt;transmitter&lt;/ARG0&gt; emits a constant ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[transmitter]</td>\n","      <td>[signal]</td>\n","      <td>[988]</td>\n","      <td>[219]</td>\n","      <td>[48]</td>\n","      <td>[0]</td>\n","    </tr>\n","    <tr>\n","      <th>7987</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>7962</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_7962_0</td>\n","      <td>Parents also experience anxiety from fear of t...</td>\n","      <td>Parents also experience &lt;ARG1&gt;anxiety&lt;/ARG1&gt; f...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[fear]</td>\n","      <td>[anxiety]</td>\n","      <td>[399]</td>\n","      <td>[842]</td>\n","      <td>[0]</td>\n","      <td>[14]</td>\n","    </tr>\n","    <tr>\n","      <th>7992</th>\n","      <td>semeval2010t8</td>\n","      <td>train.json</td>\n","      <td>7967</td>\n","      <td>0</td>\n","      <td>semeval2010t8_train.json_7967_0</td>\n","      <td>In chemical lasers the inversion is produced b...</td>\n","      <td>In chemical lasers the &lt;ARG1&gt;inversion&lt;/ARG1&gt; ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>[reaction]</td>\n","      <td>[inversion]</td>\n","      <td>[4]</td>\n","      <td>[446]</td>\n","      <td>[38]</td>\n","      <td>[6]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-511b9e3b-473d-495a-a765-bbc048899b0a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-511b9e3b-473d-495a-a765-bbc048899b0a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-511b9e3b-473d-495a-a765-bbc048899b0a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["semeval_label_topic_count_df = semeval_label_topic_count_df[['arg0_topicid', 'arg1_topicid']]\n","semeval_label_topic_count_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"7LwyuFafrzTW","executionInfo":{"status":"ok","timestamp":1668726238297,"user_tz":300,"elapsed":5,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"6f7bdda8-195c-41fd-dc3a-6c71a3ee893d"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     arg0_topicid arg1_topicid\n","6            [38]         [48]\n","13           [45]         [10]\n","22           [46]          [7]\n","26           [36]         [48]\n","31           [16]         [21]\n","...           ...          ...\n","7983         [32]          [9]\n","7984         [39]         [15]\n","7986         [48]          [0]\n","7987          [0]         [14]\n","7992         [38]          [6]\n","\n","[2006 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-19fbe7db-4881-4d3a-8c9c-d1b94cd36c2f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arg0_topicid</th>\n","      <th>arg1_topicid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>[38]</td>\n","      <td>[48]</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>[45]</td>\n","      <td>[10]</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>[46]</td>\n","      <td>[7]</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>[36]</td>\n","      <td>[48]</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>[16]</td>\n","      <td>[21]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7983</th>\n","      <td>[32]</td>\n","      <td>[9]</td>\n","    </tr>\n","    <tr>\n","      <th>7984</th>\n","      <td>[39]</td>\n","      <td>[15]</td>\n","    </tr>\n","    <tr>\n","      <th>7986</th>\n","      <td>[48]</td>\n","      <td>[0]</td>\n","    </tr>\n","    <tr>\n","      <th>7987</th>\n","      <td>[0]</td>\n","      <td>[14]</td>\n","    </tr>\n","    <tr>\n","      <th>7992</th>\n","      <td>[38]</td>\n","      <td>[6]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2006 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19fbe7db-4881-4d3a-8c9c-d1b94cd36c2f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-19fbe7db-4881-4d3a-8c9c-d1b94cd36c2f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-19fbe7db-4881-4d3a-8c9c-d1b94cd36c2f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["semeval_label_topic_count_df = semeval_label_topic_count_df.explode('arg0_topicid')\n","semeval_label_topic_count_df = semeval_label_topic_count_df.explode('arg1_topicid')\n","semeval_label_topic_count_df['index'] = semeval_label_topic_count_df.index\n","semeval_label_topic_count_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"knRf9IWFtwej","executionInfo":{"status":"ok","timestamp":1668726238594,"user_tz":300,"elapsed":3,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"7d5246ac-86d1-4305-a3e1-6149ef1a2f9e"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     arg0_topicid arg1_topicid  index\n","6              38           48      6\n","13             45           10     13\n","22             46            7     22\n","26             36           48     26\n","31             16           21     31\n","...           ...          ...    ...\n","7983           32            9   7983\n","7984           39           15   7984\n","7986           48            0   7986\n","7987            0           14   7987\n","7992           38            6   7992\n","\n","[2006 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-d4d6e802-073b-4de5-9bd7-5025f42318be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arg0_topicid</th>\n","      <th>arg1_topicid</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>38</td>\n","      <td>48</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>45</td>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>46</td>\n","      <td>7</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>36</td>\n","      <td>48</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>16</td>\n","      <td>21</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7983</th>\n","      <td>32</td>\n","      <td>9</td>\n","      <td>7983</td>\n","    </tr>\n","    <tr>\n","      <th>7984</th>\n","      <td>39</td>\n","      <td>15</td>\n","      <td>7984</td>\n","    </tr>\n","    <tr>\n","      <th>7986</th>\n","      <td>48</td>\n","      <td>0</td>\n","      <td>7986</td>\n","    </tr>\n","    <tr>\n","      <th>7987</th>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>7987</td>\n","    </tr>\n","    <tr>\n","      <th>7992</th>\n","      <td>38</td>\n","      <td>6</td>\n","      <td>7992</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2006 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4d6e802-073b-4de5-9bd7-5025f42318be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d4d6e802-073b-4de5-9bd7-5025f42318be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d4d6e802-073b-4de5-9bd7-5025f42318be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["semeval_label_topic_count_df = semeval_label_topic_count_df.groupby(['arg0_topicid', 'arg1_topicid'])['index'].count().unstack(fill_value=0)\n","semeval_label_topic_count_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sBSwpmzGuKz7","executionInfo":{"status":"ok","timestamp":1668726238595,"user_tz":300,"elapsed":3,"user":{"displayName":"Vanessa Xu","userId":"08794457152195904431"}},"outputId":"5d26ea45-f550-44e0-bc93-d33ac6754a62"},"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["arg1_topicid  0   1   2   3   4   5   6   7   8   9   ...  39  40  42  43  44  \\\n","arg0_topicid                                          ...                       \n","0              4   0   0   2   2   0  12   0   0   4  ...   2   0   2   4   2   \n","1              0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   2   \n","2              0   0   0   0   0   0   0   0   0   0  ...   2   2   0   0   0   \n","3              0   0   0   0   0   0   4   0   0   2  ...   2   2   0   0   0   \n","4              0   0   0   0   0   0   2   0   0   0  ...   0   0   0   0   0   \n","5              0   0   4   2   0   2   4   0   0   2  ...   2   0   0   0   0   \n","6              0   2   2   0   0   4   8   0   0   6  ...   8   0   2   2   0   \n","7              2   0   0   0   0   0   0   0   0   2  ...   0   0   0   0   0   \n","8              0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","9              4   0   0   2   0   0   6   0   0   8  ...   2   0   0   2   0   \n","10             2   0   0   0   0   0   4   0   0   4  ...   2   2   0   0   0   \n","11             8   0   0   0   0   0   2   0   0   0  ...   4   4   0   0   0   \n","12             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","13             0   0   2   0   0   2   4   0   0   2  ...   4   0   2   0   0   \n","14             0   0   0   0   0   2   2   0   0   2  ...   0   0   0   0   0   \n","15             2   0   2   0   0   2   0   0   0   0  ...   2   0   2   0   0   \n","16             2   0   0   0   0   2   6   2   2   0  ...   0   0   2   2   0   \n","17             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","18             0   0   2   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","19             0   0   0   0   0   2   2   0   0   2  ...   0   0   0   0   0   \n","20             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","21             2   0   2   0   0   2   2   0   0   0  ...   6   0   0   2   0   \n","22             0   0   0   0   0   0   2   0   0   0  ...   0   0   0   0   0   \n","23             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","25             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","26             0   0   0   2   0   0   0   0   0   0  ...   2   0   2   2   0   \n","27             0   2   0   0   0   0   0   0   0   0  ...   2   0   0   2   0   \n","28             2   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","29             0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   \n","30             0   0   2   4   0   0  10   0   0   6  ...   2   0   0   4   0   \n","31             2   0   0   0   0   4   0   0   0   2  ...   2   2   2   2   0   \n","32            14   4   0   4   2   0   4   2   0   8  ...   6   0   0   4   0   \n","33             2   0   0   0   0   0   4   2   0   4  ...   6   4   2   2   0   \n","34             4   0   0   0   0   4   0   0   0   0  ...   0   2   0   0   0   \n","35             2   0   0   0   0   0   2   0   0   0  ...   2   0   0   0   0   \n","36             4   0   0   4   0   2  12   0   0   0  ...   6   2   0   4   2   \n","37             2   0   0   0   0   0   2   0   2   0  ...   2   0   0   0   0   \n","38             6   0   0   2   2   2   4   2   0   2  ...   0   0   0   2   0   \n","39             6   0   0   2   0   0   0   0   0   4  ...   0   0   0   0   0   \n","40             0   0   0   0   0   2   6   0   0   0  ...   0   0   2   0   0   \n","41             0   0   0   0   0   2   0   0   0   0  ...   0   0   0   0   0   \n","42             2   0   0   4   0   0   0   0   0   0  ...   0   0   2   0   0   \n","43             4   0   0   0   0   0   2   0   0   2  ...   0   0   0   0   0   \n","44             0   0   0   0   0   2   0   0   0   0  ...   0   2   0   0   0   \n","45             6   2   0   0   2   2   0   0   0   0  ...   2   2   2   4   0   \n","46             4   2   2   4   2   0   2   2   0   4  ...   6   0   0   2   0   \n","47             8   0   0   2   0   0   4   0   0   0  ...   0   0   0   0   0   \n","48             4   0   0   0   0   4   6   2   0   0  ...   0   0   0   0   0   \n","49             0   0   0   2   0   0   0   0   0   0  ...   0   0   0   0   0   \n","\n","arg1_topicid  45  46  47  48  49  \n","arg0_topicid                      \n","0             10   4   4   2   0  \n","1              2   0   0   0   0  \n","2              0   0   0   0   0  \n","3              0   0   0   4   0  \n","4              0   0   0   0   0  \n","5              2   0   0   8   0  \n","6              2   0   2   4   0  \n","7              0   0   0   0   0  \n","8              0   0   0   0   0  \n","9              0   4   2   4   0  \n","10             4   2   0   0   2  \n","11             2   2   2   0   0  \n","12             0   0   0   0   0  \n","13             0   0   0   0   0  \n","14             0   2   0   0   0  \n","15             4   2   0   4   0  \n","16             0   2   2   6   2  \n","17             0   2   0   0   0  \n","18             4   0   0   2   0  \n","19             0   0   0   0   2  \n","20             0   0   0   0   0  \n","21             2   0   0   0   0  \n","22             4   2   2   0   0  \n","23             0   0   0   0   0  \n","25             2   0   0   0   0  \n","26             2   2   0   0   0  \n","27             2   2   2   2   0  \n","28             0   0   0   0   0  \n","29             0   0   0   2   0  \n","30             4   0   0   0   0  \n","31             6   0   0   0   0  \n","32             6   4   0   0   0  \n","33             6   0   2   2   0  \n","34             2   0   0   0   0  \n","35             2   0   0   0   0  \n","36             4   4   2   2   2  \n","37             2   0   0   2   0  \n","38             2   0   0   6   0  \n","39             0   4   2   2   0  \n","40             2   2   0   2   0  \n","41             0   0   0   0   0  \n","42             0   0   0   0   0  \n","43             0   0   2   0   0  \n","44             4   0   0   2   0  \n","45             4   0   2   8   0  \n","46             4   0   2   6   0  \n","47             0   0   0   0   0  \n","48             2   4   2   4   0  \n","49             0   0   0   0   0  \n","\n","[49 rows x 46 columns]"],"text/html":["\n","  <div id=\"df-d64ef16a-1ac9-4286-b2ef-a45c81942ac7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>arg1_topicid</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","    </tr>\n","    <tr>\n","      <th>arg0_topicid</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>49 rows × 46 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d64ef16a-1ac9-4286-b2ef-a45c81942ac7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d64ef16a-1ac9-4286-b2ef-a45c81942ac7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d64ef16a-1ac9-4286-b2ef-a45c81942ac7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":[],"metadata":{"id":"HsTfVQ8r1b81"},"execution_count":null,"outputs":[]}]}